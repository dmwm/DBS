WARNING: --write-unchanged-files/-W implies -w.
RefactoringTool: Skipping optional fixer: buffer
RefactoringTool: Skipping optional fixer: idioms
RefactoringTool: Skipping optional fixer: set_literal
RefactoringTool: Skipping optional fixer: ws_comma
RefactoringTool: Refactored ./setup.py
RefactoringTool: Refactored ./Client/setup.py
RefactoringTool: No changes to ./Client/src/python/dbs/__init__.py
RefactoringTool: No changes to ./Client/src/python/dbs/apis/__init__.py
RefactoringTool: Refactored ./Client/src/python/dbs/apis/dbsClient.py
--- ./setup.py	(original)
+++ ./setup.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import sys, os, os.path, re, shutil, string
 from distutils.core import setup, Command
 from distutils.command.build import build
--- ./Client/setup.py	(original)
+++ ./Client/setup.py	(refactored)
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 
-from __future__ import division, print_function
+
 
 import os
 import sys
--- ./Client/src/python/dbs/apis/dbsClient.py	(original)
+++ ./Client/src/python/dbs/apis/dbsClient.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 from dbs.exceptions.dbsClientException import dbsClientException
 from RestClient.ErrorHandling.RestClientExceptions import HTTPError
 from RestClient.RestApi import RestApi
@@ -9,7 +9,7 @@
 import os
 import socket
 import sys
-import urllib
+import urllib.request, urllib.parse, urllib.error
 
 def slicedIterator(sourceList, sliceSize):
     """
@@ -94,7 +94,7 @@
     for element in values:
         data[key].append(element)
         if method =='GET':
-            size = len(urllib.urlencode(data))
+            size = len(urllib.parse.urlencode(data))
         else:
             size = len(data)
         if size > size_limit:
@@ -115,9 +115,9 @@
         For example (https://cmsweb-testbed.cern.ch:8443/dbs/prod/global/filechildren), so 192 bytes should be safe.
         """
         size_limit = 8000
-        encoded_url = urllib.urlencode(kwargs)
+        encoded_url = urllib.parse.urlencode(kwargs)
         if len(encoded_url) > size_limit:
-            for key, value in kwargs.iteritems():
+            for key, value in kwargs.items():
                 ###only one (first) list at a time is splitted,
                 ###currently only file lists are supported
                 if key in ('logical_file_name', 'block_name', 'lumi_list', 'run_num') and isinstance(value, list):
@@ -268,7 +268,7 @@
 
         requiredParameters = {'forced':validParameters}
 
-        checkInputParameter(method="blockDump", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="blockDump", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("blockdump", params=kwargs)
@@ -286,7 +286,7 @@
         """
         validParameters = ['call']
 
-        checkInputParameter(method="help", parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method="help", parameters=list(kwargs.keys()), validParameters=validParameters)
 
         return self.__callServer("help", params=kwargs)
 
@@ -479,7 +479,7 @@
         validParameters = ['dataset']
 
         requiredParameters = {'forced': ['dataset']}
-        checkInputParameter(method="listParentDSTrio", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listParentDSTrio", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
         return self.__callServer("parentDSTrio", params=kwargs, callmethod='GET')
 
@@ -498,7 +498,7 @@
         validParameters = ['block_name']
 
         requiredParameters = {'forced': ['block_name']}
-        checkInputParameter(method="listBlockTrio", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listBlockTrio", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
         return self.__callServer("blockTrio", params=kwargs, callmethod='GET')
 
@@ -517,7 +517,7 @@
         validParameters = ['block_name', 'logical_file_name']
 
         requiredParameters = {'forced': ['block_name']}
-        checkInputParameter(method="listFileParentsByLumi", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listFileParentsByLumi", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
         return self.__callServer("fileparentsbylumi", data=kwargs, callmethod='POST')
 
@@ -541,7 +541,7 @@
         """
         validParameters = ['acquisition_era_name']
 
-        checkInputParameter(method="listAcquisitionEras", parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method="listAcquisitionEras", parameters=list(kwargs.keys()), validParameters=validParameters)
 
         return self.__callServer("acquisitioneras", params=kwargs)
 
@@ -558,7 +558,7 @@
         """
         validParameters = ['acquisition_era_name']
 
-        checkInputParameter(method="listAcquisitionEras", parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method="listAcquisitionEras", parameters=list(kwargs.keys()), validParameters=validParameters)
 
         return self.__callServer("acquisitioneras_ci", params=kwargs)
 
@@ -576,7 +576,7 @@
 
         requiredParameters = {'forced': validParameters}
 
-        checkInputParameter(method="listBlockChildren", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listBlockChildren", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("blockchildren", params=kwargs)
@@ -594,7 +594,7 @@
         validParameters = ['block_name']
 
         requiredParameters = {'forced': validParameters}
-        checkInputParameter(method="listBlockParents", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listBlockParents", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
         if isinstance(kwargs["block_name"], list):
             return self.__callServer("blockparents", data=kwargs, callmethod='POST')
@@ -646,10 +646,10 @@
         requiredParameters = {'multiple': ['dataset', 'block_name', 'data_tier_name', 'logical_file_name']}
 
         #set defaults
-        if 'detail' not in kwargs.keys():
+        if 'detail' not in list(kwargs.keys()):
             kwargs['detail'] = False
 
-        checkInputParameter(method="listBlocks", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listBlocks", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("blocks", params=kwargs)
@@ -689,7 +689,7 @@
 
         requiredParameters = {'standalone': ['block_name', 'dataset']}
 
-        checkInputParameter(method="listBlockSummaries", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listBlockSummaries", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer('blocksummaries', params=kwargs)
@@ -712,7 +712,7 @@
 
         requiredParameters = {'multiple': ['dataset', 'block_name']}
 
-        checkInputParameter(method="listBlockOrigin", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listBlockOrigin", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 	return self.__callServer('blockorigin', params=kwargs)
 
@@ -790,10 +790,10 @@
                            'detail', 'dataset_id']
 
         #set defaults
-        if 'detail' not in kwargs.keys():
+        if 'detail' not in list(kwargs.keys()):
             kwargs['detail'] = False
 
-        checkInputParameter(method="listDatasets", parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method="listDatasets", parameters=list(kwargs.keys()), validParameters=validParameters)
 
         return self.__callServer("datasets", params=kwargs)
 
@@ -809,7 +809,7 @@
         """
         validParameters = ['dataset_access_type']
 
-        checkInputParameter(method="listDatasetAccessTypes", parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method="listDatasetAccessTypes", parameters=list(kwargs.keys()), validParameters=validParameters)
 
         return self.__callServer("datasetaccesstypes", params=kwargs)
 
@@ -832,11 +832,11 @@
         validParameters = ['dataset', 'dataset_access_type', 'detail', 'dataset_id']
 	requiredParameters = {'multiple': ['dataset', 'dataset_id']}
 
-        checkInputParameter(method="listDatasetArray", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listDatasetArray", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         #set defaults
-        if 'detail' not in kwargs.keys():
+        if 'detail' not in list(kwargs.keys()):
             kwargs['detail'] = False
 
         return self.__callServer("datasetlist", data=kwargs, callmethod='POST')
@@ -854,7 +854,7 @@
         validParameters = ['dataset']
         requiredParameters = {'forced': validParameters}
 
-        checkInputParameter(method="listDatasetChildren", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listDatasetChildren", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("datasetchildren", params=kwargs)
@@ -872,7 +872,7 @@
         validParameters = ['dataset']
         requiredParameters = {'forced': validParameters}
 
-        checkInputParameter(method="listDatasetParents", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listDatasetParents", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("datasetparents", params=kwargs)
@@ -889,7 +889,7 @@
         """
         validParameters = ['data_tier_name']
 
-        checkInputParameter(method="listDataTiers", parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method="listDataTiers", parameters=list(kwargs.keys()), validParameters=validParameters)
 
         return self.__callServer("datatiers", params=kwargs)
 
@@ -907,7 +907,7 @@
         """
         validParameters = ['datatype', 'dataset']
 
-        checkInputParameter(method="listDataTypes", parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method="listDataTypes", parameters=list(kwargs.keys()), validParameters=validParameters)
 
         return self.__callServer("datatypes", params=kwargs)
 
@@ -946,7 +946,7 @@
 
         requiredParameters = {'standalone': validParameters}
 
-        checkInputParameter(method="listFileChildren", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listFileChildren", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("filechildren", params=kwargs)
@@ -972,7 +972,7 @@
 
         requiredParameters = {'standalone': ['logical_file_name', 'block_name']}
 
-        checkInputParameter(method="listFileLumis", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listFileLumis", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("filelumis", params=kwargs)
@@ -995,7 +995,7 @@
         validParameters = ['logical_file_name', 'run_num', 'validFileOnly']
 	requiredParameters = {'forced': ['logical_file_name']}
 
-        checkInputParameter(method="listFileLumiArray", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listFileLumiArray", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("filelumis", data=kwargs, callmethod="POST")
@@ -1035,7 +1035,7 @@
 
         requiredParameters = {'standalone': validParameters}
 
-        checkInputParameter(method="listFileParents", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listFileParents", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("fileparents", params=kwargs)
@@ -1155,10 +1155,10 @@
         requiredParameters = {'multiple': validParameters}
 
         #set defaults
-        if 'detail' not in kwargs.keys():
+        if 'detail' not in list(kwargs.keys()):
             kwargs['detail'] = False
 
-        checkInputParameter(method="listFiles", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listFiles", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("files", params=kwargs)
@@ -1216,10 +1216,10 @@
         requiredParameters = {'multiple': ['dataset', 'block_name', 'logical_file_name']}
 
         #set defaults
-        if 'detail' not in kwargs.keys():
+        if 'detail' not in list(kwargs.keys()):
             kwargs['detail'] = False
 
-        checkInputParameter(method="listFileArray", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listFileArray", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
         # In order to protect DB and make sure the query can be return in 300 seconds, we limit the length of 
         # logical file names, lumi and run num to 1000. These number may be adjusted later if 
@@ -1227,18 +1227,18 @@
 
         # CMS has all MC data with run_num=1. It almost is a full table scan if run_num=1 without lfn. So we will request lfn
         # to be present when run_num=1. YG Jan 14, 2016
-        if 'logical_file_name' in kwargs.keys() and isinstance(kwargs['logical_file_name'], list)\
+        if 'logical_file_name' in list(kwargs.keys()) and isinstance(kwargs['logical_file_name'], list)\
             and len(kwargs['logical_file_name']) > 1:
-            if 'run_num' in kwargs.keys() and isinstance(kwargs['run_num'],list) and len(kwargs['run_num']) > 1 :
+            if 'run_num' in list(kwargs.keys()) and isinstance(kwargs['run_num'],list) and len(kwargs['run_num']) > 1 :
                 raise dbsClientException('Invalid input', 'files API does not supprt two lists: run_num and lfn. ')
-            elif 'lumi_list' in kwargs.keys() and kwargs['lumi_list'] and len(kwargs['lumi_list']) > 1 :
+            elif 'lumi_list' in list(kwargs.keys()) and kwargs['lumi_list'] and len(kwargs['lumi_list']) > 1 :
                 raise dbsClientException('Invalid input', 'files API does not supprt two lists: lumi_lis and lfn. ')
                 
-        elif 'lumi_list' in kwargs.keys() and kwargs['lumi_list']:
-            if 'run_num' not in kwargs.keys() or not kwargs['run_num'] or kwargs['run_num'] ==-1 :
+        elif 'lumi_list' in list(kwargs.keys()) and kwargs['lumi_list']:
+            if 'run_num' not in list(kwargs.keys()) or not kwargs['run_num'] or kwargs['run_num'] ==-1 :
                 raise dbsClientException('Invalid input', 'When Lumi section is present, a single run is required. ')
         else:
-            if 'run_num' in kwargs.keys():
+            if 'run_num' in list(kwargs.keys()):
                 if isinstance(kwargs['run_num'], list):
                     if 1 in kwargs['run_num'] or '1' in kwargs['run_num']:
                         raise dbsClientException('Invalid input', 'files API does not supprt run_num=1 when no lumi.')
@@ -1247,7 +1247,7 @@
                         raise dbsClientException('Invalid input', 'files API does not supprt run_num=1 when no lumi.')RefactoringTool: No changes to ./Client/src/python/dbs/exceptions/__init__.py
RefactoringTool: No changes to ./Client/src/python/dbs/exceptions/dbsClientException.py
RefactoringTool: Refactored ./Client/tests/setup_test.py

 
         #check if no lfn is given, but run_num=1 is used for searching
-        if ('logical_file_name' not in kwargs.keys() or not kwargs['logical_file_name']) and 'run_num' in kwargs.keys():
+        if ('logical_file_name' not in list(kwargs.keys()) or not kwargs['logical_file_name']) and 'run_num' in list(kwargs.keys()):
             if isinstance(kwargs['run_num'], list):
                 if 1 in kwargs['run_num'] or '1' in kwargs['run_num']:
                     raise dbsClientException('Invalid input', 'files API does not supprt run_num=1 without logical_file_name.')
@@ -1260,7 +1260,7 @@
         total_lumi_len = 0
         split_lumi_list = []
         max_list_len = 1000 #this number is defined in DBS server
-        for key, value in kwargs.iteritems():
+        for key, value in kwargs.items():
             if key == 'lumi_list' and isinstance(kwargs['lumi_list'], list)\
                 and kwargs['lumi_list'] and isinstance(kwargs['lumi_list'][0], list):
                 lapp = 0
@@ -1303,7 +1303,7 @@
         #Make sure this changes when we move to 2.7 or 3.0
         #http://stackoverflow.com/questions/11092511/python-list-of-unique-dictionaries
         # YG May-26-2015
-        return dict((v['logical_file_name'], v) for v in results).values()
+        return list(dict((v['logical_file_name'], v) for v in results).values())
 
     def listFileSummaries(self, **kwargs):
         """
@@ -1339,7 +1339,7 @@
 
         requiredParameters = {'standalone': ['block_name', 'dataset']}
 
-        checkInputParameter(method="listFileSummaries", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listFileSummaries", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("filesummaries", params=kwargs)
@@ -1377,7 +1377,7 @@
                            'pset_hash', 'app_name', 'output_module_label',
                            'block_id', 'global_tag']
 
-        checkInputParameter(method="listOutputConfigs", parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method="listOutputConfigs", parameters=list(kwargs.keys()), validParameters=validParameters)
 
         return self.__callServer("outputconfigs", params=kwargs)
 
@@ -1393,7 +1393,7 @@
         """
         validParameters = ['physics_group_name']
 
-        checkInputParameter(method="listPhysicsGroups", parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method="listPhysicsGroups", parameters=list(kwargs.keys()), validParameters=validParameters)
 
         return self.__callServer("physicsgroups", params=kwargs)
 
@@ -1411,7 +1411,7 @@
         """
         validParameters = ['primary_ds_name', 'primary_ds_type']
 
-        checkInputParameter(method="listPrimaryDatasets", parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method="listPrimaryDatasets", parameters=list(kwargs.keys()), validParameters=validParameters)
 
         return self.__callServer("primarydatasets", params=kwargs)
 
@@ -1429,7 +1429,7 @@
         """
         validParameters = ['primary_ds_type', 'dataset']
 
-        checkInputParameter(method="listPrimaryDSTypes", parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method="listPrimaryDSTypes", parameters=list(kwargs.keys()), validParameters=validParameters)
 
         return self.__callServer("primarydstypes", params=kwargs)
 
@@ -1445,7 +1445,7 @@
         """
         validParameters = ['processing_version']
         
-	checkInputParameter(method="listProcessingEras", parameters=kwargs.keys(), validParameters=validParameters)
+	checkInputParameter(method="listProcessingEras", parameters=list(kwargs.keys()), validParameters=validParameters)
         
 	return self.__callServer("processingeras", params=kwargs)
 
@@ -1465,7 +1465,7 @@
         """
         validParameters = ['dataset', 'release_version', 'logical_file_name']
 
-        checkInputParameter(method="listReleaseVersions", parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method="listReleaseVersions", parameters=list(kwargs.keys()), validParameters=validParameters)
 
         return self.__callServer("releaseversions", params=kwargs)
 
@@ -1488,7 +1488,7 @@
 
         requiredParameters = {'multiple': validParameters}
 
-        checkInputParameter(method="listRuns", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listRuns", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("runs", params=kwargs)
@@ -1509,7 +1509,7 @@
 
         requiredParameters = {'forced': ['run_num']}
 
-        checkInputParameter(method="listRunSummaries", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="listRunSummaries", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("runsummaries", params=kwargs)
@@ -1543,7 +1543,7 @@
         """
         validParameters = ['migration_rqst_id', 'block_name', 'dataset', 'user']
 
-        checkInputParameter(method='statusMigration', parameters=kwargs.keys(), validParameters=validParameters)
+        checkInputParameter(method='statusMigration', parameters=list(kwargs.keys()), validParameters=validParameters)
         
         return self.__callServer("status", params=kwargs)
 
@@ -1585,7 +1585,7 @@
 
         requiredParameters = {'forced': validParameters}
 
-        checkInputParameter(method="updateAcqEraEndDate", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="updateAcqEraEndDate", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("acquisitioneras", params=kwargs, callmethod='PUT')
@@ -1604,7 +1604,7 @@
 
         requiredParameters = {'forced': validParameters}
 
-        checkInputParameter(method="updateBlockStatus", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="updateBlockStatus", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("blocks", params=kwargs, callmethod='PUT')
@@ -1623,7 +1623,7 @@
 
         requiredParameters = {'forced': validParameters}
 
-        checkInputParameter(method="updateBlockSiteName", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="updateBlockSiteName", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("blocks", params=kwargs, callmethod='PUT')
@@ -1642,7 +1642,7 @@
 
         requiredParameters = {'forced': validParameters}
 
-        checkInputParameter(method="updateDatasetType", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="updateDatasetType", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("datasets", params=kwargs, callmethod='PUT')
@@ -1668,7 +1668,7 @@
         requiredParameters = {'forced': ['is_file_valid'], 'multiple': ['logical_file_name', 'dataset']}
 
 
-        checkInputParameter(method="updateFileStatus", parameters=kwargs.keys(), validParameters=validParameters,
+        checkInputParameter(method="updateFileStatus", parameters=list(kwargs.keys()), validParameters=validParameters,
                             requiredParameters=requiredParameters)
 
         return self.__callServer("files", params=kwargs, callmethod='PUT')
--- ./Client/tests/setup_test.py	(original)
+++ ./Client/tests/setup_test.py	(refactored)RefactoringTool: No changes to ./Client/tests/dbsclient_t/__init__.py
RefactoringTool: Refactored ./Client/tests/dbsclient_t/deployment/DBSDeployment_t.py

@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import fnmatch
 import os
 import re
--- ./Client/tests/dbsclient_t/deployment/DBSDeployment_t.py	(original)
+++ ./Client/tests/dbsclient_t/deployment/DBSDeployment_t.py	(refactored)
@@ -1,7 +1,7 @@
 """
 DBS 3 Post-Deployment Tests for Operators of CMSWEB (These tests are read only!)
 """
-from __future__ import print_function
+
 import json
 import os
 import re
@@ -207,7 +207,7 @@
         self.assertFalse(False in result)
 
     def test_list_block_children(self):
-        expected_data = [{u'block_name': u'/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST_\
+        expected_data = [{'block_name': '/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST_\
 CHILD-v4711/RECO#8c0cf576-cf55-4379-8c47-dee34ee68c81'}]
 
         blockchildren = sorted(self.api.listBlockChildren(block_name="/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_\
@@ -227,9 +227,9 @@
         self.assertTrue(result)
 
     def test_list_block_parents(self):
-        expected_data = [{u'parent_block_name': u'/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_\
+        expected_data = [{'parent_block_name': '/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_\
 TEST-v4711/RAW#8c0cf576-cf55-4379-8c47-dee34ee68c81',
-                          u'this_block_name': u'/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_\
+                          'this_block_name': '/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_\
 TEST_CHILD-v4711/RECO#8c0cf576-cf55-4379-8c47-dee34ee68c81'}]
 
         blockparents = self.api.listBlockParents(block_name="/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
@@ -259,7 +259,7 @@
 
         fp.close()
 
-        expected_data = [{u'block_name': u'/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST-\
+        expected_data = [{'block_name': '/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST-\
 v4711/RAW#8c0cf576-cf55-4379-8c47-dee34ee68c81'}]
 
         blocks = self.api.listBlocks(block_name="/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_\
@@ -284,7 +284,7 @@
         self.assertTrue(result)
 
     def test_list_block_summaries(self):
-        expected_data = [{u'num_file': 10, u'num_event': 553964, u'file_size': 25350778463}]
+        expected_data = [{'num_file': 10, 'num_event': 553964, 'file_size': 25350778463}]
 
         summaries = self.api.listBlockSummaries(block_name="/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW#8c0cf576-cf55-4379-8c47-dee34ee68c81")
@@ -354,9 +354,9 @@
         fp.close()
 
     def test_list_dataset_children(self):
-        expected_data = [{u'child_dataset': u'/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST_\
+        expected_data = [{'child_dataset': '/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST_\
 CHILD-v4711/RECO',
-                          u'dataset': u'/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST-v4711\
+                          'dataset': '/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST-v4711\
 /RAW'}]
 
         children = stripChangingParameters(self.api.listDatasetChildren(dataset="/DBS3DeploymentTestPrimary/DBS3_\
@@ -376,9 +376,9 @@
         self.assertTrue(result)
 
     def test_list_dataset_parents(self):
-        expected_data = [{u'this_dataset': u'/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST_\
+        expected_data = [{'this_dataset': '/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST_\
 CHILD-v4711/RECO',
-                          u'parent_dataset': u'/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST\
+                          'parent_dataset': '/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST\
 -v4711/RAW'}]
 
         parents = stripChangingParameters(
@@ -443,9 +443,9 @@
         self.assertFalse(False in result)
 
     def test_list_file_children(self):
-        expected_data = [{u'child_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+        expected_data = [{'child_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_0.root'],
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_0.root'}]
 
         children = self.api.listFileChildren(logical_file_name="/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_\
@@ -453,45 +453,45 @@
 
         self.assertEqual(expected_data, children)
 
-        expected_data = [{u'child_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+        expected_data = [{'child_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_0.root'],
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_0.root'},
-                         {u'child_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+                         {'child_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_1.root'],
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_1.root'},
-                         {u'child_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+                         {'child_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_2.root'],
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_2.root'},
-                         {u'child_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+                         {'child_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_3.root'],
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_3.root'},
-                         {u'child_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+                         {'child_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_4.root'],RefactoringTool: No changes to ./Client/tests/dbsclient_t/deployment/__init__.py
RefactoringTool: Refactored ./Client/tests/dbsclient_t/unittests/DBSClientBlockWriter_t.py
RefactoringTool: Refactored ./Client/tests/dbsclient_t/unittests/DBSClientReader_t.py

-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_4.root'},
-                         {u'child_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+                         {'child_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_5.root'],
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_5.root'},
-                         {u'child_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+                         {'child_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_6.root'],
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_6.root'},
-                         {u'child_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+                         {'child_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_7.root'],
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_7.root'},
-                         {u'child_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+                         {'child_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_8.root'],
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_8.root'},
-                         {u'child_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+                         {'child_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_9.root'],
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_9.root'}]
 
         children = self.api.listFileChildren(logical_file_name=["/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_\
@@ -546,8 +546,8 @@
 
         fp.close()
 
-        expected_data = [{u'lumi_section_num': [24022, 24122, 24222], u'run_num': 43,
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+        expected_data = [{'lumi_section_num': [24022, 24122, 24222], 'run_num': 43,
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_0.root'}]
 
         lumis = sorted(self.api.listFileLumis(logical_file_name="/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_\
@@ -582,9 +582,9 @@
 
         fp.close()
 
-        expected_data = [{u'parent_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+        expected_data = [{'parent_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_0.root'],
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_0.root'}]
 
         parents = self.api.listFileParents(logical_file_name="/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_\
@@ -592,15 +592,15 @@
 
         self.assertEqual(expected_data, parents)
 
-        expected_data = [{u'parent_logical_file_name': [u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+        expected_data = [{'parent_logical_file_name': ['/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST-v4711/RAW/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_%s.root' % i],
-                          u'logical_file_name': u'/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
+                          'logical_file_name': '/store/mc/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-\
 DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-dee34ee68c81_%s.root' % j}
-                         for i, j in zip(xrange(10), xrange(10))]
+                         for i, j in zip(range(10), range(10))]
 
         parents = sorted(self.api.listFileParents(logical_file_name=["/store/mc/DBS3DeploymentTestPrimary/DBS3_\
 DEPLOYMENT_TEST_ERA-DBS3_DEPLOYMENT_TEST_CHILD-v4711/RECO/DBS3_DEPLOYMENT_TEST/123456789/8c0cf576-cf55-4379-8c47-\
-dee34ee68c81_%s.root" % i for i in xrange(10)]), key=lambda k: k["parent_logical_file_name"])
+dee34ee68c81_%s.root" % i for i in range(10)]), key=lambda k: k["parent_logical_file_name"])
 
         self.assertEqual(expected_data, parents)
 
@@ -655,8 +655,8 @@
         self.assertTrue(result)
 
     def test_list_file_summaries(self):
-        expected_data = [{u'num_block': 1, u'num_file': 10, u'num_event': 553964, u'num_lumi': 30,
-                          u'file_size': 25350778463}]
+        expected_data = [{'num_block': 1, 'num_file': 10, 'num_event': 553964, 'num_lumi': 30,
+                          'file_size': 25350778463}]
 
         summaries = self.api.listFileSummaries(block_name="/DBS3DeploymentTestPrimary/DBS3_DEPLOYMENT_TEST_ERA-DBS3_\
 DEPLOYMENT_TEST-v4711/RAW#8c0cf576-cf55-4379-8c47-dee34ee68c81")
@@ -747,7 +747,7 @@
 
         fp.close()
 
-        expected_data = [{u'release_version': [u'CMSSW_1_2_3']}]
+        expected_data = [{'release_version': ['CMSSW_1_2_3']}]
         versions = self.api.listReleaseVersions(release_version="CMSSW_1_2_3")
 
         self.assertEqual(expected_data, versions)
--- ./Client/tests/dbsclient_t/unittests/DBSClientBlockWriter_t.py	(original)
+++ ./Client/tests/dbsclient_t/unittests/DBSClientBlockWriter_t.py	(refactored)
@@ -1,7 +1,7 @@
 """
 client writer unittests
 """
-from __future__ import print_function
+
 import os, sys, imp
 import time
 import uuid
--- ./Client/tests/dbsclient_t/unittests/DBSClientReader_t.py	(original)
+++ ./Client/tests/dbsclient_t/unittests/DBSClientReader_t.py	(refactored)RefactoringTool: Refactored ./Client/tests/dbsclient_t/unittests/DBSClientWriter_t.py

@@ -1,7 +1,7 @@
 """
 web unittests
 """
-from __future__ import print_function
+
 import imp
 import os
 import re
@@ -1012,12 +1012,12 @@
 
     def test043a(self):
         """test043a unittestDBSClientReader_t.listFileParents with list of logical_file_name"""
-        file_list = [self.testparams['files'][0] for i in xrange(200)]
+        file_list = [self.testparams['files'][0] for i in range(200)]
         self.api.listFileParents(logical_file_name=file_list)
 
     def test043b(self):
         """test043b unittestDBSClientReader_t.listFileParents with non splitable parameter"""
-        file_list = [self.testparams['files'][0] for i in xrange(200)]
+        file_list = [self.testparams['files'][0] for i in range(200)]
         self.assertRaises(dbsClientException, self.api.listFileParents, logical_file_names=file_list)
 
     def test044(self):
@@ -1065,27 +1065,27 @@
 
     def test046a1(self):
         """test046a1 unittestDBSClientReader_t.listFileLumiArray with list of logical_file_name"""
-        file_list = [self.testparams['files'][i] for i in xrange(5)]
+        file_list = [self.testparams['files'][i] for i in range(5)]
         self.api.listFileLumiArray(logical_file_name=file_list, validFileOnly=0)
 
     def test046a2(self):
         """test046a2 unittestDBSClientReader_t.listFileLumiArray with list of logical_file_name"""
-        file_list = [self.testparams['files'][i] for i in xrange(5)]
+        file_list = [self.testparams['files'][i] for i in range(5)]
         self.api.listFileLumiArray(logical_file_name=file_list, validFileOnly=1)
 
     def test046a3(self):
         """ test046a3 unittestDBSClientReader_t.listFileLumiArray with list of logical_file_name"""
-        file_list = [self.testparams['files'][i] for i in xrange(5)]
+        file_list = [self.testparams['files'][i] for i in range(5)]
         self.api.listFileLumiArray(logical_file_name=file_list, run_num=[self.testparams['runs'][0]], validFileOnly=0)
 
     def test046a4(self):
         """ test046a4:  unittestDBSClientReader_t.listFileLumiArray with list of logical_file_name"""
-        file_list = [self.testparams['files'][i] for i in xrange(5)]
+        file_list = [self.testparams['files'][i] for i in range(5)]
         self.api.listFileLumiArray(logical_file_name=file_list, run_num=self.testparams['runs'][0], validFileOnly=1)
 
     def test046aa4(self):
         """ test046aa4:  unittestDBSClientReader_t.listFileLumiArray with list of logical_file_name"""
-        file_list = [self.testparams['files'][i] for i in xrange(5)]
+        file_list = [self.testparams['files'][i] for i in range(5)]
 	try:
             self.api.listFileLumiArray(logical_file_name=file_list, run_num=[self.testparams['runs'][0], self.testparams['runs'][1]],  validFileOnly=1)
 	except:
@@ -1282,12 +1282,12 @@
 
     def test076a(self):
         """test76a unittestDBSClientReader_t.listFileChildren with list of logical_file_name"""
-        file_list = [self.testparams['parent_files'][0] for i in xrange(200)]
+        file_list = [self.testparams['parent_files'][0] for i in range(200)]
         self.api.listFileChildren(logical_file_name=file_list)
 
     def test076b(self):
         """test76b unittestDBSClientReader_t.listFileChildren with non splitable parameter"""
-        file_list = [self.testparams['parent_files'][0] for i in xrange(200)]
+        file_list = [self.testparams['parent_files'][0] for i in range(200)]
         self.assertRaises(dbsClientException, self.api.listFileChildren, logical_file_names=file_list)
 
     def test076c(self):
--- ./Client/tests/dbsclient_t/unittests/DBSClientWriter_t.py	(original)
+++ ./Client/tests/dbsclient_t/unittests/DBSClientWriter_t.py	(refactored)
@@ -1,7 +1,7 @@
 """
 client writer unittests
 """
-from __future__ import print_function
+
 import os
 import sys
 import time
@@ -204,43 +204,43 @@
         flist=[]
         for i in range(10):
             f={
-                'adler32': u'NOTSET', 'file_type': 'EDM',
+                'adler32': 'NOTSET', 'file_type': 'EDM',
                 'file_output_config_list':
                 [
                     {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
                      'output_module_label': output_module_label,'global_tag':global_tag },
                     ],
                 'dataset': parent_dataset,
-                'file_size': u'2012211901', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
                 'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': u'97', 'event_count': u'66'},
-                    {'lumi_section_num': u'26422', 'run_num': u'97', 'event_count': u'67'},
-                    {'lumi_section_num': u'29838', 'run_num': u'97', 'event_count': u'68'},
-                    {'lumi_section_num': u'248', 'run_num': u'97',   'event_count': u'69'},
-                    {'lumi_section_num': u'250', 'run_num': u'97', 'event_count': u'70'},
-                    {'lumi_section_num': u'300', 'run_num': u'97', 'event_count': u'71'},
-                    {'lumi_section_num': u'534', 'run_num': u'97', 'event_count': u'72'},
-                    {'lumi_section_num': u'546', 'run_num': u'97', 'event_count': u'73'},
-                    {'lumi_section_num': u'638', 'run_num': u'97', 'event_count': u'74'},
-                    {'lumi_section_num': u'650', 'run_num': u'97', 'event_count': u'75'},
-                    {'lumi_section_num': u'794', 'run_num': u'97', 'event_count': u'76'},
-                    {'lumi_section_num': u'1313', 'run_num': u'97', 'event_count': u'77'},
-                    {'lumi_section_num': u'1327', 'run_num': u'97', 'event_count': u'78'},
-                    {'lumi_section_num': u'1339', 'run_num': u'97', 'event_count': u'79'},
-                    {'lumi_section_num': u'1353', 'run_num': u'97', 'event_count': u'80'},
-                    {'lumi_section_num': u'1428', 'run_num': u'97', 'event_count': u'81'},
-                    {'lumi_section_num': u'1496', 'run_num': u'97', 'event_count': u'82'},
-                    {'lumi_section_num': u'1537', 'run_num': u'97', 'event_count': u'83'},
-                    {'lumi_section_num': u'1652', 'run_num': u'97', 'event_count': u'84'},
-                    {'lumi_section_num': u'1664', 'run_num': u'97', 'event_count': u'85'},
-                    {'lumi_section_num': u'1743', 'run_num': u'97', 'event_count': u'86'},
-                    {'lumi_section_num': u'1755', 'run_num': u'97', 'event_count': u'87'},
-                    {'lumi_section_num': u'1860', 'run_num': u'97', 'event_count': u'88'},
-                    {'lumi_section_num': u'1872', 'run_num': u'97', 'event_count': u'89'}
+                    {'lumi_section_num': '27414', 'run_num': '97', 'event_count': '66'},
+                    {'lumi_section_num': '26422', 'run_num': '97', 'event_count': '67'},
+                    {'lumi_section_num': '29838', 'run_num': '97', 'event_count': '68'},
+                    {'lumi_section_num': '248', 'run_num': '97',   'event_count': '69'},
+                    {'lumi_section_num': '250', 'run_num': '97', 'event_count': '70'},
+                    {'lumi_section_num': '300', 'run_num': '97', 'event_count': '71'},
+                    {'lumi_section_num': '534', 'run_num': '97', 'event_count': '72'},
+                    {'lumi_section_num': '546', 'run_num': '97', 'event_count': '73'},
+                    {'lumi_section_num': '638', 'run_num': '97', 'event_count': '74'},
+                    {'lumi_section_num': '650', 'run_num': '97', 'event_count': '75'},
+                    {'lumi_section_num': '794', 'run_num': '97', 'event_count': '76'},
+                    {'lumi_section_num': '1313', 'run_num': '97', 'event_count': '77'},
+                    {'lumi_section_num': '1327', 'run_num': '97', 'event_count': '78'},
+                    {'lumi_section_num': '1339', 'run_num': '97', 'event_count': '79'},
+                    {'lumi_section_num': '1353', 'run_num': '97', 'event_count': '80'},RefactoringTool: No changes to ./Client/tests/dbsclient_t/unittests/__init__.py
RefactoringTool: Refactored ./Client/tests/dbsclient_t/utils/DBSDataProvider.py
RefactoringTool: No changes to ./Client/tests/dbsclient_t/utils/__init__.py
RefactoringTool: Refactored ./Client/tests/dbsclient_t/utils/timeout.py
RefactoringTool: Refactored ./Client/tests/dbsclient_t/validation/DBSValidation_t.py

+                    {'lumi_section_num': '1428', 'run_num': '97', 'event_count': '81'},
+                    {'lumi_section_num': '1496', 'run_num': '97', 'event_count': '82'},
+                    {'lumi_section_num': '1537', 'run_num': '97', 'event_count': '83'},
+                    {'lumi_section_num': '1652', 'run_num': '97', 'event_count': '84'},
+                    {'lumi_section_num': '1664', 'run_num': '97', 'event_count': '85'},
+                    {'lumi_section_num': '1743', 'run_num': '97', 'event_count': '86'},
+                    {'lumi_section_num': '1755', 'run_num': '97', 'event_count': '87'},
+                    {'lumi_section_num': '1860', 'run_num': '97', 'event_count': '88'},
+                    {'lumi_section_num': '1872', 'run_num': '97', 'event_count': '89'}
                     ],
                 'file_parent_list': [ ],
-                'event_count': u'1619',
+                'event_count': '1619',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/p%s/%i.root" %(uid, i),
                 'block_name': parent_block
                 #'is_file_valid': 1
@@ -254,22 +254,22 @@
         flist=[]
         for i in range(10):
             f={
-                'adler32': u'NOTSET', 'file_type': 'EDM',
+                'adler32': 'NOTSET', 'file_type': 'EDM',
                 'file_output_config_list':
                 [
                     {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
                      'output_module_label': output_module_label, 'global_tag':global_tag},
                     ],
                 'dataset': dataset,
-                'file_size': u'2012211901', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
                 'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': u'97'},
-                    {'lumi_section_num': u'26422', 'run_num': u'98'},
-                    {'lumi_section_num': u'29838', 'run_num': u'99'}
+                    {'lumi_section_num': '27414', 'run_num': '97'},
+                    {'lumi_section_num': '26422', 'run_num': '98'},
+                    {'lumi_section_num': '29838', 'run_num': '99'}
                     ],
                 'file_parent_list': [ {"file_parent_lfn" : "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/p%s/%i.root" %(uid, i)} ],
-                'event_count': u'1619',
+                'event_count': '1619',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uid, i),
                 'block_name': block
                 #'is_file_valid': 1
@@ -283,22 +283,22 @@
         flist=[]
         for i in range(10):
             f={
-                'adler32': u'NOTSET', 'file_type': 'EDM',
+                'adler32': 'NOTSET', 'file_type': 'EDM',
                 'file_output_config_list':
                 [
                     {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
                      'output_module_label': output_module_label, 'global_tag':global_tag},
                     ],
                 'dataset': dataset,
-                'file_size': u'2012211901', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
                 'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': u'97'},
-                    {'lumi_section_num': u'26422', 'run_num': u'98'},
-                    {'lumi_section_num': u'29838', 'run_num': u'99'}
+                    {'lumi_section_num': '27414', 'run_num': '97'},
+                    {'lumi_section_num': '26422', 'run_num': '98'},
+                    {'lumi_section_num': '29838', 'run_num': '99'}
                     ],
                 'file_parent_list': [ {"file_parent_lfn" : "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/p%s/%i.root" %(uid, i)} ],
-                'event_count': u'1619',
+                'event_count': '1619',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uid, i),
                 'block_name': block
                 #'is_file_valid': 1
@@ -385,20 +385,20 @@
         cflist=[]
         for i in range(fCount):
             f={
-                'adler32': u'NOTSET', 'file_type': 'EDM',
+                'adler32': 'NOTSET', 'file_type': 'EDM',
                 #'file_output_config_list':
                 #[
                 #    {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
                 #     'output_module_label': output_module_label,'global_tag':global_tag },
                 #    ],
-                'file_size': u'2012211901', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
                 'file_lumi_list': [
                     {'lumi_section_num': 27414+i, 'run_num': 98, 'event_count': 66},
                     {'lumi_section_num': 26422+i, 'run_num': 98, 'event_count': 67},
                     {'lumi_section_num': 29838+i, 'run_num': 98, 'event_count': 68},
                     ],
-                'event_count': u'201',
+                'event_count': '201',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/StepChain_/p%s/%i.root" %(uid, i),
                 #'is_file_valid': 1
                 }
--- ./Client/tests/dbsclient_t/utils/DBSDataProvider.py	(original)
+++ ./Client/tests/dbsclient_t/utils/DBSDataProvider.py	(refactored)
@@ -1,4 +1,4 @@
-import cPickle as pickle
+import pickle as pickle
 import random
 import uuid
 
@@ -70,7 +70,7 @@
     def files(self, block_name):
         if not (hasattr(self, '_files') and block_name in self._files):
             self._files[block_name] = []
-            for i in xrange(self._num_of_files):
+            for i in range(self._num_of_files):
                 logical_file_name = self._generate_file_name(i)
                 self._files[block_name].append({'check_sum' : self._generate_cksum(),
                                                 'file_size' : self._generate_file_size(),
@@ -138,7 +138,7 @@
     def _generate_file_lumi_list(self):
         "generate file lumi list for a given file, if not already available"
         output = []
-        for _ in xrange(0, self._num_of_runs):
+        for _ in range(0, self._num_of_runs):
             self._run_num += 1
             for _ in range(0, self._num_of_lumis):
                 self._lumi_sec += 1
@@ -174,7 +174,7 @@
         "return list of blocks"
         if not hasattr(self, '_blocks'):
             self._blocks = []
-            for i in xrange(self._num_of_blocks):
+            for i in range(self._num_of_blocks):
                 self._blocks.append(self._generate_block_name())
         return self._blocks
 
--- ./Client/tests/dbsclient_t/utils/timeout.py	(original)
+++ ./Client/tests/dbsclient_t/utils/timeout.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 from time import sleep
 import signal
 
--- ./Client/tests/dbsclient_t/validation/DBSValidation_t.py	(original)
+++ ./Client/tests/dbsclient_t/validation/DBSValidation_t.py	(refactored)
@@ -2,7 +2,7 @@
 DBS3 Validation tests
 These tests write and then immediately reads back the data from DBS3 and validate
 """
-from __future__ import print_function
+
 from random import choice
 import os
 import re
@@ -42,7 +42,7 @@
 def remove_non_comparable_keys(values, non_comparable_keys):
     for value in values:
         if isinstance(value, dict):
-            keys = set(value.iterkeys())
+            keys = set(value.keys())
             intersection = keys.intersection(set(non_comparable_keys))
             for entry in intersection:
                 del value[entry]
@@ -209,17 +209,17 @@
RefactoringTool: No changes to ./Client/tests/dbsclient_t/validation/__init__.py
RefactoringTool: No changes to ./Client/tests/profiling_t/DBSProfiling_t.py
RefactoringTool: Refactored ./Client/utils/compareInstance.py
RefactoringTool: Refactored ./Client/utils/dbs2Todbs3DatasetMigrate.py
RefactoringTool: Refactored ./Client/utils/dbs3_block_dump_comparison.py
RefactoringTool: Refactored ./Client/utils/dbs3_bulk_block_insert.py
         pflist=[]
         for i in range(10):
             f={
-                'adler32': u'NOTSET', 'file_type': 'EDM',
+                'adler32': 'NOTSET', 'file_type': 'EDM',
                 'dataset': dataset_parent,
-                'file_size': u'201221191', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
-                'event_count': u'1619',
+                'file_size': '201221191', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'event_count': '1619',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/parent_%i.root" %(uid, i),
                 'block_name': block_parent,
                 'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': u'1'},
-                    {'lumi_section_num': u'26422', 'run_num': u'1'},
-                    {'lumi_section_num': u'29838', 'run_num': u'1'}
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
                     ]
                 }
             pflist.append(f)
@@ -227,22 +227,22 @@
         #### This next block of test will now actually insert the files in the "test 'block' in this module, using the upper files as parent
         for i in range(10):
             f={
-                'adler32': u'NOTSET', 'file_type': 'EDM',
+                'adler32': 'NOTSET', 'file_type': 'EDM',
                 'file_output_config_list':
                 [
                     {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
                      'output_module_label': output_module_label, 'global_tag': global_tag},
                     ],
                 'dataset': dataset,
-                'file_size': u'201221191', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
+                'file_size': '201221191', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
                 'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': u'1'},
-                    {'lumi_section_num': u'26422', 'run_num': u'1'},
-                    {'lumi_section_num': u'29838', 'run_num': u'1'}
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
                     ],
                 'file_parent_list': [ {"file_parent_lfn" : "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/parent_%i.root" %(uid, i)} ],
-                'event_count': u'1619',
+                'event_count': '1619',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uid, i),
                 'block_name': block
                 #'is_file_valid': 1
@@ -265,11 +265,11 @@
         flParentList=self.api.listFileParents(logical_file_name=logical_file_name)
         self.assertEqual(len(flParentList), 1)
         self.assertEqual(flParentList[0]['parent_logical_file_name'][0], "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/parent_%i.root" %(uid, 0))
-        logical_file_names = ["/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uid, i) for i in xrange(10)]
+        logical_file_names = ["/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uid, i) for i in range(10)]
         flParentList=self.api.listFileParents(logical_file_name=logical_file_names)
         self.assertEqual(len(flParentList), 10)
-        self.assertEqual(sorted((flParentList[i]['parent_logical_file_name'][0] for i in xrange(10))),
-                         sorted(("/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/parent_%i.root" % (uid, i)) for i in xrange(10)))
+        self.assertEqual(sorted((flParentList[i]['parent_logical_file_name'][0] for i in range(10))),
+                         sorted(("/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/parent_%i.root" % (uid, i)) for i in range(10)))
         # Get the dataset parent -- due to fact that files had parents, dataset parentage is also inserted
         dsParentList=self.api.listDatasetParents(dataset=dataset)
         self.assertEqual(len(dsParentList), 1)
@@ -318,7 +318,7 @@
         block_dump = self.api.blockDump(block_name=input_block_dump['block']['block_name'])
         def check(input, output):
             if isinstance(input, dict):
-                for key, value in input.iteritems():
+                for key, value in input.items():
                     if key == "processing_era":
                         print("------input value----")
                         print(value)
@@ -377,7 +377,7 @@
             non_comparable_keys = ('block_id', 'dataset_id', 'last_modification_date',
                                    'parent_file_id', 'primary_ds_id')
             if isinstance(input, dict):
-                for key, value in input.iteritems():
+                for key, value in input.items():
                     if key in non_comparable_keys:
                         continue ###do not compare id's
                     if key in ('processing_era',): ###do compare create_by, creation_date for re-used entries
@@ -438,7 +438,7 @@
             non_comparable_keys = ('block_id', 'dataset_id', 'last_modification_date',
                                    'parent_file_id', 'primary_ds_id')
             if isinstance(input, dict):
-                for key, value in input.iteritems():
+                for key, value in input.items():
                     if key in non_comparable_keys:
                         continue ###do not compare id's
                     if key in ('processing_era',): ###do compare create_by, creation_date for re-used entries
--- ./Client/utils/compareInstance.py	(original)
+++ ./Client/utils/compareInstance.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import sys
 import pprint
 from time import sleep
--- ./Client/utils/dbs2Todbs3DatasetMigrate.py	(original)
+++ ./Client/utils/dbs2Todbs3DatasetMigrate.py	(refactored)
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 #
-from __future__ import print_function
+
 import sys
 import time
 #DBS-2 imports
@@ -207,17 +207,17 @@
 					    last=i
 					end_time=time.time()
 					block_time['TimeSpent']=end_time-start_time
-					block_time['block_weight']=long(len(self.files))
-					block_time['file_count']=long(len(self.files))
+					block_time['block_weight']=int(len(self.files))
+					block_time['file_count']=int(len(self.files))
 					block_time['file_lumi_section_count']=0
 					block_time['file_parent_count']=0
 					for file in self.files:
 						if 'file_lumi_list' in file:
-							block_time['block_weight']+=long(len(file['file_lumi_list']))
-							block_time['file_lumi_section_count']+=long(len(file['file_lumi_list']))
+							block_time['block_weight']+=int(len(file['file_lumi_list']))
+							block_time['file_lumi_section_count']+=int(len(file['file_lumi_list']))
 						if 'file_parent_list' in file:
-							block_time['block_weight']+=long(len(file['file_parent_list']))
-							block_time['file_parent_count']+=long(len(file['file_parent_list']))
+							block_time['block_weight']+=int(len(file['file_parent_list']))
+							block_time['file_parent_count']+=int(len(file['file_parent_list']))
 					#print "fin"
 				except Exception as ex:
 					print(ex)
--- ./Client/utils/dbs3_block_dump_comparison.py	(original)
+++ ./Client/utils/dbs3_block_dump_comparison.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from optparse import OptionParser
 
 from dbs.apis.dbsClient import DbsApi
--- ./Client/utils/dbs3_bulk_block_insert.py	(original)
+++ ./Client/utils/dbs3_bulk_block_insert.py	(refactored)
@@ -2,7 +2,7 @@
 """
 Script to insert bulk blocks into DBS 3
 """
-from __future__ import print_function
+
 from optparse import OptionParser
 from ast import literal_eval
 import glob
RefactoringTool: Refactored ./Client/utils/dbs3_bulk_block_insert2.py
RefactoringTool: Refactored ./Client/utils/getserverinfo.py
RefactoringTool: Refactored ./Client/utils/insertAcqEra.py
RefactoringTool: Refactored ./Client/utils/insertBlockBulk.py
RefactoringTool: Refactored ./Client/utils/insert_block_migration_requests-MINIAOD.py
RefactoringTool: Refactored ./Client/utils/insertblock.py
RefactoringTool: Refactored ./Client/utils/insertdataset.py
RefactoringTool: Refactored ./Client/utils/insertdatatier.py
RefactoringTool: Refactored ./Client/utils/insertfile.py
RefactoringTool: Refactored ./Client/utils/insertprimary.py
RefactoringTool: Refactored ./Client/utils/insertprocessingVersion.py
RefactoringTool: Refactored ./Client/utils/listBlockSummaries.py
RefactoringTool: Refactored ./Client/utils/listBlockTrio.py
RefactoringTool: Refactored ./Client/utils/listFileArray.py
RefactoringTool: Refactored ./Client/utils/listFileLumis.py
RefactoringTool: Refactored ./Client/utils/listFileParentsByLumi.py
RefactoringTool: Refactored ./Client/utils/listFileSummaries.py
RefactoringTool: Refactored ./Client/utils/listFileTests.py
RefactoringTool: Refactored ./Client/utils/listFileinBlock.py
RefactoringTool: Refactored ./Client/utils/listParentDSTrio.py
RefactoringTool: Refactored ./Client/utils/listblock.py
--- ./Client/utils/dbs3_bulk_block_insert2.py	(original)
+++ ./Client/utils/dbs3_bulk_block_insert2.py	(refactored)
@@ -2,7 +2,7 @@
 """
 Script to insert bulk blocks into DBS 3
 """
-from __future__ import print_function
+
 from optparse import OptionParser
 import glob
 import json
--- ./Client/utils/getserverinfo.py	(original)
+++ ./Client/utils/getserverinfo.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 #url="http://cmssrv48.fnal.gov:8989/DBSServlet"
--- ./Client/utils/insertAcqEra.py	(original)
+++ ./Client/utils/insertAcqEra.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 import os
--- ./Client/utils/insertBlockBulk.py	(original)
+++ ./Client/utils/insertBlockBulk.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 ## This is a simple example to load a block dump file into DBS3.
 ## One can use blockdump.dict as an example to see what is the basic requirement to load
 ## a block into DBS3.
--- ./Client/utils/insert_block_migration_requests-MINIAOD.py	(original)
+++ ./Client/utils/insert_block_migration_requests-MINIAOD.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from dbs.apis.dbsClient import DbsApi
 """
 This is a test for migration server to migrate mutiple blocks at the same time and these blocks share 
--- ./Client/utils/insertblock.py	(original)
+++ ./Client/utils/insertblock.py	(refactored)
@@ -1,11 +1,11 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="http://cmssrv18.fnal.gov:8585/dbs3"
 # API Object    
 dbs3api = DbsApi(url=url)
 block={'site_list' : ['cmsdcache.pi.infn.it'], 'block_name': '/anzar22/Summer09-STARTUP31X_V3-v1/GEN-SIM-DIGI-RAW#1237f9d-d44e-45a6-b87b-8fe6e27011162', 'file_count': '500', 'origin_site': 'cmssrm.fnal.gov', 'last_modification_date': '1255099739', 'create_by': 'cmsprod@caraway.hep.wisc.edu', 'block_size': '1436055868219', 'open_for_writing': 1, 'last_modified_by': '/DC=org/DC=doegrids/OU=People/CN=Ajit Kumar Mohapatra 867118', 'creation_date': '1255020532'}
-print(block.keys())
+print(list(block.keys()))
 try:
     dbs3api.insertBlock(block)
 except Exception as e:
--- ./Client/utils/insertdataset.py	(original)
+++ ./Client/utils/insertdataset.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 import os
@@ -15,7 +15,7 @@
 dataset.update({'dataset' : '/%s/%s/%s' %(dataset['primary_ds_name'], dataset['processed_ds_name'],
                 dataset['data_tier_name'])})
 
-print(dataset.keys())
+print(list(dataset.keys()))
 
 print(dbs3api.insertDataset(dataset))
 # Is service Alive
--- ./Client/utils/insertdatatier.py	(original)
+++ ./Client/utils/insertdatatier.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 #url="http://cmssrv18.fnal.gov:8585/dbs3"
--- ./Client/utils/insertfile.py	(original)
+++ ./Client/utils/insertfile.py	(refactored)
@@ -1,11 +1,11 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 #url="http://cmssrv48.fnal.gov:8989/DBSServlet"
 url="http://cmssrv18.fnal.gov:8585/dbs3"
 # API Object    
 dbs3api = DbsApi(url=url)
-file={'FILE_TYPE': 'EDM', 'LOGICAL_FILE_NAME': '/store100077/mc/Summer09/TkCosmics38T/GEN-SIM-DIGI-RAW/STARTUP31X_V3-v1/0010/66EE7132-FFB3-DE11-9D33-001E682F1FA6.root', 'FILE_SIZE': '2824329131', 'LAST_MODIFICATION_DATE': '1255099729', 'FILE_PARENT_LIST': [], 'AUTO_CROSS_SECTION': 0.0, 'MD5': 'NOTSET', 'CHECK_SUM': '862355611', 'FILE_LUMI_LIST': [{'LUMI_SECTION_NUM': u'10018', 'RUN_NUM': '1'}], 'ADLER32': 'NOTSET', 'EVENT_COUNT': '2041', 'CREATE_BY': 'cmsprod@caraway.hep.wisc.edu', 'LAST_MODIFIED_BY': '/DC=org/DC=doegrids/OU=People/CN=Ajit Kumar Mohapatra 867118', 'DATASET': '/TkCosmics38T/Summer09-STARTUP31X_V3-v1/GEN-SIM-DIGI-RAW', 'BLOCK': '/TkCosmics38T/Summer09-STARTUP31X_V3-v1/GEN-SIM-DIGI-RAW#fc31bf9d-d44e-45a6-b87b-8fe6e2701062', 'IS_FILE_VALID': 1}
+file={'FILE_TYPE': 'EDM', 'LOGICAL_FILE_NAME': '/store100077/mc/Summer09/TkCosmics38T/GEN-SIM-DIGI-RAW/STARTUP31X_V3-v1/0010/66EE7132-FFB3-DE11-9D33-001E682F1FA6.root', 'FILE_SIZE': '2824329131', 'LAST_MODIFICATION_DATE': '1255099729', 'FILE_PARENT_LIST': [], 'AUTO_CROSS_SECTION': 0.0, 'MD5': 'NOTSET', 'CHECK_SUM': '862355611', 'FILE_LUMI_LIST': [{'LUMI_SECTION_NUM': '10018', 'RUN_NUM': '1'}], 'ADLER32': 'NOTSET', 'EVENT_COUNT': '2041', 'CREATE_BY': 'cmsprod@caraway.hep.wisc.edu', 'LAST_MODIFIED_BY': '/DC=org/DC=doegrids/OU=People/CN=Ajit Kumar Mohapatra 867118', 'DATASET': '/TkCosmics38T/Summer09-STARTUP31X_V3-v1/GEN-SIM-DIGI-RAW', 'BLOCK': '/TkCosmics38T/Summer09-STARTUP31X_V3-v1/GEN-SIM-DIGI-RAW#fc31bf9d-d44e-45a6-b87b-8fe6e2701062', 'IS_FILE_VALID': 1}
 
 print(dbs3api.insertFiles([file]))
 
--- ./Client/utils/insertprimary.py	(original)
+++ ./Client/utils/insertprimary.py	(refactored)
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 #DBS-3 imports
-from __future__ import print_function
+
 from dbs.apis.dbsClient import *
 #url="http://cmssrv48.fnal.gov:8989/DBSServlet"
 #url="http://cmssrv18.fnal.gov:8585/dbs3"
--- ./Client/utils/insertprocessingVersion.py	(original)
+++ ./Client/utils/insertprocessingVersion.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 import os
--- ./Client/utils/listBlockSummaries.py	(original)
+++ ./Client/utils/listBlockSummaries.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="https://dbs3-test1.cern.ch/dbs/dev/global/DBSReader"
--- ./Client/utils/listBlockTrio.py	(original)
+++ ./Client/utils/listBlockTrio.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 import time
@@ -15,7 +15,7 @@
     for i in dbs3api.listBlockTrio(block_name=block):
         #print(i)
         t1 += 1
-        for k, v in i.items():
+        for k, v in list(i.items()):
             t2 += len(v)        
     print(t1)
     print(t2)
--- ./Client/utils/listFileArray.py	(original)
+++ ./Client/utils/listFileArray.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 import time
 from dbs.apis.dbsClient import *
--- ./Client/utils/listFileLumis.py	(original)
+++ ./Client/utils/listFileLumis.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 import time
 from dbs.apis.dbsClient import *
--- ./Client/utils/listFileParentsByLumi.py	(original)
+++ ./Client/utils/listFileParentsByLumi.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="https://cmsweb.cern.ch:8443/dbs/prod/global/DBSReader/"
--- ./Client/utils/listFileSummaries.py	(original)
+++ ./Client/utils/listFileSummaries.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 from dbs.apis.dbsClient import *
 url="https://cmsweb.cern.ch/dbs/prod/global/DBSReader/"
 #url="https://dbs3-test2.cern.ch/dbs/dev/global/DBSReader/"
--- ./Client/utils/listFileTests.py	(original)
+++ ./Client/utils/listFileTests.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 import time
 from dbs.apis.dbsClient import *
--- ./Client/utils/listFileinBlock.py	(original)
+++ ./Client/utils/listFileinBlock.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import sys
 import time
 
--- ./Client/utils/listParentDSTrio.py	(original)
+++ ./Client/utils/listParentDSTrio.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 import time
@@ -15,7 +15,7 @@
     for i in dbs3api.listParentDSTrio(dataset=dataset):
         #print(i)
         t1 += 1
-        for k, v in i.items():
+        for k, v in list(i.items()):
             t2 += len(v)
     print(t1)
     print(t2)
--- ./Client/utils/listblock.py	(original)
+++ ./Client/utils/listblock.py	(refactored)RefactoringTool: Refactored ./Client/utils/listblockchildren.py
RefactoringTool: Refactored ./Client/utils/listblockparents.py
RefactoringTool: Refactored ./Client/utils/listdataset.py
RefactoringTool: Refactored ./Client/utils/listdatasetchildren.py
RefactoringTool: Refactored ./Client/utils/listdatasetparents.py
RefactoringTool: Refactored ./Client/utils/listdatatier.py
RefactoringTool: Refactored ./Client/utils/listfile.py
RefactoringTool: Refactored ./Client/utils/listfilechildren.py
RefactoringTool: Refactored ./Client/utils/listfileparents.py
RefactoringTool: Refactored ./Client/utils/listfiles.py
RefactoringTool: Refactored ./Client/utils/listprimary.py
RefactoringTool: Refactored ./Client/utils/listsites.py
RefactoringTool: Refactored ./Client/utils/migrate.py
RefactoringTool: Refactored ./Client/utils/readTestLogs.py
RefactoringTool: Refactored ./Client/utils/run_test.py
RefactoringTool: Refactored ./Client/utils/updateDatasetType.py
RefactoringTool: No changes to ./Client/utils/updateFileStatus.py
RefactoringTool: No changes to ./Client/utils/updateblock.py
RefactoringTool: No changes to ./Client/utils/DataOpsScripts/DBS3SetDatasetStatus.py
RefactoringTool: No changes to ./Client/utils/DataOpsScripts/DBS3SetFileStatus.py
RefactoringTool: Refactored ./Client/utils/DataOpsScripts/EventsPerDay.py
RefactoringTool: No changes to ./Client/utils/UserScripts/DBS3ListRunLumiInfo.py
RefactoringTool: Refactored ./DBS2To3Migration/SQL/recreateSequence.py
RefactoringTool: Refactored ./DBS2To3Migration/test/DBSSqlQueries.py
RefactoringTool: Refactored ./DBS2To3Migration/test/ValidateDualDBSWriting.py
RefactoringTool: Refactored ./DBS2To3Migration/test/ValidateMigration_t.py
RefactoringTool: Refactored ./DBS2To3Migration/test/ValidateOriginSiteName_t.py
RefactoringTool: Refactored ./PycurlClient/setup.py
RefactoringTool: No changes to ./PycurlClient/src/python/RestClient/RestApi.py
RefactoringTool: No changes to ./PycurlClient/src/python/RestClient/__init__.py
RefactoringTool: Refactored ./PycurlClient/src/python/RestClient/AuthHandling/BasicAuth.py
RefactoringTool: No changes to ./PycurlClient/src/python/RestClient/AuthHandling/X509Auth.py
RefactoringTool: No changes to ./PycurlClient/src/python/RestClient/AuthHandling/__init__.py
RefactoringTool: No changes to ./PycurlClient/src/python/RestClient/ErrorHandling/RestClientExceptions.py
RefactoringTool: No changes to ./PycurlClient/src/python/RestClient/ErrorHandling/__init__.py
RefactoringTool: Refactored ./PycurlClient/src/python/RestClient/ProxyPlugins/Socks5Proxy.py
RefactoringTool: No changes to ./PycurlClient/src/python/RestClient/ProxyPlugins/__init__.py
RefactoringTool: Refactored ./PycurlClient/src/python/RestClient/RequestHandling/HTTPRequest.py

@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="https://cmsweb-test3.cern.ch/dbs/int/global/DBSReader"
--- ./Client/utils/listblockchildren.py	(original)
+++ ./Client/utils/listblockchildren.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="http://cmssrv18.fnal.gov:8585/dbs3"
--- ./Client/utils/listblockparents.py	(original)
+++ ./Client/utils/listblockparents.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="http://cmssrv18.fnal.gov:8585/dbs3"
--- ./Client/utils/listdataset.py	(original)
+++ ./Client/utils/listdataset.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 #url="https://cmsweb-testbed.cern.ch:8443/dbs/int/global/DBSReader/"
--- ./Client/utils/listdatasetchildren.py	(original)
+++ ./Client/utils/listdatasetchildren.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="http://cmssrv18.fnal.gov:8585/dbs3"
--- ./Client/utils/listdatasetparents.py	(original)
+++ ./Client/utils/listdatasetparents.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="http://cmssrv18.fnal.gov:8585/dbs3"
--- ./Client/utils/listdatatier.py	(original)
+++ ./Client/utils/listdatatier.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="http://cmssrv18.fnal.gov:8585/dbs3"
--- ./Client/utils/listfile.py	(original)
+++ ./Client/utils/listfile.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 import time
 from dbs.apis.dbsClient import *
--- ./Client/utils/listfilechildren.py	(original)
+++ ./Client/utils/listfilechildren.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="http://cmssrv18.fnal.gov:8585/dbs3"
--- ./Client/utils/listfileparents.py	(original)
+++ ./Client/utils/listfileparents.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="https://cmsweb-testbed.cern.ch:8443/dbs/int/global/DBSReader/"
--- ./Client/utils/listfiles.py	(original)
+++ ./Client/utils/listfiles.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 import time
 import pprint
--- ./Client/utils/listprimary.py	(original)
+++ ./Client/utils/listprimary.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 #url="http://cmssrv48.fnal.gov:8989/DBSServlet"
--- ./Client/utils/listsites.py	(original)
+++ ./Client/utils/listsites.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="http://cmssrv18.fnal.gov:8585/dbs3"
--- ./Client/utils/migrate.py	(original)
+++ ./Client/utils/migrate.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #!/usr/bin/env python
 from dbs.apis.dbsClient import DbsApi
 
--- ./Client/utils/readTestLogs.py	(original)
+++ ./Client/utils/readTestLogs.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 import os
 import sys
 
--- ./Client/utils/run_test.py	(original)
+++ ./Client/utils/run_test.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import subprocess
 #
 #subprocess.call("python2.6 test.py", shell=True)
@@ -34,7 +34,7 @@
 			wait = 1
 """
 		
-for item in procs.keys():
+for item in list(procs.keys()):
 	stdout_value = procs[item].communicate()[0]
 	print(stdout_value)
 
--- ./Client/utils/updateDatasetType.py	(original)
+++ ./Client/utils/updateDatasetType.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 #DBS-3 imports
 from dbs.apis.dbsClient import *
 url="https://dbs3-test1.cern.ch/dbs/dev/global/DBSWriter"
--- ./Client/utils/DataOpsScripts/EventsPerDay.py	(original)
+++ ./Client/utils/DataOpsScripts/EventsPerDay.py	(refactored)
@@ -6,7 +6,7 @@
  python EventsPerDay.py -d '/*/*Fall13-POST*/GEN-SIM'
                        
 """ 
-from __future__ import print_function
+
 import  sys, time
 from optparse import OptionParser
 from time import gmtime
--- ./DBS2To3Migration/SQL/recreateSequence.py	(original)
+++ ./DBS2To3Migration/SQL/recreateSequence.py	(refactored)
@@ -85,7 +85,7 @@
     logger = logging.getLogger()
     dbapi = DBApi(logger, connectUrlDBS3, ownerDBS3)
     results = dbapi.getTrig()
-    for t, s in results.iteritems():
+    for t, s in results.items():
         #print '\n **********'
         #print t, s
         primary = dbapi.getPrimaryKey(t)
--- ./DBS2To3Migration/test/DBSSqlQueries.py	(original)
+++ ./DBS2To3Migration/test/DBSSqlQueries.py	(refactored)
@@ -1,7 +1,7 @@
 """
 DBS SQL QUERIES for unittests to validate the DBS2 to DBS3 migration
 """
-from __future__ import print_function
+
 from WMCore.Database.DBFactory import DBFactory
 from WMCore.Database.DBFormatter import DBFormatter
 
--- ./DBS2To3Migration/test/ValidateDualDBSWriting.py	(original)
+++ ./DBS2To3Migration/test/ValidateDualDBSWriting.py	(refactored)
@@ -2,7 +2,7 @@
 """
 This scripts is used to check WMAgent data injected into DBS2 and DBS3 simutanously
 """
-from __future__ import print_function
+
 from optparse import OptionParser
 from string import Template
 import pprint, logging, os, sys, unittest
--- ./DBS2To3Migration/test/ValidateMigration_t.py	(original)
+++ ./DBS2To3Migration/test/ValidateMigration_t.py	(refactored)
@@ -1,7 +1,7 @@
 """
 Unittests to validate the DBS2 to DBS3 migration
 """
-from __future__ import print_function
+
 
 import logging
 import unittest
@@ -63,7 +63,7 @@
             return False
 
 def diffKeys(resultDBS2, resultDBS3):
-    for i in resultDBS3.keys():
+    for i in list(resultDBS3.keys()):
         if i in resultDBS2:
             if resultDBS3[i] == resultDBS2[i]:
                 pass
--- ./DBS2To3Migration/test/ValidateOriginSiteName_t.py	(original)
+++ ./DBS2To3Migration/test/ValidateOriginSiteName_t.py	(refactored)
@@ -1,7 +1,7 @@
 """
 Unittests to validate translation of the origin_site_name between DBS 2 nad DBS 3
 """
-from __future__ import print_function
+
 
 import logging
 import unittest
--- ./PycurlClient/setup.py	(original)
+++ ./PycurlClient/setup.py	(refactored)
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 
-from __future__ import division, print_function
+
 
 import os
 import sys
--- ./PycurlClient/src/python/RestClient/AuthHandling/BasicAuth.py	(original)
+++ ./PycurlClient/src/python/RestClient/AuthHandling/BasicAuth.py	(refactored)
@@ -9,7 +9,7 @@
         self._password = password
 
         if sys.stdin.isatty() and not self._username:
-            self._username = raw_input('User:')
+            self._username = input('User:')
 
         if sys.stdin.isatty() and not self._password:
             self._password = getpass("Password:")
--- ./PycurlClient/src/python/RestClient/ProxyPlugins/Socks5Proxy.py	(original)
+++ ./PycurlClient/src/python/RestClient/ProxyPlugins/Socks5Proxy.py	(refactored)
@@ -1,4 +1,4 @@
-from urlparse import urlparse
+from urllib.parse import urlparse
 
 class Socks5Proxy(object):
     """Socks5 Proxy Plugin for pycurl
--- ./PycurlClient/src/python/RestClient/RequestHandling/HTTPRequest.py	(original)
+++ ./PycurlClient/src/python/RestClient/RequestHandling/HTTPRequest.py	(refactored)
@@ -2,12 +2,12 @@
 from RestClient.ErrorHandling.RestClientExceptions import HTTPError
 
 import pycurl
-import urllib
+import urllib.request, urllib.parse, urllib.error
 
 try:
-    from cStringIO import StringIO
+    from io import StringIO
 except ImportError:
-    import StringIO
+    import io
 
 class HTTPRequest(object):
     supported_methods = {'GET'    : {pycurl.HTTPGET : True},
@@ -28,7 +28,7 @@
         if not params:
             self._curl_options[pycurl.URL] = ("%s/%s") % (url, api)
         else:RefactoringTool: Refactored ./PycurlClient/src/python/RestClient/RequestHandling/HTTPResponse.py
RefactoringTool: No changes to ./PycurlClient/src/python/RestClient/RequestHandling/__init__.py
RefactoringTool: No changes to ./Schema/DDL/generate-schema-deployable.py
RefactoringTool: Refactored ./Schema/SQLTester/genSQLFromListDatasetContents.py
RefactoringTool: Refactored ./Schema/Scripts/generate_dao.py
RefactoringTool: No changes to ./Schema/Scripts/generate_dataobjs.py
RefactoringTool: Refactored ./Schema/Scripts/generate_trigs.py
RefactoringTool: Refactored ./Schema/Scripts/process_sql.py
RefactoringTool: Refactored ./Schema/Scripts/process_sql_4sqlexplorer.py
RefactoringTool: Refactored ./Server/Python/control/DBSConfig.py
RefactoringTool: No changes to ./Server/Python/control/default_config_multiple_service.py
RefactoringTool: No changes to ./Server/Python/control/default_config_single_server.py
RefactoringTool: No changes to ./Server/Python/control/default_config_single_server_reader_writer.py
RefactoringTool: No changes to ./Server/Python/src/dbs/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/business/DBSAcquisitionEra.py
RefactoringTool: Refactored ./Server/Python/src/dbs/business/DBSBlock.py
RefactoringTool: Refactored ./Server/Python/src/dbs/business/DBSBlockInsert.py
RefactoringTool: Refactored ./Server/Python/src/dbs/business/DBSDataTier.py
RefactoringTool: No changes to ./Server/Python/src/dbs/business/DBSDataType.py
RefactoringTool: No changes to ./Server/Python/src/dbs/business/DBSDataset.py
RefactoringTool: Refactored ./Server/Python/src/dbs/business/DBSDatasetAccessType.py
RefactoringTool: No changes to ./Server/Python/src/dbs/business/DBSDoNothing.py
RefactoringTool: Refactored ./Server/Python/src/dbs/business/DBSFile.py

-            self._curl_options[pycurl.URL] = ("%s/%s?%s") % (url, api, urllib.urlencode(params))
+            self._curl_options[pycurl.URL] = ("%s/%s?%s") % (url, api, urllib.parse.urlencode(params))
 
         if method == 'POST':
             self._curl_options[pycurl.POSTFIELDS] = data
@@ -44,10 +44,10 @@
             ### set content-length header to ensure performant cherrypy reads
             request_headers['Content-Length'] = str(content_length)
 
-        self._curl_options[pycurl.HTTPHEADER] = ["%s: %s" % (key, value) for key, value in request_headers.iteritems()]
+        self._curl_options[pycurl.HTTPHEADER] = ["%s: %s" % (key, value) for key, value in request_headers.items()]
 
     def __call__(self, curl_object):
-        for key, value in self._curl_options.iteritems():
+        for key, value in self._curl_options.items():
             curl_object.setopt(key, value)
 
         http_response  = HTTPResponse()
--- ./PycurlClient/src/python/RestClient/RequestHandling/HTTPResponse.py	(original)
+++ ./PycurlClient/src/python/RestClient/RequestHandling/HTTPResponse.py	(refactored)
@@ -1,7 +1,7 @@
 try:
-    from cStringIO import StringIO
+    from io import StringIO
 except ImportError:
-    import StringIO
+    import io
 
 class HTTPResponse(object):
     def __init__(self):
--- ./Schema/SQLTester/genSQLFromListDatasetContents.py	(original)
+++ ./Schema/SQLTester/genSQLFromListDatasetContents.py	(refactored)
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 #
-from __future__ import print_function
+
 import sys
 from DBSAPI.dbsApi import DbsApi
 from DBSAPI.dbsException import *
--- ./Schema/Scripts/generate_dao.py	(original)
+++ ./Schema/Scripts/generate_dao.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import os
 
 def makeMethodNameOld(str):
--- ./Schema/Scripts/generate_trigs.py	(original)
+++ ./Schema/Scripts/generate_trigs.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import os
 lines=open("../DDL/create-oracle-schema.sql", "r").readlines()
 tablefound=0
--- ./Schema/Scripts/process_sql.py	(original)
+++ ./Schema/Scripts/process_sql.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import os
 lines=open("../DDL/create-oracle-schema.sql", "r").readlines()
 tablefound=0
--- ./Schema/Scripts/process_sql_4sqlexplorer.py	(original)
+++ ./Schema/Scripts/process_sql_4sqlexplorer.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import os
 lines=open("../DDL/DBS3-Oracle.sql", "r").readlines()
 tablefound=0
--- ./Server/Python/control/DBSConfig.py	(original)
+++ ./Server/Python/control/DBSConfig.py	(refactored)
@@ -59,7 +59,7 @@
 config.dbs.section_('views')
 config.dbs.admin = 'cmsdbs'
 config.dbs.default_expires = 900
-config.dbs.instances = list(set([i for r in view_mapping[VARIANT].values() for i in r]))
+config.dbs.instances = list(set([i for r in list(view_mapping[VARIANT].values()) for i in r]))
 
 ### Create views for DBSReader, DBSWriter and DBSMigrate
 active = config.dbs.views.section_('active')
--- ./Server/Python/src/dbs/business/DBSBlock.py	(original)
+++ ./Server/Python/src/dbs/business/DBSBlock.py	(refactored)
@@ -190,7 +190,7 @@
             msg = " DBSBlock/listBlockParents. Block_name must be provided as a string or a list. \
                 No wildcards allowed in block_name/s."
             dbsExceptionHandler('dbsException-invalid-input', msg)
-        elif isinstance(block_name, basestring):
+        elif isinstance(block_name, str):
             try:
                 block_name = str(block_name)
                 if '%' in block_name or '*' in block_name:
--- ./Server/Python/src/dbs/business/DBSBlockInsert.py	(original)
+++ ./Server/Python/src/dbs/business/DBSBlockInsert.py	(refactored)
@@ -283,7 +283,7 @@
                 key = (fc['app_name'] + ':' + fc['release_version'] + ':' +
                        fc['pset_hash'] + ':' +
                        fc['output_module_label'] + ':' + fc['global_tag'])
-                if not key in (self.datasetCache['conf']).keys():
+                if not key in list((self.datasetCache['conf']).keys()):
                     #we expect the config is inserted when the dataset is in.
                     if tran:tran.rollback()
                     if conn:conn.close()
--- ./Server/Python/src/dbs/business/DBSDataTier.py	(original)
+++ ./Server/Python/src/dbs/business/DBSDataTier.py	(refactored)
@@ -26,7 +26,7 @@
         """
         List data tier(s)
         """
-        if not isinstance(data_tier_name, basestring) :
+        if not isinstance(data_tier_name, str) :
             dbsExceptionHandler('dbsException-invalid-input',
                                 'data_tier_name given is not valid : %s' % data_tier_name)
         else:
--- ./Server/Python/src/dbs/business/DBSDatasetAccessType.py	(original)
+++ ./Server/Python/src/dbs/business/DBSDatasetAccessType.py	(refactored)
@@ -25,7 +25,7 @@
         """
         List dataset access types
         """
-        if isinstance(dataset_access_type, basestring):
+        if isinstance(dataset_access_type, str):
             try:
                 dataset_access_type = str(dataset_access_type)
             except:    
@@ -39,7 +39,7 @@
             if plist:
                 t = []
                 for i in plist:
-                    for k, v in i.iteritems():
+                    for k, v in i.items():
                         t.append(v)
                 result[0]['dataset_access_type'] = t
             return result
--- ./Server/Python/src/dbs/business/DBSFile.py	(original)
+++ ./Server/Python/src/dbs/business/DBSFile.py	(refactored)
@@ -3,7 +3,7 @@
 """
 This module provides business object class to interact with File.
 """
-from __future__ import print_function
+
 from WMCore.DAOFactory import DAOFactory
 from dbs.utils.dbsExceptionHandler import dbsExceptionHandler
 from sqlalchemy.exc import IntegrityError as SQLAlchemyIntegrityError
@@ -111,7 +111,7 @@
                 k = i['this_logical_file_name']
                 v = i['parent_logical_file_name']
                 d.setdefault(k, []).append(v)
-            for k, v in d.iteritems():
+            for k, v in d.items():
                 yield {'logical_file_name':k, 'parent_logical_file_name': v}
             del d     
 
@@ -148,7 +148,7 @@
                     d[k].append(v)
                 else:
                     d[k] = [v]
-            for k, v in d.iteritems():
+            for k, v in d.items():
                 r = {'logical_file_name':k, 'child_logical_file_name': v}
                 result.append(r)
             return result
@@ -223,7 +223,7 @@
             if run_num==-1:
                 dbsExceptionHandler('dbsException-invalid-input', "Lumi list must accompany A single run number, \
                         use run_num=123", self.logger.exception)
-            elif isinstance(run_num, basestring):
+            elif isinstance(run_num, str):
                 try:
                     run_num = int(run_num)
                 except:
@@ -493,7 +493,7 @@
                 if not qInserts:
                     blkParams = self.blkstats.execute(conn, block_id,
                                                       transaction=tran)
-                    blkParams['block_size'] = long(blkParams['block_size'])
+                    blkParams['block_size'] = int(blkParams['block_size'])
                     self.blkstatsin.execute(conn, blkParams, transaction=tran)
 
             # All good ?
@@ -526,7 +526,7 @@
         Y. Guo 
         July 18, 2018 
         """
-        if "block_name" not in businput.keys() or "child_parent_id_list" not in businput.keys() or not businput["child_parent_id_list"] or not businput["block_name"]:
+        if "block_name" not in list(businput.keys()) or "child_parent_id_list" not in list(businput.keys()) or not businput["child_parent_id_list"] or not businput["block_name"]:
             dbsExceptionHandler("dbsException-invalid-input2", "DBSFile/insertFileParents: require child block_name and list of child/parent file id pairs" , self.logger.exception, "DBSFile/insertFileParents: require child block_name and list of child/parent file id pairs")RefactoringTool: Refactored ./Server/Python/src/dbs/business/DBSFileBuffer.py
RefactoringTool: Refactored ./Server/Python/src/dbs/business/DBSMigrate.py
RefactoringTool: No changes to ./Server/Python/src/dbs/business/DBSOutputConfig.py
RefactoringTool: Refactored ./Server/Python/src/dbs/business/DBSPhysicsGroup.py
RefactoringTool: No changes to ./Server/Python/src/dbs/business/DBSPrimaryDataset.py
RefactoringTool: No changes to ./Server/Python/src/dbs/business/DBSProcessingEra.py
RefactoringTool: Refactored ./Server/Python/src/dbs/business/DBSReleaseVersion.py
RefactoringTool: No changes to ./Server/Python/src/dbs/business/DBSRun.py
RefactoringTool: No changes to ./Server/Python/src/dbs/business/DBSSite.py
RefactoringTool: No changes to ./Server/Python/src/dbs/business/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/components/__init__.py
RefactoringTool: Refactored ./Server/Python/src/dbs/components/migration/DBSMigrationServer.py
RefactoringTool: No changes to ./Server/Python/src/dbs/components/migration/DefaultConfig.py
RefactoringTool: Refactored ./Server/Python/src/dbs/components/migration/StartUp.py
RefactoringTool: No changes to ./Server/Python/src/dbs/components/migration/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/SequenceManager.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/dbsListPrimaryDatasetTypes.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/dbsListPrimaryDatasets.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/AcquisitionEra/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/AcquisitionEra/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/AcquisitionEra/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/AcquisitionEra/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ApplicationExecutable/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ApplicationExecutable/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ApplicationExecutable/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/AssociatedFile/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/AssociatedFile/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Block/BriefList.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Block/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Block/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Block/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Block/ListStats.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Block/UpdateStats.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Block/UpdateStatus.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Block/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/BlockParent/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/BlockParent/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/BlockParent/ListChild.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/BlockParent/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/BlockSite/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/BlockSite/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/BlockStorageElement/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/BlockStorageElement/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/BranchHashe/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/BranchHashe/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/BranchHashe/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ComponentStatus/DBSStatus.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ComponentStatus/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ComponentStatus/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ComponentStatus/Update.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ComponentStatus/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DataTier/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DataTier/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DataTier/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DataTier/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Dataset/BriefList.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Dataset/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Dataset/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Dataset/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Dataset/UpdateStatus.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Dataset/UpdateType.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Dataset/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetOutputMod_config/GetDSConfigs.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetOutputMod_config/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetOutputMod_config/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetParent/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetParent/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetParent/ListChild.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetParent/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetRun/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetRun/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetRun/ListBlockRuns.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetRun/ListDSRuns.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetRun/ListFileRuns.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetRun/UpdateStatus.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetRun/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetType/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetType/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DatasetType/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DbsVersion/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/DbsVersion/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/File/BriefList.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/File/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/File/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/File/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/File/MgrtList.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/File/UpdateStatus.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/File/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileBuffer/DeleteDuplicates.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileBuffer/DeleteFiles.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileBuffer/FindDuplicates.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileBuffer/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileBuffer/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileBuffer/ListBlocks.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileBuffer/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileLumi/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileLumi/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileLumi/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileOutputMod_config/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileOutputMod_config/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileParent/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileParent/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileParent/ListChild.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileParent/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileParentBlock/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileParentBlock/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileType/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileType/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/FileType/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/MigrationBlock/FindMigrateableBlocks.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/MigrationBlock/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/MigrationBlock/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/MigrationBlock/Update.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/MigrationBlock/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/FindPendingRequest.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/ListOldest.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/Update.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/UpdateRequestStatus.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/OutputModuleConfig/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/OutputModuleConfig/GetIDForBlockInsert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/OutputModuleConfig/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/OutputModuleConfig/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/OutputModuleConfig/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ParameterSetHashe/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ParameterSetHashe/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ParameterSetHashe/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/PhysicsGroup/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/PhysicsGroup/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/PhysicsGroup/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/PrimaryDSType/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/PrimaryDSType/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/PrimaryDSType/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/PrimaryDSType/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/PrimaryDataset/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/PrimaryDataset/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/PrimaryDataset/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/PrimaryDataset/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ProcessedDataset/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ProcessedDataset/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ProcessedDataset/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ProcessingEra/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ProcessingEra/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ProcessingEra/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ProcessingEra/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ReleaseVersion/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ReleaseVersion/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/ReleaseVersion/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Service/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Service/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Service/Update.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Service/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Site/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Site/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Site/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Site/ListBlockSite.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/Site/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/StorageElement/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/StorageElement/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/StorageElement/ListBlockSE.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/MySQL/StorageElement/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/SequenceManager.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/AcquisitionEra/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/AcquisitionEra/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/AcquisitionEra/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/AcquisitionEra/List_CI.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/AcquisitionEra/UpdateEndDate.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/AcquisitionEra/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ApplicationExecutable/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ApplicationExecutable/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ApplicationExecutable/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/AssociatedFile/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/AssociatedFile/__init__.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/Block/BriefList.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Block/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Block/Insert.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/Block/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Block/ListBlockOrigin.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Block/ListStats.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/Block/SummaryList.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Block/UpdateSiteName.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Block/UpdateStats.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Block/UpdateStatus.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Block/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/BlockParent/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/BlockParent/Insert2.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/BlockParent/Insert3.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/BlockParent/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/BlockParent/ListChild.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/BlockParent/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/BlockSite/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/BlockSite/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/BranchHashe/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/BranchHashe/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/BranchHashe/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DataTier/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DataTier/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DataTier/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DataTier/__init__.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/Dataset/BriefList.py

         tran = None
         conn = None  
--- ./Server/Python/src/dbs/business/DBSFileBuffer.py	(original)
+++ ./Server/Python/src/dbs/business/DBSFileBuffer.py	(refactored)
@@ -180,7 +180,7 @@
                 # Update block parameters, file_count, block_size
                 blkParams = self.blkstats.execute(conn, block_id,
                                                   transaction=tran)
-                blkParams['block_size'] = long(blkParams['block_size'])
+                blkParams['block_size'] = int(blkParams['block_size'])
                 self.blkstatsin.execute(conn, blkParams, transaction=tran)
 
             # Delete the just inserted files
--- ./Server/Python/src/dbs/business/DBSMigrate.py	(original)
+++ ./Server/Python/src/dbs/business/DBSMigrate.py	(refactored)
@@ -3,7 +3,7 @@
 """
 This module provides dataset migration business object class.
 """
-from __future__ import print_function
+
 
 __revision__ = "$Id: DBSMigrate.py,v 1.17 2010/09/14 14:53:54 yuyi Exp $"
 __version__ = "$Revision: 1.17 $"
@@ -16,8 +16,8 @@
 import json
 import os
 import socket
-import urllib2
-import urlparse
+import urllib.request, urllib.error, urllib.parse
+import urllib.parse
 
 from dbs.utils.dbsUtils import dbsUtils
 from dbs.utils.dbsExceptionHandler import dbsExceptionHandler
@@ -33,7 +33,7 @@
 def remove_duplicated_items(ordered_dict):
     unique_block_list = set()
 
-    for key, value in reversed(ordered_dict.items()):
+    for key, value in reversed(list(ordered_dict.items())):
         for entry in list(value):#copy the list since value is modified during iteration
             if entry not in unique_block_list:
                 unique_block_list.add(entry)
@@ -276,8 +276,8 @@
                     if tmp_ordered_dict != {}:
                         #ordered_dict[order_counter+1] = []
                         #ordered_dict.update(tmp_ordered_dict)
-                        for i in tmp_ordered_dict.keys():
-                                if i in ordered_dict.keys():
+                        for i in list(tmp_ordered_dict.keys()):
+                                if i in list(ordered_dict.keys()):
                                     ordered_dict[i] += tmp_ordered_dict[i]
                                 else:
                                     ordered_dict[i] = tmp_ordered_dict[i]
@@ -529,7 +529,7 @@
 
     def callDBSService(self, resturl, method='', params={}, data={}):
         try:
-            spliturl = urlparse.urlparse(resturl)
+            spliturl = urllib.parse.urlparse(resturl)
             callType = spliturl[0]
             if callType != 'http' and callType != 'https':
                 raise ValueError("unknown URL type: %s" % callType)
@@ -542,9 +542,9 @@
             restapi = self.rest_client_pool.get_rest_client()
             httpresponse = restapi.get(resturl, method, params, data, request_headers)
             return httpresponse.body
-        except urllib2.HTTPError as httperror:
+        except urllib.error.HTTPError as httperror:
             raise httperror
-        except urllib2.URLError as urlerror:
+        except urllib.error.URLError as urlerror:
             raise urlerror
         except HTTPError as DBShttp_error:
             raise DBShttp_error
--- ./Server/Python/src/dbs/business/DBSPhysicsGroup.py	(original)
+++ ./Server/Python/src/dbs/business/DBSPhysicsGroup.py	(refactored)
@@ -24,7 +24,7 @@
         """
         Returns all physics groups if physics group names are not passed.
         """
-        if not isinstance(physics_group_name, basestring):
+        if not isinstance(physics_group_name, str):
             dbsExceptionHandler('dbsException-invalid-input',
                 'physics group name given is not valid : %s' %
                  physics_group_name)
--- ./Server/Python/src/dbs/business/DBSReleaseVersion.py	(original)
+++ ./Server/Python/src/dbs/business/DBSReleaseVersion.py	(refactored)
@@ -41,7 +41,7 @@
             if plist:
                 t = []
                 for i in plist:
-                    for k, v in i.iteritems():
+                    for k, v in i.items():
                         t.append(v)
                 result[0]['release_version'] = t
             return result
--- ./Server/Python/src/dbs/components/migration/DBSMigrationServer.py	(original)
+++ ./Server/Python/src/dbs/components/migration/DBSMigrationServer.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import time
 import cherrypy
 import threading
--- ./Server/Python/src/dbs/components/migration/StartUp.py	(original)
+++ ./Server/Python/src/dbs/components/migration/StartUp.py	(refactored)
@@ -50,8 +50,8 @@
     options = get_command_line_options(__name__, sys.argv)
     migration_config = configure(options.config)
 
-    for instance in migration_config['database'].keys():
-        for thread in xrange(migration_config['database'][instance]['threads']):
+    for instance in list(migration_config['database'].keys()):
+        for thread in range(migration_config['database'][instance]['threads']):
             DBSMigrationServer(MigrationTask(migration_config['database'][instance]), duration = 5)
 
     root = MigrationWebMonitoring()
--- ./Server/Python/src/dbs/dao/Oracle/Block/BriefList.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/Block/BriefList.py	(refactored)
@@ -111,7 +111,7 @@
             wheresql_run_range=''
             #
             for r in parseRunRange(run_num):
-                if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):
+                if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):
                     run_list.append(str(r))
                 if isinstance(r, run_tuple):
                     if r[0] == r[1]:
--- ./Server/Python/src/dbs/dao/Oracle/Block/List.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/Block/List.py	(refactored)
@@ -112,7 +112,7 @@
             wheresql_run_range=''
             #
             for r in parseRunRange(run_num):
-                if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):
+                if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):
                     #if not wheresql_run_list:
                         #wheresql_run_list = " FLM.RUN_NUM = :run_list "
                     run_list.append(str(r))
--- ./Server/Python/src/dbs/dao/Oracle/Block/SummaryList.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/Block/SummaryList.py	(refactored)
@@ -63,7 +63,7 @@
         else:
             # Oracle IN only supports a maximum of 1,000 values
             # (ORA-01795: maximum number of expressions in a list is 1000)
-            if isinstance(block_name, basestring):
+            if isinstance(block_name, str):
                 block_name=[block_name]
             block_clause = "BS.BLOCK_NAME IN (SELECT TOKEN FROM TOKEN_GENERATOR) "
             generatedsql, run_binds = create_token_generator(block_name)
--- ./Server/Python/src/dbs/dao/Oracle/BlockParent/List.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/BlockParent/List.py	(refactored)
@@ -33,7 +33,7 @@
 
         sql = self.sql
         
-	if isinstance(block_name, basestring):
+	if isinstance(block_name, str):
 	    binds = {'block_name' :block_name}
         elif isinstance(block_name, list):
             binds = [{'block_name':x} for x in block_name]
--- ./Server/Python/src/dbs/dao/Oracle/Dataset/BriefList.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/Dataset/BriefList.py	(refactored)
@@ -62,7 +62,7 @@
 	    wheresql_dataset_id_list=''
             wheresql_dataset_id_range=''
 	    for id in parseRunRange(dataset_id):
-		if isinstance(id, basestring) or isinstance(id, int) or isinstance(id, long):
+		if isinstance(id, str) or isinstance(id, int) or isinstance(id, int):
 		    dataset_id_list.append(str(id))
                 if isinstance(id, run_tuple):
                     if id[0] == id[1]:
@@ -131,7 +131,7 @@
             if prep_id:
                 wheresql += "AND D.prep_id = :prep_id "
                 binds.update(prep_id = prep_id)
-            if dataset and isinstance(dataset, basestring) and dataset != "%":
+            if dataset and isinstance(dataset, str) and dataset != "%":RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Dataset/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Dataset/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Dataset/Insert2.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/Dataset/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Dataset/UpdateStatus.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Dataset/UpdateType.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Dataset/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetOutputMod_config/GetDSConfigs.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetOutputMod_config/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetOutputMod_config/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetParent/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetParent/Insert2.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetParent/Insert3.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetParent/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetParent/ListChild.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetParent/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetRun/Insert.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/DatasetRun/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetRun/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetType/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetType/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetType/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DatasetType/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DbsVersion/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DbsVersion/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DoNothing/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/DoNothing/__init__.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/File/BriefList.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/File/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/File/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/File/Insert2.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/File/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/File/MgrtList.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/File/SummaryList.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/File/UpdateStatus.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/File/__init__.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/FileBuffer/DeleteDuplicates.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileBuffer/DeleteFiles.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/FileBuffer/FindDuplicates.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileBuffer/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileBuffer/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileBuffer/ListBlocks.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileBuffer/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileLumi/Insert.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/FileLumi/List.py

                 op = ("=", "like")["%" in dataset]
                 wheresql += " AND D.DATASET %s :dataset " % op
                 binds.update(dataset = dataset)
@@ -245,7 +245,7 @@
                 wheresql_run_list=''
                 wheresql_run_range=''
                 for r in parseRunRange(run_num):
-                    if isinstance(r, basestring) or isinstance(r, int)  or isinstance(r, long):
+                    if isinstance(r, str) or isinstance(r, int)  or isinstance(r, int):
                         run_list.append(str(r))
                     if isinstance(r, run_tuple):
                         if r[0] == r[1]:
--- ./Server/Python/src/dbs/dao/Oracle/Dataset/List.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/Dataset/List.py	(refactored)
@@ -83,7 +83,7 @@
             wheresql_dataset_id_list=''
             wheresql_dataset_id_range=''
             for id in parseRunRange(dataset_id):
-                if isinstance(id, basestring) or isinstance(id, int) or isinstance(id, long):
+                if isinstance(id, str) or isinstance(id, int) or isinstance(id, int):
                     dataset_id_list.append(str(id))
                 if isinstance(id, run_tuple):
                     if id[0] == id[1]:
@@ -266,7 +266,7 @@
                 wheresql_run_list=''
                 wheresql_run_range=''
                 for r in parseRunRange(run_num):
-                    if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):
+                    if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):
                         run_list.append(str(r))
                     if isinstance(r, run_tuple):
                         if r[0] == r[1]:
--- ./Server/Python/src/dbs/dao/Oracle/DatasetRun/List.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/DatasetRun/List.py	(refactored)
@@ -57,7 +57,7 @@
             wheresql_run_range = ''
             #
             for r in parseRunRange(run_num):
-                if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):
+                if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):
                     run_list.append(str(r))
                 if isinstance(r, run_tuple):
                     if r[0] == r[1]:
--- ./Server/Python/src/dbs/dao/Oracle/File/BriefList.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/File/BriefList.py	(refactored)
@@ -130,11 +130,11 @@
             wheresql_run_range=''
 	    wheresql_run_range_ct = 0	
             try:
-                run_num = long(run_num)
+                run_num = int(run_num)
                 wheresql += " and FL.RUN_NUM = :run_num "
                 binds.update({"run_num":run_num})
             except:
-		if isinstance(run_num, basestring):
+		if isinstance(run_num, str):
 		    for r in parseRunRange(run_num):
 			if isinstance(r, run_tuple):
 			    if r[0] == r[1]:
@@ -152,7 +152,7 @@
 			    dbsExceptionHandler('dbsException-invalid-input', "Invalid run_num. if run_num input as a string, it has to be converted into a int/long or in format of 'run_min-run_max'. ", self.logger.exception)
                 elif isinstance(run_num, list) and len(run_num)==1:
                     try:
-                        run_num = long(run_num[0])
+                        run_num = int(run_num[0])
 			wheresql += " and FL.RUN_NUM = :run_num "
 			binds.update({"run_num":run_num})
                     except:
@@ -173,7 +173,7 @@
                                 dbsExceptionHandler('dbsException-invalid-input', "run_num as a list must be a number or a range str, such as ['10'], [10] or ['1-10']", self.logger.exception)
                 else:
                     for r in parseRunRange(run_num):
-                        if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):
+                        if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):
                             run_list.append(str(r))
                         if isinstance(r, run_tuple):
                             if r[0] == r[1]:
@@ -205,7 +205,7 @@
             wheresql += " AND FL.LUMI_SECTION_NUM in (SELECT TOKEN FROM TOKEN_GENERATOR) "
             #Do I need to convert lumi_list to be a str list? YG 10/03/13
             #Yes, you do. YG
-            lumi_list = map(str, lumi_list)
+            lumi_list = list(map(str, lumi_list))
             lumi_generator, lumi_binds = create_token_generator(lumi_list)
             binds.update(lumi_binds)
             #binds["run_num"]=run_list[0]
--- ./Server/Python/src/dbs/dao/Oracle/File/List.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/File/List.py	(refactored)
@@ -125,11 +125,11 @@
             wheresql_run_range=''
 	    wheresql_run_range_ct = 0
 	    try:
-		run_num = long(run_num)
+		run_num = int(run_num)
 		sql += " and FL.RUN_NUM = :run_num "
 		binds.update({"run_num":run_num})
 	    except:
-		if isinstance(run_num, basestring):
+		if isinstance(run_num, str):
                     for r in parseRunRange(run_num):
                         if isinstance(r, run_tuple):
                             if r[0] == r[1]:
@@ -150,7 +150,7 @@
 			    "Invalid run_num. if run_num input as a string, it has to be converted into a int/long or in format of 'run_min-run_max'. ", self.logger.exception)
 		elif isinstance(run_num, list) and len(run_num)==1:
 		    try:
-			run_num = long(run_num[0])
+			run_num = int(run_num[0])
 			sql += " and FL.RUN_NUM = :run_num "
                         binds.update({"run_num":run_num})	
 		    except:
@@ -175,7 +175,7 @@
 				self.logger.exception)	
 		else:		
 		    for r in parseRunRange(run_num):
-			if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):
+			if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):
 			    run_list.append(str(r))
 			if isinstance(r, run_tuple):
 			    if r[0] == r[1]:
@@ -212,7 +212,7 @@
             sql_lumi = " FL.LUMI_SECTION_NUM in (SELECT TOKEN FROM TOKEN_GENERATOR) "
             #Do I need to convert lumi_list to be a str list? YG 10/03/13
             #Yes, you do. YG
-            lumi_list = map(str, lumi_list)
+            lumi_list = list(map(str, lumi_list))
             lumi_generator, lumi_binds = create_token_generator(lumi_list)
             #sql_sel = "{lumi_generator}".format(lumi_generator=lumi_generator) + sql_sel
             binds.update(lumi_binds)
--- ./Server/Python/src/dbs/dao/Oracle/File/SummaryList.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/File/SummaryList.py	(refactored)
@@ -49,7 +49,7 @@
         if run_num != -1:
             #
             for r in parseRunRange(run_num):
-                if isinstance(r, basestring) or isinstance(r, (long, int)):
+                if isinstance(r, str) or isinstance(r, int):
                     #if not wheresql_run_list:
                     #    wheresql_run_list = " fl.RUN_NUM = :run_list "
                     run_list.append(str(r))
--- ./Server/Python/src/dbs/dao/Oracle/FileBuffer/DeleteDuplicates.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/FileBuffer/DeleteDuplicates.py	(refactored)
@@ -2,7 +2,7 @@
 """
 This module provides FileBuffer.DeleteDuplicates data access object.
 """
-from __future__ import print_function
+
 from WMCore.Database.DBFormatter import DBFormatter
 from dbs.utils.dbsExceptionHandler import dbsExceptionHandler
 
--- ./Server/Python/src/dbs/dao/Oracle/FileBuffer/FindDuplicates.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/FileBuffer/FindDuplicates.py	(refactored)
@@ -2,7 +2,7 @@
 """
 This module provides FileBuffer.FindDuplicates data access object.
 """
-from __future__ import print_function
+
 from WMCore.Database.DBFormatter import DBFormatter
 from dbs.utils.dbsExceptionHandler import dbsExceptionHandler
 
--- ./Server/Python/src/dbs/dao/Oracle/FileLumi/List.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/FileLumi/List.py	(refactored)
@@ -91,7 +91,7 @@
             wheresql_run_list=''
             wheresql_run_range=''
             for r in parseRunRange(run_num):
-                if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long) or isinstance(r, str):
+                if isinstance(r, str) or isinstance(r, int) or isinstance(r, int) or isinstance(r, str):RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileLumi/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileOutputMod_config/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileOutputMod_config/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileParent/Insert.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/FileParent/Insert2.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/FileParent/List.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/FileParent/ListBlockFileLumiIds.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/FileParent/ListChild.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/FileParent/ListFileParentageByLumi.py
RefactoringTool: Refactored ./Server/Python/src/dbs/dao/Oracle/FileParent/ListParentDatasetFileLumiIds.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileParent/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileParentBlock/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileParentBlock/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileType/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileType/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/FileType/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/InsertTable/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/InsertTable/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationBlock/FindMigrateableBlocks.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationBlock/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationBlock/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationBlock/Update.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationBlock/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/FindPendingRequest.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/ListOldest.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/Remove.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/Update.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/UpdateRequestStatus.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/OutputModuleConfig/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/OutputModuleConfig/GetIDForBlockInsert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/OutputModuleConfig/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/OutputModuleConfig/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/OutputModuleConfig/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ParameterSetHashe/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ParameterSetHashe/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ParameterSetHashe/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PhysicsGroup/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PhysicsGroup/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PhysicsGroup/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PhysicsGroup/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PrimaryDSType/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PrimaryDSType/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PrimaryDSType/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PrimaryDSType/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PrimaryDataset/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PrimaryDataset/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PrimaryDataset/Insert2.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PrimaryDataset/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/PrimaryDataset/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ProcessedDataset/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ProcessedDataset/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ProcessedDataset/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ProcessingEra/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ProcessingEra/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ProcessingEra/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ProcessingEra/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ReleaseVersion/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ReleaseVersion/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ReleaseVersion/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/ReleaseVersion/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Run/SummaryList.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Run/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Service/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Service/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Service/Update.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Service/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Site/GetID.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Site/Insert.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Site/List.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Site/ListBlockSite.py
RefactoringTool: No changes to ./Server/Python/src/dbs/dao/Oracle/Site/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/utils/DBSDaoTools.py
RefactoringTool: Refactored ./Server/Python/src/dbs/utils/DBSInputValidation.py
RefactoringTool: Refactored ./Server/Python/src/dbs/utils/DBSTransformInputType.py

                     run_list.append(str(r))
                 if isinstance(r, run_tuple):
                     if r[0] == r[1]:
@@ -104,7 +104,7 @@
             if run_list:
 		if len(run_list) == 1:
 		    wheresql_run_list = " fl.RUN_NUM = :single_run "
-		    binds.update({"single_run": long(run_list[0])})
+		    binds.update({"single_run": int(run_list[0])})
 
 		else:
 		    wheresql_run_list = " fl.RUN_NUM in (SELECT TOKEN FROM TOKEN_GENERATOR) "
@@ -145,7 +145,7 @@
                     file_run_lumi.setdefault((f, r), []).append([i['lumi_section_num'], i['event_count']])
                 else:
                     file_run_lumi.setdefault((f, r), []).append(i['lumi_section_num'])
-	    for k, v in file_run_lumi.iteritems():
+	    for k, v in file_run_lumi.items():
                 if event_ct:
                     lumi=[]
                     event=[]
--- ./Server/Python/src/dbs/dao/Oracle/FileParent/Insert2.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/FileParent/Insert2.py	(refactored)
@@ -31,14 +31,14 @@
         binds = {} 
         bindlist=[]
         
-        if isinstance(daoinput, dict) and "block_name" in daoinput.keys():
+        if isinstance(daoinput, dict) and "block_name" in list(daoinput.keys()):
             binds = {"block_name": daoinput["block_name"]}
             r = self.dbi.processData(self.sql_sel, binds, conn, False)
             bfile = self.format(r)
             bfile_list = []
             for f in bfile:
                 bfile_list.append(f[0])           
-            if "child_parent_id_list" in daoinput.keys():
+            if "child_parent_id_list" in list(daoinput.keys()):
                 files = []
                 for i in daoinput["child_parent_id_list"]:
                     files.append(i[0])
--- ./Server/Python/src/dbs/dao/Oracle/FileParent/List.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/FileParent/List.py	(refactored)
@@ -36,7 +36,7 @@
         binds = {}
 
         if logical_file_name:
-            if isinstance(logical_file_name, basestring):
+            if isinstance(logical_file_name, str):
                 wheresql = "WHERE F.LOGICAL_FILE_NAME = :logical_file_name"
                 binds = {"logical_file_name": logical_file_name}
                 sql = "{sql} {wheresql}".format(sql=self.sql, wheresql=wheresql)
--- ./Server/Python/src/dbs/dao/Oracle/FileParent/ListBlockFileLumiIds.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/FileParent/ListBlockFileLumiIds.py	(refactored)
@@ -4,7 +4,7 @@
 
 Y Guo May 1, 2020
 """
-from __future__ import print_function
+
 from types import GeneratorType
 from WMCore.Database.DBFormatter import DBFormatter
 from dbs.utils.dbsExceptionHandler import dbsExceptionHandler
--- ./Server/Python/src/dbs/dao/Oracle/FileParent/ListChild.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/FileParent/ListChild.py	(refactored)
@@ -35,7 +35,7 @@
         sql = ''
 
         if logical_file_name:
-            if isinstance(logical_file_name, basestring):
+            if isinstance(logical_file_name, str):
                 wheresql = "WHERE F.LOGICAL_FILE_NAME = :logical_file_name"
                 binds = {"logical_file_name": logical_file_name}
                 sql = "{sql} {wheresql}".format(sql=self.sql, wheresql=wheresql)
--- ./Server/Python/src/dbs/dao/Oracle/FileParent/ListFileParentageByLumi.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/FileParent/ListFileParentageByLumi.py	(refactored)
@@ -6,7 +6,7 @@
 
 Y Guo July 13, 2018 
 """
-from __future__ import print_function
+
 from types import GeneratorType
 from WMCore.Database.DBFormatter import DBFormatter
 from dbs.utils.dbsExceptionHandler import dbsExceptionHandler
--- ./Server/Python/src/dbs/dao/Oracle/FileParent/ListParentDatasetFileLumiIds.py	(original)
+++ ./Server/Python/src/dbs/dao/Oracle/FileParent/ListParentDatasetFileLumiIds.py	(refactored)
@@ -3,7 +3,7 @@
 This module provides IDs of File, lumi and run of the parent dataset by a given block name.
 Y Guo May 1, 2020
 """
-from __future__ import print_function
+
 from types import GeneratorType
 from WMCore.Database.DBFormatter import DBFormatter
 from dbs.utils.dbsExceptionHandler import dbsExceptionHandler
--- ./Server/Python/src/dbs/utils/DBSInputValidation.py	(original)
+++ ./Server/Python/src/dbs/utils/DBSInputValidation.py	(refactored)
@@ -41,7 +41,7 @@
             ka.update(list(zip(arg_names, args)))
             ka.update(kw)
             #print ka
-            for name, value in ka.iteritems():
+            for name, value in ka.items():
                 #In fact the framework removes all the input variables that is not in the args list of _addMethod.
                 #So DBS list API will never see these variables. For example, if one has
                 #http://hostname/cms_dbs/DBS/datatiers?name=abc, the API will get a request to list all the datatiers because
@@ -56,7 +56,7 @@
                                             logger=log.error, serverError=serverlog)
                     else:
                         try:
-                            if isinstance(value, basestring):
+                            if isinstance(value, str):
                                 try:
                                     value = str(value)
                                 except:
@@ -181,7 +181,7 @@
     ################
     list:['dataset', 'run_num', 'logical_file_name', 'dataset_id', 'lumi_list', 'child_parent_id_list', 'dataset_parent_list'],
     ################
-    long:['lumi_section_num', 'run_num', 'xtcrosssection', 'auto_cross_section', 'dataset_id', 'lumi_list'],
+    int:['lumi_section_num', 'run_num', 'xtcrosssection', 'auto_cross_section', 'dataset_id', 'lumi_list'],
     ################
     float:['xtcrosssection', 'auto_cross_section']
 }
@@ -262,7 +262,7 @@
 def validateJSONInputNoCopy(input_key,input_data, read=False):
     log = clog.error_log
     if isinstance(input_data, dict):
-        for key in input_data.keys():
+        for key in list(input_data.keys()):
             if key not in acceptedInputKeys[input_key]:
                 dbsExceptionHandler('dbsException-invalid-input2', message="Invalid Input Key %s..." %key[:10],\
                                     logger=log.error, serverError="%s is not a valid input key for %s"%(key, input_key))
@@ -275,7 +275,7 @@
         for x in input_data:
             l.append(validateJSONInputNoCopy(input_key, x, read=read))
         input_data = l
-    elif isinstance(input_data, basestring):
+    elif isinstance(input_data, str):
         if input_key not in acceptedInputDataTypes[str]:
             dbsExceptionHandler('dbsException-invalid-input2', message="Invalid input data type str for key-value %s... %s..." \
                 %(input_key[:10], input_data[:10]), logger=log.error,\
@@ -286,8 +286,8 @@
         if input_key not in acceptedInputDataTypes[int]:
             dbsExceptionHandler('dbsException-invalid-input2', message="Invalid input data type int for key-value %s..,  %s"\
             %(input_key[:10], input_data), logger=log.error, serverError="Input data %s is not a valid input type for key %s"%(input_data, input_key))
-    elif isinstance(input_data, long):
-        if input_key not in acceptedInputDataTypes[long]:
+    elif isinstance(input_data, int):
+        if input_key not in acceptedInputDataTypes[int]:
             dbsExceptionHandler('dbsException-invalid-input2', message="Invalid input data type long for key-value %s... %s" \
             %(input_key[:10], input_data), logger=log.error, serverError="Input data %s is not a valid date type for key %s"%(input_data, input_key))
     elif isinstance(input_data, float):
--- ./Server/Python/src/dbs/utils/DBSTransformInputType.py	(original)
+++ ./Server/Python/src/dbs/utils/DBSTransformInputType.py	(refactored)
@@ -33,7 +33,7 @@
         for item in run_range:
             for new_item in parseRunRange(item):
                 yield new_item
-    elif isinstance(run_range, basestring):
+    elif isinstance(run_range, str):
         try:
             min_run, max_run = run_range.split('-', 1)
         except ValueError:
@@ -43,7 +43,7 @@
                 yield run_range
             else:RefactoringTool: No changes to ./Server/Python/src/dbs/utils/RestClientPool.py
RefactoringTool: No changes to ./Server/Python/src/dbs/utils/__init__.py
RefactoringTool: No changes to ./Server/Python/src/dbs/utils/dbsException.py
RefactoringTool: No changes to ./Server/Python/src/dbs/utils/dbsExceptionDef.py
RefactoringTool: No changes to ./Server/Python/src/dbs/utils/dbsExceptionHandler.py
RefactoringTool: Refactored ./Server/Python/src/dbs/utils/dbsHTTPSAuthHandler.py
RefactoringTool: Refactored ./Server/Python/src/dbs/utils/dbsUtils.py
RefactoringTool: No changes to ./Server/Python/src/dbs/utils/docstring_parser.py
RefactoringTool: Refactored ./Server/Python/src/dbs/web/DBSMigrateModel.py
RefactoringTool: Refactored ./Server/Python/src/dbs/web/DBSReaderModel.py

                 yield run_tuple(min_run, max_run)
-    elif isinstance(run_range, (int, long)):
+    elif isinstance(run_range, int):
         yield run_range
     else:
         dbsExceptionHandler('dbsException-invalid-input2', "Invalid input: run/run_range or dataset_id.",
--- ./Server/Python/src/dbs/utils/dbsHTTPSAuthHandler.py	(original)
+++ ./Server/Python/src/dbs/utils/dbsHTTPSAuthHandler.py	(refactored)
@@ -1,20 +1,20 @@
 import os, sys, socket
-import urllib, urllib2
-import httplib
+import urllib.request, urllib.parse, urllib.error, urllib.request, urllib.error, urllib.parse
+import http.client
 
-class HTTPSAuthHandler(urllib2.HTTPSHandler):
+class HTTPSAuthHandler(urllib.request.HTTPSHandler):
     """
     HTTPS authentication class to provide path of key/cert files.
     """
     def __init__(self, key=None, cert=None, level=0):
-        urllib2.HTTPSHandler.__init__(self, debuglevel=level)
+        urllib.request.HTTPSHandler.__init__(self, debuglevel=level)
         self.key = key
         self.cert = cert
 
     def get_connection(self, host, timeout=300):
         if  self.key and self.cert:
-            return httplib.HTTPSConnection(host, key_file=self.key, cert_file=self.cert)
-        return httplib.HTTPSConnection(host)
+            return http.client.HTTPSConnection(host, key_file=self.key, cert_file=self.cert)
+        return http.client.HTTPSConnection(host)
 
 
     def https_open(self, req):
--- ./Server/Python/src/dbs/utils/dbsUtils.py	(original)
+++ ./Server/Python/src/dbs/utils/dbsUtils.py	(refactored)
@@ -39,7 +39,7 @@
 	   [a1, a2, a3] """
 	errmessage = "lumi intervals must be of one of the two following formats: '[[a,b], [c,d], ...],' or [a1, a2, a3 ...] "
 
-        if isinstance(lumi_list, basestring):
+        if isinstance(lumi_list, str):
             try:
                 lumi_list = cjson.decode(lumi_list)
             except:
@@ -58,7 +58,7 @@
             for lumiinterval in lumi_list:
                 if not isinstance(lumiinterval, list) or len(lumiinterval) != 2:
                     dbsExceptionHandler("dbsException-invalid-input2", "invalid lumi input", None, errmessage)
-                resultext(range(lumiinterval[0], lumiinterval[1]+1))
+                resultext(list(range(lumiinterval[0], lumiinterval[1]+1)))
             result = list(set(result)) #removes the dublicates, no need to sort
             return result
         
--- ./Server/Python/src/dbs/web/DBSMigrateModel.py	(original)
+++ ./Server/Python/src/dbs/web/DBSMigrateModel.py	(refactored)
@@ -34,7 +34,7 @@
     Authorization function for general insert  
     """
     if not role: return True
-    for k, v in user['roles'].iteritems():
+    for k, v in user['roles'].items():
         for g in v['group']:
             if k in role.get(g, '').split(':'):
                 return True
@@ -96,7 +96,7 @@
 	    else:	
 		dbsExceptionHandler('dbsException-server-error',  str(ex), self.logger.exception, sError)
     
-    @inputChecks(migration_rqst_id=(long, int, basestring), block_name=basestring, dataset=basestring, user=basestring)
+    @inputChecks(migration_rqst_id=(int, int, str), block_name=str, dataset=str, user=str)
     def status(self, migration_rqst_id="", block_name="", dataset="", user=""):
         """
         Interface to query status of a migration request
--- ./Server/Python/src/dbs/web/DBSReaderModel.py	(original)
+++ ./Server/Python/src/dbs/web/DBSReaderModel.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import with_statement
+
 #!/usr/bin/env python
 #pylint: disable=C0103
 """
@@ -58,7 +58,7 @@
     """
     if not role:
         return True
-    for k, v in user['roles'].iteritems():
+    for k, v in user['roles'].items():
         for g in v['group']:
             if k in role.get(g, '').split(':'):
                 return True
@@ -223,7 +223,7 @@
             doc = self.methods['GET'][call]['call'].__doc__
             return dict(params=params, doc=doc)
         else:
-            return self.methods['GET'].keys()
+            return list(self.methods['GET'].keys())
 
     @thr.make_throttled()
     def getServerInfo(self):
@@ -237,7 +237,7 @@
         """
         return dict(dbs_version=self.version, dbs_instance=self.instance)
 
-    @inputChecks(primary_ds_name=basestring, primary_ds_type=basestring)
+    @inputChecks(primary_ds_name=str, primary_ds_type=str)
     @thr.make_throttled()
     def listPrimaryDatasets(self, primary_ds_name="", primary_ds_type=""):
         """
@@ -264,7 +264,7 @@
                     % (ex, traceback.format_exc() )
             dbsExceptionHandler('dbsException-server-error',  dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(primary_ds_type=basestring, dataset=basestring)
+    @inputChecks(primary_ds_type=str, dataset=str)
     @thr.make_throttled()
     def listPrimaryDsTypes(self, primary_ds_type="", dataset=""):
         """
@@ -292,13 +292,13 @@
             dbsExceptionHandler('dbsException-server-error',  dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
     @transformInputType('run_num')
-    @inputChecks( dataset=basestring, parent_dataset=basestring, release_version=basestring, pset_hash=basestring,
-                 app_name=basestring, output_module_label=basestring, global_tag=basestring, processing_version=(int, basestring), acquisition_era_name=basestring,
-                 run_num=(long, int, basestring, list), physics_group_name=basestring, logical_file_name=basestring, primary_ds_name=basestring,
-                 primary_ds_type=basestring, processed_ds_name=basestring, data_tier_name=basestring, dataset_access_type=basestring, prep_id=basestring,
-                 create_by=(basestring), last_modified_by=(basestring), min_cdate=(int, basestring), max_cdate=(int, basestring),
-                 min_ldate=(int, basestring), max_ldate=(int, basestring), cdate=(int, basestring), ldate=(int, basestring), detail=(bool, basestring),
-                 dataset_id=(int, long, basestring))
+    @inputChecks( dataset=str, parent_dataset=str, release_version=str, pset_hash=str,
+                 app_name=str, output_module_label=str, global_tag=str, processing_version=(int, str), acquisition_era_name=str,
+                 run_num=(int, int, str, list), physics_group_name=str, logical_file_name=str, primary_ds_name=str,
+                 primary_ds_type=str, processed_ds_name=str, data_tier_name=str, dataset_access_type=str, prep_id=str,
+                 create_by=(str), last_modified_by=(str), min_cdate=(int, str), max_cdate=(int, str),
+                 min_ldate=(int, str), max_ldate=(int, str), cdate=(int, str), ldate=(int, str), detail=(bool, str),
+                 dataset_id=(int, int, str))
     @thr.make_throttled()
     def listDatasets(self, dataset="", parent_dataset="", is_dataset_valid=1,
         release_version="", pset_hash="", app_name="", output_module_label="", global_tag="",
@@ -405,7 +405,7 @@
         #
         if (run_num != -1 and logical_file_name ==''):
             for r in parseRunRange(run_num):
-                if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):    
+                if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):    
                     if r == 1 or r == '1':
                         dbsExceptionHandler("dbsException-invalid-input", "Run_num=1 is not a valid input.",
                                 self.logger.exception)
@@ -439,7 +439,7 @@
             dbsExceptionHandler("dbsException-invalid-input2", "Invalid Input for create_by or last_modified_by.\
             No wildcard allowed.",  self.logger.exception, 'No wildcards allowed for create_by or last_modified_by')
         try:
-            if isinstance(min_cdate, basestring) and ('*' in min_cdate or '%' in min_cdate):
+            if isinstance(min_cdate, str) and ('*' in min_cdate or '%' in min_cdate):
                 min_cdate = 0
             else:
                 try:
@@ -447,7 +447,7 @@
                 except:
                     dbsExceptionHandler("dbsException-invalid-input", "invalid input for min_cdate")
             
-            if isinstance(max_cdate, basestring) and ('*' in max_cdate or '%' in max_cdate):
+            if isinstance(max_cdate, str) and ('*' in max_cdate or '%' in max_cdate):
                 max_cdate = 0
             else:
                 try:
@@ -455,7 +455,7 @@
                 except:
                     dbsExceptionHandler("dbsException-invalid-input", "invalid input for max_cdate")
             
-            if isinstance(min_ldate, basestring) and ('*' in min_ldate or '%' in min_ldate):
+            if isinstance(min_ldate, str) and ('*' in min_ldate or '%' in min_ldate):
                 min_ldate = 0
             else:
                 try:
@@ -463,7 +463,7 @@
                 except:
                     dbsExceptionHandler("dbsException-invalid-input", "invalid input for min_ldate")
             
-            if isinstance(max_ldate, basestring) and ('*' in max_ldate or '%' in max_ldate):
+            if isinstance(max_ldate, str) and ('*' in max_ldate or '%' in max_ldate):
                 max_ldate = 0
             else:
                 try:
@@ -471,7 +471,7 @@
                 except:
                     dbsExceptionHandler("dbsException-invalid-input", "invalid input for max_ldate")
             
-            if isinstance(cdate, basestring) and ('*' in cdate or '%' in cdate):
+            if isinstance(cdate, str) and ('*' in cdate or '%' in cdate):
                 cdate = 0
             else:
                 try:
@@ -479,7 +479,7 @@
                 except:
                     dbsExceptionHandler("dbsException-invalid-input", "invalid input for cdate")
             
-            if isinstance(ldate, basestring) and ('*' in ldate or '%' in ldate):
+            if isinstance(ldate, str) and ('*' in ldate or '%' in ldate):
                 ldate = 0
             else:
                 try:
@@ -533,8 +533,8 @@
                 #the API can be finished in 300 second. 
                 # YG Nov-05-2015
                 max_array_size = 1000
-                if ( 'dataset' in data.keys() and isinstance(data['dataset'], list) and len(data['dataset'])>max_array_size)\
-                    or ('dataset_id' in data.keys() and isinstance(data['dataset_id'], list) and len(data['dataset_id'])>max_array_size):
+                if ( 'dataset' in list(data.keys()) and isinstance(data['dataset'], list) and len(data['dataset'])>max_array_size)\
+                    or ('dataset_id' in list(data.keys()) and isinstance(data['dataset_id'], list) and len(data['dataset_id'])>max_array_size):
                     dbsExceptionHandler("dbsException-invalid-input",
                                         "The Max list length supported in listDatasetArray is %s." %max_array_size, self.logger.exception)    
                 ret = self.dbsDataset.listDatasetArray(data)
@@ -551,7 +551,7 @@
         for item in ret:
                     yield item
 
-    @inputChecks(data_tier_name=basestring)
+    @inputChecks(data_tier_name=str)
     @thr.make_throttled()
     def listDataTiers(self, data_tier_name=""):
         """
@@ -584,9 +584,9 @@
                 conn.close()
 
     @transformInputType('run_num')
-    @inputChecks(dataset=basestring, block_name=basestring, data_tier_name=basestring, origin_site_name=basestring, logical_file_name=basestring,
-                 run_num=(long, int, basestring, list), min_cdate=(int, basestring), max_cdate=(int, basestring), min_ldate=(int, basestring),
-                 max_ldate=(int, basestring), cdate=(int, basestring),  ldate=(int, basestring), open_for_writing=(int, basestring), detail=(basestring, bool))
+    @inputChecks(dataset=str, block_name=str, data_tier_name=str, origin_site_name=str, logical_file_name=str,
+                 run_num=(int, int, str, list), min_cdate=(int, str), max_cdate=(int, str), min_ldate=(int, str),
+                 max_ldate=(int, str), cdate=(int, str),  ldate=(int, str), open_for_writing=(int, str), detail=(str, bool))
     @thr.make_throttled()
     def listBlocks(self, dataset="", block_name="", data_tier_name="", origin_site_name="",
                    logical_file_name="",run_num=-1, min_cdate='0', max_cdate='0',
@@ -635,7 +635,7 @@
         # 
         if (run_num != -1 and logical_file_name ==''):
             for r in parseRunRange(run_num):
-                if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):    
+                if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):    
                     if r == 1 or r == '1':
                         dbsExceptionHandler("dbsException-invalid-input", "Run_num=1 is not a valid input.",
                                 self.logger.exception)
@@ -652,7 +652,7 @@
         logical_file_name = logical_file_name.replace("*", "%")
         origin_site_name = origin_site_name.replace("*", "%")
         #
-	if isinstance(min_cdate, basestring) and ('*' in min_cdate or '%' in min_cdate):
+	if isinstance(min_cdate, str) and ('*' in min_cdate or '%' in min_cdate):
             min_cdate = 0
         else:
             try:
@@ -660,7 +660,7 @@
             except:
                 dbsExceptionHandler("dbsException-invalid-input", "invalid input for min_cdate")
         #
-        if isinstance(max_cdate, basestring) and ('*' in max_cdate or '%' in max_cdate):
+        if isinstance(max_cdate, str) and ('*' in max_cdate or '%' in max_cdate):
             max_cdate = 0
         else:
             try:
@@ -668,7 +668,7 @@
             except:
                 dbsExceptionHandler("dbsException-invalid-input", "invalid input for max_cdate")
         #
-        if isinstance(min_ldate, basestring) and ('*' in min_ldate or '%' in min_ldate):
+        if isinstance(min_ldate, str) and ('*' in min_ldate or '%' in min_ldate):
             min_ldate = 0
         else:
             try:
@@ -676,7 +676,7 @@
             except:
                 dbsExceptionHandler("dbsException-invalid-input", "invalid input for max_cdate")
         #
-	if isinstance(max_ldate, basestring) and ('*' in max_ldate or '%' in max_ldate):
+	if isinstance(max_ldate, str) and ('*' in max_ldate or '%' in max_ldate):
             max_ldate = 0
         else:
             try:
@@ -684,7 +684,7 @@
             except:
                 dbsExceptionHandler("dbsException-invalid-input", "invalid input for max_ldate")
         #
-        if isinstance(cdate, basestring) and ('*' in cdate or '%' in cdate):
+        if isinstance(cdate, str) and ('*' in cdate or '%' in cdate):
             cdate = 0
         else:
             try:
@@ -692,7 +692,7 @@
             except:
                 dbsExceptionHandler("dbsException-invalid-input", "invalid input for cdate")
         #
-        if isinstance(cdate, basestring) and ('*' in ldate or '%' in ldate):
+        if isinstance(cdate, str) and ('*' in ldate or '%' in ldate):
             ldate = 0
         else:
             try:
@@ -716,7 +716,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(origin_site_name=basestring, dataset=basestring, block_name=basestring)
+    @inputChecks(origin_site_name=str, dataset=str, block_name=str)
     @thr.make_throttled()
     def listBlockOrigin(self, origin_site_name="",  dataset="", block_name=""):
         """
@@ -743,7 +743,7 @@
                                 self.logger.exception, sError)
 
 
-    @inputChecks(block_name=basestring)
+    @inputChecks(block_name=str)
     @thr.make_throttled()
     def listBlockParents(self, block_name=""):
         """
@@ -781,7 +781,7 @@
             #the API can be finished in 300 second. 
             # YG Nov-05-2015
             max_array_size = 1000
-            if ( 'block_names' in data.keys() and isinstance(data['block_names'], list) and len(data['block_names'])>max_array_size):
+            if ( 'block_names' in list(data.keys()) and isinstance(data['block_names'], list) and len(data['block_names'])>max_array_size):
                     dbsExceptionHandler("dbsException-invalid-input",
                                         "The Max list length supported in listBlocksParents is %s." %max_array_size, self.logger.exception)
             return self.dbsBlock.listBlockParents(data["block_name"])
@@ -799,7 +799,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(block_name=basestring)
+    @inputChecks(block_name=str)
     @thr.make_throttled()
     def listBlockChildren(self, block_name=""):
         """
@@ -821,7 +821,7 @@
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
     @transformInputType('block_name')
-    @inputChecks(block_name=(basestring, list), dataset=basestring, detail=(bool, basestring))
+    @inputChecks(block_name=(str, list), dataset=str, detail=(bool, str))
     @thr.make_throttled()
     def listBlockSummaries(self, block_name="", dataset="", detail=False):
         """
@@ -842,7 +842,7 @@
                                 self.logger.exception,
                                 "Dataset or block_names must be specified at a time.")
 
-        if block_name and isinstance(block_name, basestring):
+        if block_name and isinstance(block_name, str):
             try:
                 block_name = [str(block_name)]
             except:
@@ -876,10 +876,10 @@
                     yield item
 
     @transformInputType( 'run_num')
-    @inputChecks(dataset =basestring, block_name=basestring, logical_file_name =(basestring), release_version=basestring, pset_hash=basestring, app_name=basestring,\
-                 output_module_label=basestring, run_num=(long, int, basestring, list), origin_site_name=basestring,
-                 lumi_list=(basestring, list), detail=(basestring, bool), validFileOnly=(basestring, int), 
-                 sumOverLumi=(basestring, int))
+    @inputChecks(dataset =str, block_name=str, logical_file_name =(str), release_version=str, pset_hash=str, app_name=str,\
+                 output_module_label=str, run_num=(int, int, str, list), origin_site_name=str,
+                 lumi_list=(str, list), detail=(str, bool), validFileOnly=(str, int), 
+                 sumOverLumi=(str, int))
     @thr.make_throttled()
     def listFiles(self, dataset = "", block_name = "", logical_file_name = "",
         release_version="", pset_hash="", app_name="", output_module_label="",
@@ -953,7 +953,7 @@
         #
         if (run_num != -1 and logical_file_name ==''):
             for r in parseRunRange(run_num):
-                if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):    
+                if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):    
                     if r == 1 or r == '1':
                         dbsExceptionHandler("dbsException-invalid-input", "While Run_num=1, LFN is required.",
                                 self.logger.exception)
@@ -1069,7 +1069,7 @@
                 # YG Nov-4-2020
                 if ('logical_file_name' not in data or not data['logical_file_name']) and 'run_num' in data:
                     for r in parseRunRange(data['run_num']):
-                        if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):    
+                        if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):    
                             if r == 1 or r == '1': 
                                 dbsExceptionHandler("dbsException-invalid-input", "run_num cannot be 1 w/o lfn.",
                                     self.logger.exception)
@@ -1104,7 +1104,7 @@
             yield item
 
     @transformInputType('run_num')
-    @inputChecks(block_name=basestring, dataset=basestring, run_num=(long, int, basestring, list), validFileOnly=(int, basestring), sumOverLumi=(int, basestring))
+    @inputChecks(block_name=str, dataset=str, run_num=(int, int, str, list), validFileOnly=(int, str), sumOverLumi=(int, str))
     @thr.make_throttled()
     def listFileSummaries(self, block_name='', dataset='', run_num=-1, validFileOnly=0, sumOverLumi=0):
         """
@@ -1141,7 +1141,7 @@
         #
         if (run_num != -1)  :
             for r in parseRunRange(run_num):
-                if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):    
+                if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):    
                     if r == 1 or r == '1':
                         dbsExceptionHandler("dbsException-invalid-input", "Run_num=1 is not a valid input.",
                                 self.logger.exception)
@@ -1167,7 +1167,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', ex.message, self.logger.exception, sError)
 
-    @inputChecks(dataset=basestring)
+    @inputChecks(dataset=str)
     @thr.make_throttled()
     def listDatasetParents(self, dataset=''):
         """
@@ -1188,7 +1188,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(dataset=basestring)
+    @inputChecks(dataset=str)
     @thr.make_throttled()
     def listDatasetChildren(self, dataset):
         """
@@ -1209,8 +1209,8 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(dataset=basestring, logical_file_name=basestring, release_version=basestring, pset_hash=basestring, app_name=basestring,\
-                 output_module_label=basestring, block_id=(int, basestring), global_tag=basestring)
+    @inputChecks(dataset=str, logical_file_name=str, release_version=str, pset_hash=str, app_name=str,\
+                 output_module_label=str, block_id=(int, str), global_tag=str)
     @thr.make_throttled()
     def listOutputConfigs(self, dataset="", logical_file_name="",
                           release_version="", pset_hash="", app_name="",
@@ -1257,7 +1257,7 @@
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
     @transformInputType('logical_file_name')
-    @inputChecks(logical_file_name=(basestring, list), block_id=(int, basestring), block_name=basestring)
+    @inputChecks(logical_file_name=(str, list), block_id=(int, str), block_name=str)
     @thr.make_throttled()
     def listFileParents(self, logical_file_name='', block_id=0, block_name=''):
         """
@@ -1286,7 +1286,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', ex.message,  self.logger.exception, sError)
    
-    @inputChecks(dataset=basestring)
+    @inputChecks(dataset=str)
     @thr.make_throttled()
     def listParentDSTrio(self, dataset=''):
         """
@@ -1318,7 +1318,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', ex.message,  self.logger.exception, sError)
 
-    @inputChecks(block_name=basestring)
+    @inputChecks(block_name=str)
     @thr.make_throttled()
     def listBlockTrio(self, block_name=''):
         """
@@ -1377,15 +1377,15 @@
             #Because CMSWEB has a 300 seconds responding time. We have to limit the array siz to make sure that
             #the API can be finished in 300 second. 
             max_array_size = 1000
-            if ('logical_file_name' in data.keys() and isinstance(data['logical_file_name'], list) and len(data['logical_file_name'])>max_array_size):
+            if ('logical_file_name' in list(data.keys()) and isinstance(data['logical_file_name'], list) and len(data['logical_file_name'])>max_array_size):
                 dbsExceptionHandler("dbsException-invalid-input",
                                         "The Max list length supported in listFilePArentsByLumi is %s." %max_array_size, self.logger.exception)
 
             lfn = []
-            if "block_name" not in data.keys():
+            if "block_name" not in list(data.keys()):
                  dbsExceptionHandler('dbsException-invalid-input', "block_name is required for fileparentsbylumi")RefactoringTool: No changes to ./Server/Python/src/dbs/web/DBSServicesRegistry.py
RefactoringTool: Refactored ./Server/Python/src/dbs/web/DBSWriterModel.py

             else:
-                if "logical_file_name" in data.keys():
+                if "logical_file_name" in list(data.keys()):
                     lfn = data["logical_file_name"]
             result = self.dbsFile.listFileParentsByLumi(block_name=data["block_name"], logical_file_name=lfn)
             for r in result:
@@ -1402,7 +1402,7 @@
             dbsExceptionHandler('dbsException-server-error', ex.message, self.logger.exception, sError)
 
     @transformInputType('logical_file_name')
-    @inputChecks(logical_file_name=(basestring, list), block_name=(basestring), block_id=(basestring, int))
+    @inputChecks(logical_file_name=(str, list), block_name=(str), block_id=(str, int))
     @thr.make_throttled()
     def listFileChildren(self, logical_file_name='', block_name='', block_id=0):
         """
@@ -1434,7 +1434,7 @@
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
     @transformInputType('run_num')
-    @inputChecks(logical_file_name=(basestring, list), block_name=basestring, run_num=(long, int, basestring, list), validFileOnly=(int, basestring))
+    @inputChecks(logical_file_name=(str, list), block_name=str, run_num=(int, int, str, list), validFileOnly=(int, str))
     @thr.make_throttled()
     def listFileLumis(self, logical_file_name="", block_name="", run_num=-1, validFileOnly=0):
         """
@@ -1458,7 +1458,7 @@
         # YG Jan. 16 2019
         if (run_num != -1  and logical_file_name ==''):
             for r in parseRunRange(run_num):
-                if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):    
+                if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):    
                     if r == 1 or r == '1':
                         dbsExceptionHandler("dbsException-invalid-input", "Run_num=1 is not a valid input.",
                                 self.logger.exception)
@@ -1515,7 +1515,7 @@
             # YG Nov-4-2020
             if ('run_num' in data):
                 for r in parseRunRange(data['run_num']):
-                    if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):    
+                    if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):    
                         if (r == 1 or r == '1') and ('logical_file_name' not in data or not data['logical_file_name']): 
                             dbsExceptionHandler("dbsException-invalid-input", "run_num cannot be 1 w/o lfn in filelumiarray API.",
                                 self.logger.exception)
@@ -1541,7 +1541,7 @@
 
 
     @transformInputType('run_num')
-    @inputChecks(run_num=(long, int, basestring, list), logical_file_name=basestring, block_name=basestring, dataset=basestring)
+    @inputChecks(run_num=(int, int, str, list), logical_file_name=str, block_name=str, dataset=str)
     @thr.make_throttled()
     def listRuns(self, run_num=-1, logical_file_name="", block_name="", dataset=""):
         """
@@ -1562,7 +1562,7 @@
         # YG Jan. 16 2019
         if (run_num != -1  and logical_file_name ==''):
             for r in parseRunRange(run_num):
-                if isinstance(r, basestring) or isinstance(r, int) or isinstance(r, long):    
+                if isinstance(r, str) or isinstance(r, int) or isinstance(r, int):    
                     if r == 1 or r == '1':
                         dbsExceptionHandler("dbsException-invalid-input", "Run_num=1 is not a valid input.",
                                 self.logger.exception)
@@ -1592,7 +1592,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(datatype=basestring, dataset=basestring)
+    @inputChecks(datatype=str, dataset=str)
     @thr.make_throttled()
     def listDataTypes(self, datatype="", dataset=""):
         """
@@ -1615,7 +1615,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(block_name=basestring)
+    @inputChecks(block_name=str)
     def dumpBlock(self, block_name):
         """
         API the list all information related with the block_name
@@ -1635,7 +1635,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', ex.message, self.logger.exception, sError)
 
-    @inputChecks(acquisition_era_name=basestring)
+    @inputChecks(acquisition_era_name=str)
     @thr.make_throttled()
     def listAcquisitionEras(self, acquisition_era_name=''):
         """
@@ -1656,7 +1656,7 @@
             sError = "DBSReaderModel/listAcquisitionEras. %s\n. Exception trace: \n %s" % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(acquisition_era_name=basestring)
+    @inputChecks(acquisition_era_name=str)
     @thr.make_throttled()
     def listAcquisitionEras_CI(self, acquisition_era_name=''):
         """
@@ -1678,7 +1678,7 @@
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'],
                                 self.logger.exception, sError)
 
-    @inputChecks(processing_version=(basestring, int))
+    @inputChecks(processing_version=(str, int))
     @thr.make_throttled()
     def listProcessingEras(self, processing_version=0):
         """
@@ -1700,7 +1700,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(release_version=basestring, dataset=basestring, logical_file_name=basestring)
+    @inputChecks(release_version=str, dataset=str, logical_file_name=str)
     @thr.make_throttled()
     def listReleaseVersions(self, release_version='', dataset='', logical_file_name=''):
         """
@@ -1727,7 +1727,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(dataset_access_type=basestring)
+    @inputChecks(dataset_access_type=str)
     @thr.make_throttled()
     def listDatasetAccessTypes(self, dataset_access_type=''):
         """
@@ -1750,7 +1750,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(physics_group_name=basestring)
+    @inputChecks(physics_group_name=str)
     @thr.make_throttled()
     def listPhysicsGroups(self, physics_group_name=''):
         """
@@ -1773,7 +1773,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error', dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(dataset=basestring, run_num=(basestring, int, long))
+    @inputChecks(dataset=str, run_num=(str, int, int))
     @thr.make_throttled()
     def listRunSummaries(self, dataset="", run_num=-1):
         """
--- ./Server/Python/src/dbs/web/DBSWriterModel.py	(original)
+++ ./Server/Python/src/dbs/web/DBSWriterModel.py	(refactored)
@@ -147,7 +147,7 @@
                             % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error',  dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(acquisition_era_name=basestring, end_date=(int, basestring))
+    @inputChecks(acquisition_era_name=str, end_date=(int, str))
     def updateAcqEraEndDate(self, acquisition_era_name ="", end_date=0):
         """
         API to update the end_date of an acquisition era
@@ -455,7 +455,7 @@
             dbsExceptionHandler('dbsException-server-error',  dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)RefactoringTool: No changes to ./Server/Python/src/dbs/web/__init__.py
RefactoringTool: Refactored ./Server/Python/tests/DBS3SimpleClient.py
RefactoringTool: Refactored ./Server/Python/tests/SimpleBizlogicTest.py

 
     @transformInputType('logical_file_name')
-    @inputChecks(logical_file_name=(basestring, list), is_file_valid=(int, basestring), lost=(int, basestring, bool ), dataset=basestring)
+    @inputChecks(logical_file_name=(str, list), is_file_valid=(int, str), lost=(int, str, bool ), dataset=str)
     def updateFile(self, logical_file_name=[], is_file_valid=1, lost=0, dataset=''):
         """
         API to update file status
@@ -491,7 +491,7 @@
                     % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error',  dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
 
-    @inputChecks(dataset=basestring, dataset_access_type=basestring)
+    @inputChecks(dataset=str, dataset_access_type=str)
     def updateDataset(self, dataset="", is_dataset_valid=-1, dataset_access_type=""):
         """
         API to update dataset type
@@ -520,10 +520,10 @@
         except HTTPError as he:
             raise he
         except Exception as ex:
-            sError = "DBSWriterModel\updateDataset. %s\n. Exception trace: \n %s" % (ex, traceback.format_exc())
-            dbsExceptionHandler('dbsException-server-error',  dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
-
-    @inputChecks(block_name=basestring, open_for_writing=(int, basestring), origin_site_name=basestring)
+            sError = "DBSWriterModel\\updateDataset. %s\n. Exception trace: \n %s" % (ex, traceback.format_exc())
+            dbsExceptionHandler('dbsException-server-error',  dbsExceptionCode['dbsException-server-error'], self.logger.exception, sError)
+
+    @inputChecks(block_name=str, open_for_writing=(int, str), origin_site_name=str)
     def updateBlock(self, block_name="", open_for_writing=-1, origin_site_name=""):
         """
         API to update block status
@@ -546,7 +546,7 @@
         except HTTPError as he:
             raise he
         except Exception as ex:
-            sError = "DBSWriterModel\updateStatus. %s\n. Exception trace: \n %s" % (ex, traceback.format_exc())
+            sError = "DBSWriterModel\\updateStatus. %s\n. Exception trace: \n %s" % (ex, traceback.format_exc())
             dbsExceptionHandler('dbsException-server-error',  dbsExceptionCode['dbsException-server-error'],
                                 self.logger.exception, sError)
 
--- ./Server/Python/tests/DBS3SimpleClient.py	(original)
+++ ./Server/Python/tests/DBS3SimpleClient.py	(refactored)
@@ -2,28 +2,28 @@
 """
 Very simple dbs3 client:
 """
-from __future__ import print_function
+
 
 __revision__ = "$Id: DBS3SimpleClient.py,v 1.8 2009/11/29 11:37:54 akhukhun Exp $"
 __version__ = "$Revision: 1.8 $"
 
 import sys
 import cjson
-import urllib, urllib2
+import urllib.request, urllib.parse, urllib.error, urllib.request, urllib.error, urllib.parse
 
 class DBS3Client:
     def __init__(self, baseurl):
         self.baseurl = baseurl
         self.header =  {"Accept": "application/json"}
-        self.opener =  urllib2.build_opener()
+        self.opener =  urllib.request.build_opener()
         
     def get(self, apiurl, params = {}):
         "method for GET verb"
         url = self.baseurl + apiurl
         if not params == {}:
-            url = "?".join((url, urllib.urlencode(params, doseq=True)))
+            url = "?".join((url, urllib.parse.urlencode(params, doseq=True)))
         #req = urllib2.Request(url = url, headers = self.header)
-        req = urllib2.Request(url = url)
+        req = urllib.request.Request(url = url)
         data = self.opener.open(req)
         #ddata = cjson.decode(data.read())
         #return ddata
@@ -37,7 +37,7 @@
         header = self.header
         header['Content-Type'] = 'application/json'
         endata = cjson.encode(indata)
-        req = urllib2.Request(url = url, data = endata, headers = header)
+        req = urllib.request.Request(url = url, data = endata, headers = header)
         req.get_method = lambda: 'POST'
         self.opener.open(req)
 
--- ./Server/Python/tests/SimpleBizlogicTest.py	(original)
+++ ./Server/Python/tests/SimpleBizlogicTest.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import logging
 from WMCore.Database.DBFactory import DBFactory
 from dbs.business.DBSPrimaryDataset import DBSPrimaryDataset
@@ -64,21 +64,21 @@
                         'output_configs' : [  {'app_name': 'Repacker', 'release_version': 'CMSSW_2_1_7',  'pset_hash': 'NO_PSET_HASH'}  ] 
 		}
 
-	binput = {'is_dataset_valid': 1, 'physics_group_name': 'Tracker', 'dataset': u'/QCD_BCtoMu_Pt20/Summer08_IDEAL_V9_v1/GEN-SIM-RAW', 
-		    'dataset_type': 'PRODUCTION', 'processed_ds_name': u'Summer08_IDEAL_V9_v1', 'primary_ds_name': u'QCD_BCtoMu_Pt20', 
-		    'output_configs': [{'release_version': u'CMSSW_2_1_7', 'pset_hash': u'NO_PSET_HASH', 'app_name': u'cmsRun', 'output_module_label': u'Merged'}, 
-		    {'release_version': u'CMSSW_2_1_7', 'pset_hash': u'76e303993a1c2f842159dbfeeed9a0dd', 'app_name': u'cmsRun', 'output_module_label': u'output'}], 
-		    'global_tag': u'', 'xtcrosssection': 123, 'primary_ds_type': 'test', 'data_tier_name': 'GEN-SIM-RAW',
+	binput = {'is_dataset_valid': 1, 'physics_group_name': 'Tracker', 'dataset': '/QCD_BCtoMu_Pt20/Summer08_IDEAL_V9_v1/GEN-SIM-RAW', 
+		    'dataset_type': 'PRODUCTION', 'processed_ds_name': 'Summer08_IDEAL_V9_v1', 'primary_ds_name': 'QCD_BCtoMu_Pt20', 
+		    'output_configs': [{'release_version': 'CMSSW_2_1_7', 'pset_hash': 'NO_PSET_HASH', 'app_name': 'cmsRun', 'output_module_label': 'Merged'}, 
+		    {'release_version': 'CMSSW_2_1_7', 'pset_hash': '76e303993a1c2f842159dbfeeed9a0dd', 'app_name': 'cmsRun', 'output_module_label': 'output'}], 
+		    'global_tag': '', 'xtcrosssection': 123, 'primary_ds_type': 'test', 'data_tier_name': 'GEN-SIM-RAW',
 		    'creation_date': 1234, 'create_by': 'anzar', "last_modification_date": 1234, "last_modified_by": "anzar",
 		    #'processing_version': '1',  'acquisition_era_name': u'',
 		}
-	binput = {'is_dataset_valid': 1, 'physics_group_name': 'Tracker', 'dataset': u'/TkCosmics38T/Summer09-STARTUP31X_V3-v1/GEN-SIM-DIGI-RAW', 
-		    'dataset_type': 'PRODUCTION', 'processed_ds_name': u'Summer09-STARTUP31X_V3-v1', 'primary_ds_name': u'TkCosmics38T', 
-		    'data_tier_name': 'GEN-SIM-DIGI-RAW', 'global_tag': u'STARTUP31X_V3::All', 'xtcrosssection': 123, 'primary_ds_type': 'test', 
+	binput = {'is_dataset_valid': 1, 'physics_group_name': 'Tracker', 'dataset': '/TkCosmics38T/Summer09-STARTUP31X_V3-v1/GEN-SIM-DIGI-RAW', 
+		    'dataset_type': 'PRODUCTION', 'processed_ds_name': 'Summer09-STARTUP31X_V3-v1', 'primary_ds_name': 'TkCosmics38T', 
+		    'data_tier_name': 'GEN-SIM-DIGI-RAW', 'global_tag': 'STARTUP31X_V3::All', 'xtcrosssection': 123, 'primary_ds_type': 'test', 
 		    'output_configs': [
-			    {'release_version': u'CMSSW_3_1_2', 'pset_hash': u'4847ed25a7e108a7b1e704a26f345aa8', 'app_name': u'cmsRun', 'output_module_label': u'Merged'}, 
-			    {'release_version': u'CMSSW_3_1_2', 'pset_hash': u'NO_PSET_HASH', 'app_name': u'cmsRun', 'output_module_label': u'Merged'}, 
-			    {'release_version': u'CMSSW_3_1_2', 'pset_hash': u'4847ed25a7e108a7b1e704a26f345aa8', 'app_name': u'cmsRun', 'output_module_label': u'output'}
+			    {'release_version': 'CMSSW_3_1_2', 'pset_hash': '4847ed25a7e108a7b1e704a26f345aa8', 'app_name': 'cmsRun', 'output_module_label': 'Merged'}, 
+			    {'release_version': 'CMSSW_3_1_2', 'pset_hash': 'NO_PSET_HASH', 'app_name': 'cmsRun', 'output_module_label': 'Merged'}, 
+			    {'release_version': 'CMSSW_3_1_2', 'pset_hash': '4847ed25a7e108a7b1e704a26f345aa8', 'app_name': 'cmsRun', 'output_module_label': 'output'}
 			],
 		    'creation_date': 1234, 'create_by': 'anzar', "last_modification_date": 1234, "last_modified_by": "anzar",
 		}
@@ -111,10 +111,10 @@
 		"""
 		
 		bo = DBSBlock(self.logger, self.dbi, self.owner)
-		binput = {'block_name': u'/QCD_BCtoMu_Pt20/Summer08_IDEAL_V9_v1/GEN-SIM-RAW#f930d82a-f72b-4f9e-8351-8a3cb0c43b79', 'file_count': u'100', 
-			    'origin_site': u'cmssrm.fnal.gov', 'last_modification_date': u'1263231733', 
-			    'create_by': u'/DC=org/DC=doegrids/OU=People/CN=Ajit Kumar Mohapatra 867118', 'block_size': u'228054411650', RefactoringTool: Refactored ./Server/Python/tests/SimpleDAOTest.py
RefactoringTool: No changes to ./Server/Python/tests/TestBusiness.py
RefactoringTool: Refactored ./Server/Python/tests/TestDAO.py
RefactoringTool: Refactored ./Server/Python/tests/TestInsertTime.py
RefactoringTool: Refactored ./Server/Python/tests/TestListTime.py
RefactoringTool: No changes to ./Server/Python/tests/TestServer.py
RefactoringTool: Refactored ./Server/Python/tests/TestStress.py
RefactoringTool: Refactored ./Server/Python/tests/setup_test.py
RefactoringTool: No changes to ./Server/Python/tests/dao/Dataset/testInsert.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/__init__.py
RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/performance/BusinessList.py
RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/performance/CXOracleList.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/performance/DBSRestProfile.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/performance/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSBlock_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSDatasetParent_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSDataset_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSFileLumi_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSFileParent_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSFile_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSOutputConfig_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSPrimaryDataset_t.py
RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/unittests/business_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/components_t/__init__.py
RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/unittests/components_t/migration_t/DBSMigrationServer_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/components_t/migration_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/__init__.py
RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/AcquisitionEra_t/Insert_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/AcquisitionEra_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/AcquisitionEra_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/BlockParent_t/Insert_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/BlockParent_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/BlockParent_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/Block_t/Insert_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/Block_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/Block_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/DataTier_t/Insert_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/DataTier_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/DataTier_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/DatasetParent_t/Insert_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/DatasetParent_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/DatasetParent_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/Dataset_t/Insert_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/Dataset_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/Dataset_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/FileLumi_t/Insert_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/FileLumi_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/FileLumi_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/FileParent_t/Insert_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/FileParent_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/FileParent_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/File_t/Insert_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/File_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/File_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/OutputModuleConfig_t/Insert_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/OutputModuleConfig_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/OutputModuleConfig_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PhysicsGroup_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PhysicsGroup_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PrimaryDSType_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PrimaryDSType_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PrimaryDataset_t/Insert_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PrimaryDataset_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PrimaryDataset_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/ProcessingEra_t/Insert_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/ProcessingEra_t/List_t.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/ProcessingEra_t/__init__.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/unittests/web_t/DBSMigrateModel_t.py
RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/unittests/web_t/DBSReaderModel_t.py
RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/unittests/web_t/DBSWriterModel_t.py

-			    'open_for_writing': 1, 'last_modified_by': u'anzar@cmssrv49.fnal.gov', 'creation_date': u'1228050132'}
+		binput = {'block_name': '/QCD_BCtoMu_Pt20/Summer08_IDEAL_V9_v1/GEN-SIM-RAW#f930d82a-f72b-4f9e-8351-8a3cb0c43b79', 'file_count': '100', 
+			    'origin_site': 'cmssrm.fnal.gov', 'last_modification_date': '1263231733', 
+			    'create_by': '/DC=org/DC=doegrids/OU=People/CN=Ajit Kumar Mohapatra 867118', 'block_size': '228054411650', 
+			    'open_for_writing': 1, 'last_modified_by': 'anzar@cmssrv49.fnal.gov', 'creation_date': '1228050132'}
 		bo.insertBlock(binput)
 
     def testFiles(self):
@@ -125,13 +125,13 @@
 		bo = DBSFile(self.logger, self.dbi, self.owner)
     
 		binput = [
-		{'adler32': u'NOTSET', 'file_type': 'EDM', 'file_output_config_list': [{'release_version': 'CMSSW_1_2_3', 'pset_hash': '76e303993a1c2f842159dbfeeed9a0dd', 'app_name': 
+		{'adler32': 'NOTSET', 'file_type': 'EDM', 'file_output_config_list': [{'release_version': 'CMSSW_1_2_3', 'pset_hash': '76e303993a1c2f842159dbfeeed9a0dd', 'app_name': 
 		'cmsRun', 'output_module_label': 'Merged'}], 'dataset': '/unittest_web_primary_ds_name_684/unittest_web_dataset_684/GEN-SIM-RAW', 
-		'file_size': u'2012211901', 'auto_cross_section': 0.0, 'check_sum': u'1504266448', 
-		'file_lumi_list': [{'lumi_section_num': u'27414', 'run_num': u'1'}, 
-				    {'lumi_section_num': u'26422', 'run_num': u'1'}, 
-				    {'lumi_section_num': u'29838', 'run_num': u'1'}], 
-				    'file_parent_list': [], 'event_count': u'1619', 'logical_file_name': 
+		'file_size': '2012211901', 'auto_cross_section': 0.0, 'check_sum': '1504266448', 
+		'file_lumi_list': [{'lumi_section_num': '27414', 'run_num': '1'}, 
+				    {'lumi_section_num': '26422', 'run_num': '1'}, 
+				    {'lumi_section_num': '29838', 'run_num': '1'}], 
+				    'file_parent_list': [], 'event_count': '1619', 'logical_file_name': 
 					'/store/mc/parent_684/0.root', 
 		'block': '/unittest_web_primary_ds_name_684/unittest_web_dataset_684/GEN-SIM-RAW#684',
 		'creation_date': 1234, 'create_by': 'anzar', "last_modification_date": 1234, "last_modified_by": "anzar",
--- ./Server/Python/tests/SimpleDAOTest.py	(original)
+++ ./Server/Python/tests/SimpleDAOTest.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import logging
 from WMCore.Database.DBFactory import DBFactory
 
--- ./Server/Python/tests/TestDAO.py	(original)
+++ ./Server/Python/tests/TestDAO.py	(refactored)
@@ -2,7 +2,7 @@
 As the number of tests increase we will probably repackage these tests
 to separate package/modules for each DAO object
 """
-from __future__ import print_function
+
 
 __revision__ = "$Id: TestDAO.py,v 1.5 2009/11/29 11:37:55 akhukhun Exp $"
 __version__ = "$Revision: 1.5 $"
--- ./Server/Python/tests/TestInsertTime.py	(original)
+++ ./Server/Python/tests/TestInsertTime.py	(refactored)
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 #
-from __future__ import print_function
+
 import os
 import sys
 import time
--- ./Server/Python/tests/TestListTime.py	(original)
+++ ./Server/Python/tests/TestListTime.py	(refactored)
@@ -1,5 +1,5 @@
 """This module provides all basic tests for the server"""
-from __future__ import print_function
+
 
 __revision__ = "$Id: TestListTime.py,v 1.2 2009/11/29 11:37:54 akhukhun Exp $"
 __version__ = "$Revision: 1.2 $"
--- ./Server/Python/tests/TestStress.py	(original)
+++ ./Server/Python/tests/TestStress.py	(refactored)
@@ -1,5 +1,5 @@
 """This module provides all basic tests for the server"""
-from __future__ import print_function
+
 
 __revision__ = "$Id: TestStress.py,v 1.3 2009/11/29 11:37:54 akhukhun Exp $"
 __version__ = "$Revision: 1.3 $"
--- ./Server/Python/tests/setup_test.py	(original)
+++ ./Server/Python/tests/setup_test.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import sys, os
 import fnmatch
 from glob import glob
--- ./Server/Python/tests/dbsserver_t/performance/BusinessList.py	(original)
+++ ./Server/Python/tests/dbsserver_t/performance/BusinessList.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import os
 import time
 import logging
--- ./Server/Python/tests/dbsserver_t/performance/CXOracleList.py	(original)
+++ ./Server/Python/tests/dbsserver_t/performance/CXOracleList.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import cx_Oracle
 import threading
 import time
--- ./Server/Python/tests/dbsserver_t/unittests/business_t/__init__.py	(original)
+++ ./Server/Python/tests/dbsserver_t/unittests/business_t/__init__.py	(refactored)
@@ -1,5 +1,5 @@
 """ business unittests package"""
-from __future__ import print_function
+
 
 __revision__ = "$Id: __init__.py,v 1.1 2010/01/01 19:54:37 akhukhun Exp $"
 __version__ = "$Revision: 1.1 $"
--- ./Server/Python/tests/dbsserver_t/unittests/components_t/migration_t/DBSMigrationServer_t.py	(original)
+++ ./Server/Python/tests/dbsserver_t/unittests/components_t/migration_t/DBSMigrationServer_t.py	(refactored)
@@ -2,7 +2,7 @@
 """
 DBS 3 Migration Service unittests
 """
-from __future__ import print_function
+
 from dbsserver_t.utils.DBSRestApi import DBSRestApi
 from dbsserver_t.utils.DBSDataProvider import DBSBlockDataProvider, create_child_data_provider
 from dbsserver_t.utils.TestTools import expectedFailure
@@ -38,7 +38,7 @@
 def remove_non_comparable_keys(values, non_comparable_keys):
     for value in values:
         if isinstance(value, dict):
-            keys = set(value.iterkeys())
+            keys = set(value.keys())
             intersection = keys.intersection(set(non_comparable_keys))
             for entry in intersection:
                 del value[entry]
@@ -185,7 +185,7 @@
             non_comparable_keys = ('block_id', 'dataset_id', 'last_modification_date',
                                    'parent_file_id', 'primary_ds_id', 'description')
             if isinstance(input, dict):
-                for key, value in input.iteritems():
+                for key, value in input.items():
                     if key in non_comparable_keys:
                         continue ###do not compare id's
                     self.assertTrue(key in output)
--- ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/__init__.py	(original)
+++ ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/__init__.py	(refactored)
@@ -1,7 +1,7 @@
 """
 Oracle DAO unittests package.
 """
-from __future__ import print_function
+
 
 __revision__ = "$Id: __init__.py,v 1.1 2010/01/01 19:54:40 akhukhun Exp $"
 __version__ = "$Revision: 1.1 $"
--- ./Server/Python/tests/dbsserver_t/unittests/web_t/DBSReaderModel_t.py	(original)
+++ ./Server/Python/tests/dbsserver_t/unittests/web_t/DBSReaderModel_t.py	(refactored)
@@ -1,7 +1,7 @@
 """
 web unittests
 """
-from __future__ import absolute_import
+
 import imp
 import os
 import re
@@ -1191,7 +1191,7 @@
         lfn1 = testparams['files'][1]
         lfn2 = testparams['files'][2]
         print ("******* test010w() ******")
-        print (lfn1, lfn2)
+        print((lfn1, lfn2))
         data={"logical_file_name": [lfn1, lfn2], "run_num":'1-%s' %testparams['run_num'], "validFileOnly" :0}
         api.insert('filelumis', data)
 
--- ./Server/Python/tests/dbsserver_t/unittests/web_t/DBSWriterModel_t.py	(original)
+++ ./Server/Python/tests/dbsserver_t/unittests/web_t/DBSWriterModel_t.py	(refactored)
@@ -1,7 +1,7 @@
 """
 web unittests
 """
-from __future__ import print_function
+
 
 __revision__ = "$Id: DBSWriterModel_t.py,v 1.27 2010/08/24 19:48:44 yuyi Exp $"
 __version__ = "$Revision: 1.27 $"
@@ -420,15 +420,15 @@
                      'output_module_label': output_module_label, 'global_tag': global_tag},
                     ],
                 'dataset': dataset,
-                'file_size': u'2012211901', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
-                'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': uid},
-                    {'lumi_section_num': u'26422', 'run_num': uid},
-                    {'lumi_section_num': u'29838', 'run_num': uid}
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': uid},
+                    {'lumi_section_num': '26422', 'run_num': uid},
+                    {'lumi_section_num': '29838', 'run_num': uid}
                     ],
                 'file_parent_list': [ ],
-                'event_count': u'1619',
+                'event_count': '1619',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uid, i),
                 'block_name': block,
                 #'is_file_valid': 1
@@ -451,15 +451,15 @@
                      'output_module_label': output_module_label, 'global_tag': global_tag},
                     ],
                 'dataset': dataset2,
-                'file_size': u'2012211901', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
-                'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': uid+1, 'event_count': 10},
-                    {'lumi_section_num': u'26422', 'run_num': uid+1, 'event_count': 20},
-                    {'lumi_section_num': u'29838', 'run_num': uid+1, 'event_count': 30}
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': uid+1, 'event_count': 10},
+                    {'lumi_section_num': '26422', 'run_num': uid+1, 'event_count': 20},
+                    {'lumi_section_num': '29838', 'run_num': uid+1, 'event_count': 30}
                     ],
                 'file_parent_list': [ ],
-                'event_count': u'60',
+                'event_count': '60',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/RAW/IDEAL_/%s/%i.root" %(uid+1, i),
                 'block_name': block2,
                 #'is_file_valid': 1
@@ -481,15 +481,15 @@
                      'output_module_label': output_module_label, 'global_tag': global_tag},
                     ],
                 'dataset': dataset,
-                'file_size': u'2012211901', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
-                'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': u'1'},
-                    {'lumi_section_num': u'26422', 'run_num': u'1'},
-                    {'lumi_section_num': u'29838', 'run_num': u'1'}
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
                     ],
                 'file_parent_list': [ ],
-                'event_count': u'1619',
+                'event_count': '1619',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uid, i),
                 'block_name': block,
                 #'is_file_valid': 1
@@ -506,22 +506,22 @@
 
         for i in range(10):
             f={
-                'adler32': u'NOSET', 'file_type': 'EDM',
+                'adler32': 'NOSET', 'file_type': 'EDM',
                 'file_output_config_list':
                 [
                     {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
                      'output_module_label': output_module_label, 'global_tag': global_tag},
                     ],
                 'dataset': child_dataset,
-                'file_size': u'2012211901', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
-                'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': u'1'},
-                    {'lumi_section_num': u'26422', 'run_num': u'1'},
-                    {'lumi_section_num': u'29838', 'run_num': u'1'}
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
                     ],
                 'file_parent_list': [{"file_parent_lfn": "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uid, i)}],
-                'event_count': u'1619',
+                'event_count': '1619',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL-child/%s/%i.root" %(uid, i),
                 'block_name': child_block
                 #'is_file_valid': 1
@@ -546,15 +546,15 @@
                      'output_module_label': output_module_label, 'global_tag': global_tag},
                     ],
                 'dataset': dataset,
-                'file_size': u'2012211901', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
-                'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': u'1'},
-                    {'lumi_section_num': u'26422', 'run_num': u'1'},
-                    {'lumi_section_num': u'29838', 'run_num': u'1'}
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
                     ],
                 'file_parent_list': [ ],
-                'event_count': u'1619',
+                'event_count': '1619',
                 'block_name': block,
                 #'is_file_valid': 1
                 }
@@ -577,15 +577,15 @@
                      'output_module_label': output_module_label, 'global_tag': global_tag},
                     ],
                 'dataset': dataset,
-                'file_size': u'2012211901', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
-                'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': u'1'},
-                    {'lumi_section_num': u'26422', 'run_num': u'1'},
-                    {'lumi_section_num': u'29838', 'run_num': u'1'}
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
                     ],
                 'file_parent_list': [ ],
-                'event_count': u'1619',
+                'event_count': '1619',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uid, i),
                 #'is_file_valid': 1
                 }
@@ -607,15 +607,15 @@
                     {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
                      'output_module_label': output_module_label, 'global_tag': global_tag},
                     ],
-                'file_size': u'2012211901', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
-                'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': u'1'},
-                    {'lumi_section_num': u'26422', 'run_num': u'1'},
-                    {'lumi_section_num': u'29838', 'run_num': u'1'}
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
                     ],
                 'file_parent_list': [ ],
-                'event_count': u'1619',
+                'event_count': '1619',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uid, i),
                 'block_name': block,
                 #'is_file_valid': 1
@@ -814,15 +814,15 @@
             f={
                 'adler32': '', 'file_type': 'EDM',
                 'dataset': dataset,
-                'file_size': u'2012211901', 'auto_cross_section': 0.0,
-                'check_sum': u'1504266448',
-                'file_lumi_list': [
-                    {'lumi_section_num': u'27414', 'run_num': u'1'},
-                    {'lumi_section_num': u'26422', 'run_num': u'1'},
-                    {'lumi_section_num': u'29838', 'run_num': u'1'}
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
                     ],
                 'file_parent_list': [ ],
-                'event_count': u'1619',
+                'event_count': '1619',
                 'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uid, i),
                 }
             fileList.append(f)
@@ -853,26 +853,26 @@
                                       tier)
         bulk_block="%s#%s" % (dataset, uniq_id)
 
-        dataset_dict = {u'dataset': bulk_dataset,
-                        u'physics_group_name': 'Tracker',
-                        u'dataset_access_type': 'VALID', u'processed_ds_name': bulk_procdataset,
-                        u'xtcrosssection': 123, u'data_tier_name': tier,
-                        u'prep_id':prep_id}
-
-        block_dict = data = {u'block_name': bulk_block,
-                             u'origin_site_name': site}
-
-        processing_dict = {u'processing_version': processing_version,
-                           u'description':'this-is-a-test'}
-
-        acquisition_dict = {u'acquisition_era_name': acquisition_era_name, u'start_date':1234567890}
-
-        primary_dict = {u'primary_ds_name':bulk_primary_ds_name,
-                        u'primary_ds_type':primary_ds_type}
-
-        output_module_dict = {u'release_version': release_version, u'pset_hash': pset_hash,
-                              u'app_name': app_name, u'output_module_label': output_module_label,
-                              u'global_tag':global_tag}
+        dataset_dict = {'dataset': bulk_dataset,
+                        'physics_group_name': 'Tracker',
+                        'dataset_access_type': 'VALID', 'processed_ds_name': bulk_procdataset,
+                        'xtcrosssection': 123, 'data_tier_name': tier,
+                        'prep_id':prep_id}
+
+        block_dict = data = {'block_name': bulk_block,
+                             'origin_site_name': site}
+
+        processing_dict = {'processing_version': processing_version,
+                           'description':'this-is-a-test'}
+
+        acquisition_dict = {'acquisition_era_name': acquisition_era_name, 'start_date':1234567890}
+
+        primary_dict = {'primary_ds_name':bulk_primary_ds_name,
+                        'primary_ds_type':primary_ds_type}
+
+        output_module_dict = {'release_version': release_version, 'pset_hash': pset_hash,
+                              'app_name': app_name, 'output_module_label': output_module_label,
+                              'global_tag':global_tag}
 
         fileList = []
         fileConfigList = []
@@ -880,52 +880,52 @@
         for i in range(10):
 	    uniq_id = int(time.time())*1000
             f={
-                u'md5': 'abc', u'file_type': 'EDM',
-                u'file_size': '2012211901', u'auto_cross_section': 0.0,
-                u'check_sum': '1504266448',
-                u'file_lumi_list': [
-                    {u'lumi_section_num': '27414', u'run_num': '1'},
-                    {u'lumi_section_num': '26422', u'run_num': '1'},
-                    {u'lumi_section_num': '29838', u'run_num': '1'}
-                    ],
-                u'event_count': u'1619',
-                u'logical_file_name': "/store/mc/Fall09/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
+                'md5': 'abc', 'file_type': 'EDM',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
+                    ],
+                'event_count': '1619',
+                'logical_file_name': "/store/mc/Fall09/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
                 }
             fileList.append(f)
 
         for i in range(2):
 	    uniq_id = int(time.time())*1000	
             f={
-                u'file_type': 'EDM',
-                u'file_size': '2012211901', u'auto_cross_section': 0.0,
-                u'adler32': 'abc123',
-                u'file_lumi_list': [
-                    {u'lumi_section_num': '27414', u'run_num': '1'},
-                    {u'lumi_section_num': '26422', u'run_num': '1'},
-                    {u'lumi_section_num': '29838', u'run_num': '1'}
-                    ],
-                u'event_count': u'1619',
-                u'logical_file_name': "/store/mc/Fall/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
+                'file_type': 'EDM',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'adler32': 'abc123',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
+                    ],
+                'event_count': '1619',
+                'logical_file_name': "/store/mc/Fall/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
                 }
             fileList.append(f)
         for i in range(2):
 	    uniq_id = int(time.time())*1000	
             f={
-                u'adler32': 'abc1234', u'file_type': 'EDM',
-                u'file_size': '2012211901', u'auto_cross_section': 0.0,
-                u'check_sum': '1504266448',
-                u'file_lumi_list': [
-                    {u'lumi_section_num': '27414', u'run_num': '1'},
-                    {u'lumi_section_num': '26422', u'run_num': '1'},
-                    {u'lumi_section_num': '29838', u'run_num': '1'}
-                    ],
-                u'event_count': u'1619',
-                u'logical_file_name': "/store/mc/Fall15/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
+                'adler32': 'abc1234', 'file_type': 'EDM',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
+                    ],
+                'event_count': '1619',
+                'logical_file_name': "/store/mc/Fall15/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
                 }
             fileList.append(f)
 
-            file_output_dict = {u'release_version': release_version, u'pset_hash': pset_hash, u'app_name': app_name,
-                                u'output_module_label': output_module_label, u'global_tag': global_tag, u'lfn':f["logical_file_name"]}
+            file_output_dict = {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
+                                'output_module_label': output_module_label, 'global_tag': global_tag, 'lfn':f["logical_file_name"]}
             fileConfigList.append(file_output_dict)
 
         data = {'file_conf_list': fileConfigList,
@@ -953,78 +953,78 @@
         bulk_block="%s#%s" % (bulk_dataset, uniq_id)
         print('dataset = ' , bulk_dataset)
         print('block = ',  bulk_block)
-        dataset_dict = {u'dataset': bulk_dataset,
-                        u'physics_group_name': 'Tracker',
-                        u'dataset_access_type': 'VALID', u'processed_ds_name': bulk_procdataset,
-                        u'xtcrosssection': 123, u'data_tier_name': tier2,
-                        u'prep_id':prep_id}
-
-        block_dict = data = {u'block_name': bulk_block,
-                             u'origin_site_name': site}
-
-        processing_dict = {u'processing_version': processing_version,
-                           u'description':'this-is-a-test'}
-
-        acquisition_dict = {u'acquisition_era_name': acquisition_era_name, u'start_date':1234567890}
-
-        primary_dict = {u'primary_ds_name':bulk_primary_ds_name,
-                        u'primary_ds_type':primary_ds_type}
-
-        output_module_dict = {u'release_version': release_version, u'pset_hash': pset_hash,
-                              u'app_name': app_name, u'output_module_label': output_module_label,
-                              u'global_tag':global_tag}
+        dataset_dict = {'dataset': bulk_dataset,
+                        'physics_group_name': 'Tracker',
+                        'dataset_access_type': 'VALID', 'processed_ds_name': bulk_procdataset,
+                        'xtcrosssection': 123, 'data_tier_name': tier2,
+                        'prep_id':prep_id}
+
+        block_dict = data = {'block_name': bulk_block,
+                             'origin_site_name': site}
+
+        processing_dict = {'processing_version': processing_version,
+                           'description':'this-is-a-test'}
+
+        acquisition_dict = {'acquisition_era_name': acquisition_era_name, 'start_date':1234567890}
+
+        primary_dict = {'primary_ds_name':bulk_primary_ds_name,
+                        'primary_ds_type':primary_ds_type}
+
+        output_module_dict = {'release_version': release_version, 'pset_hash': pset_hash,
+                              'app_name': app_name, 'output_module_label': output_module_label,
+                              'global_tag':global_tag}
         fileList = []
         fileConfigList = []
 
         for i in range(20,30):
             uniq_id = int(time.time())*1000
             f={
-                u'md5': 'abc', u'file_type': 'EDM',
-                u'file_size': '2012211901', u'auto_cross_section': 0.0,
-                u'check_sum': '1504266448',
-                u'file_lumi_list': [
-                    {u'lumi_section_num': '27', u'run_num': '11', u'event_count': '100' },
-                    {u'lumi_section_num': '28', u'run_num': '11', u'event_count': '111'},
-                    {u'lumi_section_num': '29', u'run_num': '11', u'event_count': '222'}
-                    ],
-                u'event_count': u'1619',
-                u'logical_file_name': "/store/mc/Fall09/BBJets250to500-madgraph/RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
+                'md5': 'abc', 'file_type': 'EDM',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27', 'run_num': '11', 'event_count': '100' },
+                    {'lumi_section_num': '28', 'run_num': '11', 'event_count': '111'},
+                    {'lumi_section_num': '29', 'run_num': '11', 'event_count': '222'}
+                    ],
+                'event_count': '1619',
+                'logical_file_name': "/store/mc/Fall09/BBJets250to500-madgraph/RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
                 }
             fileList.append(f)
 
         for i in range(40,42):
             uniq_id = int(time.time())*10000
             f={
-                u'file_type': 'EDM',
-                u'file_size': '2012211901', u'auto_cross_section': 0.0,
-                u'adler32': 'abc123',
-                u'file_lumi_list': [
-                    {u'lumi_section_num': '270', u'run_num': '12', u'event_count': '300'},
-                    {u'lumi_section_num': '280', u'run_num': '12', u'event_count': '301'},
-                    {u'lumi_section_num': '290', u'run_num': '12', u'event_count': '302'}
-                    ],
-                u'event_count': u'1619',
-                u'logical_file_name': "/store/mc/Fall/BBJets250to500-madgraph/RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
+                'file_type': 'EDM',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'adler32': 'abc123',
+                'file_lumi_list': [
+                    {'lumi_section_num': '270', 'run_num': '12', 'event_count': '300'},
+                    {'lumi_section_num': '280', 'run_num': '12', 'event_count': '301'},
+                    {'lumi_section_num': '290', 'run_num': '12', 'event_count': '302'}
+                    ],
+                'event_count': '1619',
+                'logical_file_name': "/store/mc/Fall/BBJets250to500-madgraph/RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
                 }
             fileList.append(f)
         for i in range(50,52):
             uniq_id = int(time.time())*1000
             f={
-                u'adler32': 'abc1234', u'file_type': 'EDM',
-                u'file_size': '2012211901', u'auto_cross_section': 0.0,
-                u'check_sum': '1504266448',
-                u'file_lumi_list': [
-                    {u'lumi_section_num': '27414', u'run_num': '13', u'event_count': '400'},
-                    {u'lumi_section_num': '26422', u'run_num': '13', u'event_count': '401'},
-                    {u'lumi_section_num': '29838', u'run_num': '13', u'event_count': '402'}
-                    ],
-                u'event_count': u'1619',
-                u'logical_file_name': "/store/mc/Fall15/BBJets250to500-madgraph/RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
+                'adler32': 'abc1234', 'file_type': 'EDM',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '13', 'event_count': '400'},
+                    {'lumi_section_num': '26422', 'run_num': '13', 'event_count': '401'},
+                    {'lumi_section_num': '29838', 'run_num': '13', 'event_count': '402'}
+                    ],
+                'event_count': '1619',
+                'logical_file_name': "/store/mc/Fall15/BBJets250to500-madgraph/RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
                 }
             fileList.append(f)
 
-            file_output_dict = {u'release_version': release_version, u'pset_hash': pset_hash, u'app_name': app_name,
-                                u'output_module_label': output_module_label, u'global_tag': global_tag, u'lfn':f["logical_file_name"]}
+            file_output_dict = {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
+                                'output_module_label': output_module_label, 'global_tag': global_tag, 'lfn':f["logical_file_name"]}
             fileConfigList.append(file_output_dict)
 
         data = {'file_conf_list': fileConfigList,
@@ -1053,46 +1053,46 @@
                                       tier)
         bulk_block="%s#%s" % (dataset, uniq_id)
 
-        dataset_dict = {u'dataset': bulk_dataset,
-                        u'physics_group_name': 'Tracker',
-                        u'dataset_access_type': 'VALID', u'processed_ds_name': bulk_procdataset,
-                        u'xtcrosssection': 123, u'data_tier_name': tier,
-                        u'prep_id':prep_id}
-
-        block_dict = data = {u'block_name': bulk_block,
-                             u'origin_site_name': site}
-
-        processing_dict = {u'processing_version': processing_version,
-                           u'description':'this-is-a-test'}
-
-        acquisition_dict = {u'acquisition_era_name': acquisition_era_name, u'start_date':1234567890}
-
-        primary_dict = {u'primary_ds_name':bulk_primary_ds_name,
-                        u'primary_ds_type':primary_ds_type}
-
-        output_module_dict = {u'release_version': release_version, u'pset_hash': pset_hash,
-                              u'app_name': app_name, u'output_module_label': output_module_label,
-                              u'global_tag':global_tag}
+        dataset_dict = {'dataset': bulk_dataset,
+                        'physics_group_name': 'Tracker',
+                        'dataset_access_type': 'VALID', 'processed_ds_name': bulk_procdataset,
+                        'xtcrosssection': 123, 'data_tier_name': tier,
+                        'prep_id':prep_id}
+
+        block_dict = data = {'block_name': bulk_block,
+                             'origin_site_name': site}
+
+        processing_dict = {'processing_version': processing_version,
+                           'description':'this-is-a-test'}
+
+        acquisition_dict = {'acquisition_era_name': acquisition_era_name, 'start_date':1234567890}
+
+        primary_dict = {'primary_ds_name':bulk_primary_ds_name,
+                        'primary_ds_type':primary_ds_type}
+
+        output_module_dict = {'release_version': release_version, 'pset_hash': pset_hash,
+                              'app_name': app_name, 'output_module_label': output_module_label,
+                              'global_tag':global_tag}
 
         fileList = []
         fileConfigList = []
 
         for i in range(2):
             f={
-                u'file_type': 'EDM',
-                u'file_size': '2012211901', u'auto_cross_section': 0.0,
-                u'file_lumi_list': [
-                    {u'lumi_section_num': '27414', u'run_num': '1'},
-                    {u'lumi_section_num': '26422', u'run_num': '1'},
-                    {u'lumi_section_num': '29838', u'run_num': '1'}
-                    ],
-                u'event_count': u'1619',
-                u'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
+                'file_type': 'EDM',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
+                    ],
+                'event_count': '1619',
+                'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/%s/%i.root" %(uniq_id, i),
                 }
             fileList.append(f)
 
-            file_output_dict = {u'release_version': release_version, u'pset_hash': pset_hash, u'app_name': app_name,
-                                u'output_module_label': output_module_label, u'global_tag': global_tag, u'lfn':f["logical_file_name"]}
+            file_output_dict = {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
+                                'output_module_label': output_module_label, 'global_tag': global_tag, 'lfn':f["logical_file_name"]}
             fileConfigList.append(file_output_dict)
 
         data = {'file_conf_list': fileConfigList,
@@ -1110,47 +1110,47 @@
     def test12d(self):
         """test12d: web.DBSWriterModel.insertBulkBlock: basic test for inserting child dataset with files"""
 
-        dataset_dict = {u'dataset': parent_stepchain_dataset,
-                        u'physics_group_name': 'Tracker',
-                        u'dataset_access_type': 'VALID', u'processed_ds_name': parent_procdataset,
-                        u'xtcrosssection': 123, u'data_tier_name': tier,
-                        u'prep_id':prep_id}
-
-        block_dict = data = {u'block_name': parent_stepchain_block,
-                             u'origin_site_name': site}
-
-        processing_dict = {u'processing_version': processing_version,
-                           u'description':'this-is-a-test'}
-
-        acquisition_dict = {u'acquisition_era_name': acquisition_era_name, u'start_date':1234567890}
-
-        primary_dict = {u'primary_ds_name':primary_ds_name + "_stepchain",
-                        u'primary_ds_type':primary_ds_type}
-
-        output_module_dict = {u'release_version': release_version, u'pset_hash': pset_hash,
-                              u'app_name': app_name, u'output_module_label': output_module_label,
-                              u'global_tag':global_tag}
+        dataset_dict = {'dataset': parent_stepchain_dataset,
+                        'physics_group_name': 'Tracker',
+                        'dataset_access_type': 'VALID', 'processed_ds_name': parent_procdataset,
+                        'xtcrosssection': 123, 'data_tier_name': tier,
+                        'prep_id':prep_id}
+
+        block_dict = data = {'block_name': parent_stepchain_block,
+                             'origin_site_name': site}
+
+        processing_dict = {'processing_version': processing_version,
+                           'description':'this-is-a-test'}
+
+        acquisition_dict = {'acquisition_era_name': acquisition_era_name, 'start_date':1234567890}
+
+        primary_dict = {'primary_ds_name':primary_ds_name + "_stepchain",
+                        'primary_ds_type':primary_ds_type}
+
+        output_module_dict = {'release_version': release_version, 'pset_hash': pset_hash,
+                              'app_name': app_name, 'output_module_label': output_module_label,
+                              'global_tag':global_tag}
 
         fileList = []
         fileConfigList = []
 
         for i in range(2):
             f={
-                u'file_type': 'EDM',
-                u'file_size': '2012211901', u'auto_cross_section': 0.0,
-                u'check_sum': u'1504266448',
-                u'file_lumi_list': [
-                    {u'lumi_section_num': '27414', u'run_num': '1'},
-                    {u'lumi_section_num': '26422', u'run_num': '1'},
-                    {u'lumi_section_num': '29838', u'run_num': '1'}
-                    ],
-                u'event_count': u'1619',
-                u'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/parent_%s/%i.root" %(uid, i),
+                'file_type': 'EDM',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
+                    ],
+                'event_count': '1619',
+                'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/parent_%s/%i.root" %(uid, i),
                 }
             fileList.append(f)
 
-            file_output_dict = {u'release_version': release_version, u'pset_hash': pset_hash, u'app_name': app_name,
-                                u'output_module_label': output_module_label, u'global_tag': global_tag, u'lfn':f["logical_file_name"]}
+            file_output_dict = {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
+                                'output_module_label': output_module_label, 'global_tag': global_tag, 'lfn':f["logical_file_name"]}
             fileConfigList.append(file_output_dict)
 
         data = {'file_conf_list': fileConfigList,
@@ -1169,47 +1169,47 @@
     def test12e(self):
         """test12e: web.DBSWriterModel.insertBulkBlock: insert bulk block with parent dataset negative test"""
 
-        dataset_dict = {u'dataset': stepchain_dataset,
-                        u'physics_group_name': 'Tracker',
-                        u'dataset_access_type': 'VALID', u'processed_ds_name': procdataset,
-                        u'xtcrosssection': 123, u'data_tier_name': tier,
-                        u'prep_id':prep_id}
-
-        block_dict = data = {u'block_name': stepchain_block,
-                             u'origin_site_name': site}
-
-        processing_dict = {u'processing_version': processing_version,RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/unittests/web_t/__init__.py
RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/unittests/web_t/listprofiling.py

-                           u'description':'this-is-a-test'}
-
-        acquisition_dict = {u'acquisition_era_name': acquisition_era_name, u'start_date':1234567890}
-
-        primary_dict = {u'primary_ds_name':primary_ds_name,
-                        u'primary_ds_type':primary_ds_type}
-
-        output_module_dict = {u'release_version': release_version, u'pset_hash': pset_hash,
-                              u'app_name': app_name, u'output_module_label': output_module_label,
-                              u'global_tag':global_tag}
+        dataset_dict = {'dataset': stepchain_dataset,
+                        'physics_group_name': 'Tracker',
+                        'dataset_access_type': 'VALID', 'processed_ds_name': procdataset,
+                        'xtcrosssection': 123, 'data_tier_name': tier,
+                        'prep_id':prep_id}
+
+        block_dict = data = {'block_name': stepchain_block,
+                             'origin_site_name': site}
+
+        processing_dict = {'processing_version': processing_version,
+                           'description':'this-is-a-test'}
+
+        acquisition_dict = {'acquisition_era_name': acquisition_era_name, 'start_date':1234567890}
+
+        primary_dict = {'primary_ds_name':primary_ds_name,
+                        'primary_ds_type':primary_ds_type}
+
+        output_module_dict = {'release_version': release_version, 'pset_hash': pset_hash,
+                              'app_name': app_name, 'output_module_label': output_module_label,
+                              'global_tag':global_tag}
 
         fileList = []
         fileConfigList = []
 
         for i in range(2):
             f={
-                u'file_type': 'EDM',
-                u'file_size': '2012211901', u'auto_cross_section': 0.0,
-                u'check_sum': u'1504266448',
-                u'file_lumi_list': [
-                    {u'lumi_section_num': '27414', u'run_num': '1'},
-                    {u'lumi_section_num': '26422', u'run_num': '1'},
-                    {u'lumi_section_num': '29838', u'run_num': '1'}
-                    ],
-                u'event_count': u'1619',
-                u'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/child_%s/%i.root" %(uid, i),
+                'file_type': 'EDM',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
+                    ],
+                'event_count': '1619',
+                'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/child_%s/%i.root" %(uid, i),
                 }
             fileList.append(f)
 
-            file_output_dict = {u'release_version': release_version, u'pset_hash': pset_hash, u'app_name': app_name,
-                                u'output_module_label': output_module_label, u'global_tag': global_tag, u'lfn':f["logical_file_name"]}
+            file_output_dict = {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
+                                'output_module_label': output_module_label, 'global_tag': global_tag, 'lfn':f["logical_file_name"]}
             fileConfigList.append(file_output_dict)
 
         data = {'file_conf_list': fileConfigList,
@@ -1231,47 +1231,47 @@
     def test12f(self):
         """test12f: web.DBSWriterModel.insertBulkBlock: insert bulk block with parent dataset"""
 
-        dataset_dict = {u'dataset': stepchain_dataset,
-                        u'physics_group_name': 'Tracker',
-                        u'dataset_access_type': 'VALID', u'processed_ds_name': procdataset,
-                        u'xtcrosssection': 123, u'data_tier_name': tier,
-                        u'prep_id':prep_id}
-
-        block_dict = data = {u'block_name': stepchain_block,
-                             u'origin_site_name': site}
-
-        processing_dict = {u'processing_version': processing_version,
-                           u'description':'this-is-a-test'}
-
-        acquisition_dict = {u'acquisition_era_name': acquisition_era_name, u'start_date':1234567890}
-
-        primary_dict = {u'primary_ds_name':primary_ds_name + "_stepchain",
-                        u'primary_ds_type':primary_ds_type}
-
-        output_module_dict = {u'release_version': release_version, u'pset_hash': pset_hash,
-                              u'app_name': app_name, u'output_module_label': output_module_label,
-                              u'global_tag':global_tag}
+        dataset_dict = {'dataset': stepchain_dataset,
+                        'physics_group_name': 'Tracker',
+                        'dataset_access_type': 'VALID', 'processed_ds_name': procdataset,
+                        'xtcrosssection': 123, 'data_tier_name': tier,
+                        'prep_id':prep_id}
+
+        block_dict = data = {'block_name': stepchain_block,
+                             'origin_site_name': site}
+
+        processing_dict = {'processing_version': processing_version,
+                           'description':'this-is-a-test'}
+
+        acquisition_dict = {'acquisition_era_name': acquisition_era_name, 'start_date':1234567890}
+
+        primary_dict = {'primary_ds_name':primary_ds_name + "_stepchain",
+                        'primary_ds_type':primary_ds_type}
+
+        output_module_dict = {'release_version': release_version, 'pset_hash': pset_hash,
+                              'app_name': app_name, 'output_module_label': output_module_label,
+                              'global_tag':global_tag}
 
         fileList = []
         fileConfigList = []
 
         for i in range(2):
             f={
-                u'file_type': 'EDM',
-                u'file_size': '2012211901', u'auto_cross_section': 0.0,
-                u'check_sum': u'1504266448',
-                u'file_lumi_list': [
-                    {u'lumi_section_num': '27414', u'run_num': '1'},
-                    {u'lumi_section_num': '26422', u'run_num': '1'},
-                    {u'lumi_section_num': '29838', u'run_num': '1'}
-                    ],
-                u'event_count': u'1619',
-                u'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/child_%s/%i.root" %(uid, i),
+                'file_type': 'EDM',
+                'file_size': '2012211901', 'auto_cross_section': 0.0,
+                'check_sum': '1504266448',
+                'file_lumi_list': [
+                    {'lumi_section_num': '27414', 'run_num': '1'},
+                    {'lumi_section_num': '26422', 'run_num': '1'},
+                    {'lumi_section_num': '29838', 'run_num': '1'}
+                    ],
+                'event_count': '1619',
+                'logical_file_name': "/store/mc/Fall08/BBJets250to500-madgraph/GEN-SIM-RAW/IDEAL_/child_%s/%i.root" %(uid, i),
                 }
             fileList.append(f)
 
-            file_output_dict = {u'release_version': release_version, u'pset_hash': pset_hash, u'app_name': app_name,
-                                u'output_module_label': output_module_label, u'global_tag': global_tag, u'lfn':f["logical_file_name"]}
+            file_output_dict = {'release_version': release_version, 'pset_hash': pset_hash, 'app_name': app_name,
+                                'output_module_label': output_module_label, 'global_tag': global_tag, 'lfn':f["logical_file_name"]}
             fileConfigList.append(file_output_dict)
 
         data = {'file_conf_list': fileConfigList,
--- ./Server/Python/tests/dbsserver_t/unittests/web_t/__init__.py	(original)
+++ ./Server/Python/tests/dbsserver_t/unittests/web_t/__init__.py	(refactored)
@@ -1,7 +1,7 @@
 """
 web unittests package.
 """
-from __future__ import print_function
+
 
 __all__ = ['DBSWriterModel_t', 'DBSReaderModel_t']
 
--- ./Server/Python/tests/dbsserver_t/unittests/web_t/listprofiling.py	(original)
+++ ./Server/Python/tests/dbsserver_t/unittests/web_t/listprofiling.py	(refactored)RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/utils/CXOracleSQL.py
RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/utils/CreateTestData.py
RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/utils/DBSDataProvider.py
RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/utils/DBSRestApi.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/utils/DaoConfig.py
RefactoringTool: Refactored ./Server/Python/tests/dbsserver_t/utils/ParseConfig.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/utils/TestTools.py
RefactoringTool: No changes to ./Server/Python/tests/dbsserver_t/utils/__init__.py
RefactoringTool: Refactored ./Server/Python/utils/das_logfile_analyser.py

@@ -1,4 +1,4 @@
-from __future__ import absolute_import
+
 import cProfile
 from .DBSReaderModel_t import DBSReaderModel_test
 from  .DBSReaderModel_t import importCode
--- ./Server/Python/tests/dbsserver_t/utils/CXOracleSQL.py	(original)
+++ ./Server/Python/tests/dbsserver_t/utils/CXOracleSQL.py	(refactored)
@@ -1,5 +1,5 @@
 """Module provides the the class for sql/ tests with cx_Oracle"""
-from __future__ import print_function
+
 
 __revision__ = "$Id: CXOracleSQL.py,v 1.1 2010/01/01 19:54:38 akhukhun Exp $"
 __version__ = "$Revision: 1.1 $"
--- ./Server/Python/tests/dbsserver_t/utils/CreateTestData.py	(original)
+++ ./Server/Python/tests/dbsserver_t/utils/CreateTestData.py	(refactored)
@@ -1,11 +1,11 @@
 import os
-import cPickle
+import pickle
 import uuid
 
 persistent_data = {'physics_group':
              [{'physics_group_name': 'Bphys'}, {'physics_group_name': 'Btag'}, {'physics_group_name': 'Diffraction'}, {'physics_group_name': 'EWK'}, {'physics_group_name': 'Egamma'}, {'physics_group_name': 'HeavyIon'}, {'physics_group_name': 'Higgs'}, {'physics_group_name': 'Individual'}, {'physics_group_name': 'JetMet'}, {'physics_group_name': 'Muons'}, {'physics_group_name': 'OnSel'}, {'physics_group_name': 'PFlowTau'}, {'physics_group_name': 'PhysVal'}, {'physics_group_name': 'QCD'}, {'physics_group_name': 'RelVal'}, {'physics_group_name': 'SUSYBSM'}, {'physics_group_name': 'Top'}, {'physics_group_name': 'Tracker'}],
              'primary_ds_type':
-             [{u'data_type': 'mc'}, {u'data_type': 'data'}, {u'data_type': 'test'}]
+             [{'data_type': 'mc'}, {'data_type': 'data'}, {'data_type': 'test'}]
             }
 
 acquisition_era_name = "DBS3UNITTESTACQERA_@unique_id@"
@@ -183,10 +183,10 @@
              }
 
 f = file(os.path.join(os.path.dirname(os.path.abspath(__file__)), '../data/template_transient_test_data.pkl'), "w")
-cPickle.dump(transient_data, f)
+pickle.dump(transient_data, f)
 f.close()
 
 f = file(os.path.join(os.path.dirname(os.path.abspath(__file__)), '../data/persistent_test_data.pkl'), "w")
-cPickle.dump(persistent_data, f)
+pickle.dump(persistent_data, f)
 f.close()
 
--- ./Server/Python/tests/dbsserver_t/utils/DBSDataProvider.py	(original)
+++ ./Server/Python/tests/dbsserver_t/utils/DBSDataProvider.py	(refactored)
@@ -1,9 +1,9 @@
 """
 Class to provide data for unit- and integration tests
 """
-from itertools import izip
+
 from collections import defaultdict
-import cPickle as pickle
+import pickle as pickle
 import getpass
 import os.path
 import uuid
@@ -29,7 +29,7 @@
     if isinstance(data, list):
         return [strip_volatile_fields(entry) for entry in data]
 
-    for key in data.keys():
+    for key in list(data.keys()):
         if key in volatile_fields:
             del data[key]
 
@@ -91,7 +91,7 @@
         generated_data = []
 
         for list_entry in template_data:
-            for entry, value in list_entry.iteritems():
+            for entry, value in list_entry.items():
                 if isinstance(value, str):
                     if value.find("@unique_id@") != -1:
                         list_entry[entry] = list_entry[entry].replace("@unique_id@", self.unixtime)
@@ -224,13 +224,13 @@
     parent_block_dump = parent_data_provider.block_dump()
     child_block_dump = child_data_provider.block_dump()
 
-    for parent_block, child_block in izip(parent_block_dump, child_block_dump):
+    for parent_block, child_block in zip(parent_block_dump, child_block_dump):
         parent_logical_file_names = (this_file['logical_file_name'] for this_file in parent_block['files'])
         child_logical_file_names = (this_file['logical_file_name'] for this_file in child_block['files'])
 
         file_parent_list = []
 
-        for parent_logical_file_name, child_logical_file_name in izip(parent_logical_file_names,
+        for parent_logical_file_name, child_logical_file_name in zip(parent_logical_file_names,
                                                                       child_logical_file_names):
             file_parent_list.append(dict(parent_logical_file_name=parent_logical_file_name,
                                          logical_file_name=child_logical_file_name))
@@ -309,7 +309,7 @@
         if not (hasattr(self, '_files') and block_name in self._files):
             self._files[block_name] = []
             num_of_created_blocks = len(self._files)
-            for i in xrange((num_of_created_blocks-1) * self._num_of_files,
+            for i in range((num_of_created_blocks-1) * self._num_of_files,
                             num_of_created_blocks * self._num_of_files):
                 logical_file_name = self._generate_file_name(i)
                 self._files[block_name].append({'check_sum' : self._generate_cksum(),
@@ -389,7 +389,7 @@
     def _generate_file_lumi_list(self):
         "generate file lumi list for a given file, if not already available"
         output = []
-        for _ in xrange(0, self._num_of_runs):
+        for _ in range(0, self._num_of_runs):
             self._run_num += 1
             for _ in range(0, self._num_of_lumis):
                 self._lumi_sec += 1
@@ -425,7 +425,7 @@
         "return list of blocks"
         if not hasattr(self, '_blocks'):
             self._blocks = []
-            for i in xrange(self._num_of_blocks):
+            for i in range(self._num_of_blocks):
                 self._blocks.append(self._generate_block_name())
         return self._blocks
 
--- ./Server/Python/tests/dbsserver_t/utils/DBSRestApi.py	(original)
+++ ./Server/Python/tests/dbsserver_t/utils/DBSRestApi.py	(refactored)
@@ -2,7 +2,7 @@
 This module provides a stand-alone client for DBS server
 Also DBSRestApi will be used in various stand-alone tests
 """
-from __future__ import print_function
+
 import json
 import os, logging
 import getpass
--- ./Server/Python/tests/dbsserver_t/utils/ParseConfig.py	(original)
+++ ./Server/Python/tests/dbsserver_t/utils/ParseConfig.py	(refactored)
@@ -1,5 +1,5 @@
 """Used to parse the config file and get reader and writer instances informations from it"""
-from __future__ import print_function
+
 
 import sys
 from WMCore.Configuration import loadConfigurationFile
--- ./Server/Python/utils/das_logfile_analyser.py	(original)
+++ ./Server/Python/utils/das_logfile_analyser.py	(refactored)
@@ -1,7 +1,7 @@
 #!/usr/bin/env python
 from optparse import OptionParser
-import urllib
-import urlparse
+import urllib.request, urllib.parse, urllib.error
+import urllib.parse
 import yaml
 import json, os, re, sys
 import pprint
@@ -69,7 +69,7 @@
             ###Needs translation using das3dbs_param_map
             das2dbs_param_map = api_call['das2dbs_param_map']
             das2dbs_key_changer = lambda key, map=das2dbs_param_map: map[key] if key in map else key
-            das_param_keys = set(map(das2dbs_key_changer, das_params.keys()))
+            das_param_keys = set(map(das2dbs_key_changer, list(das_params.keys())))
 
             api_params = set(api_call['params'].keys())
 
@@ -82,7 +82,7 @@
 
         dbs_params = {}
 
-        for param in matching_api['params'].keys():
+        for param in list(matching_api['params'].keys()):
             try:
                 dbs_params[param] = das_params[param]
             except KeyError as ke:
@@ -91,7 +91,7 @@
                 if matching_api['params'][param] != 'optional':
                     dbs_params[param] = matching_api['params'][param]
 
-        dbs_url = urlparse.urlparse(matching_api['url'])
+        dbs_url = urllib.parse.urlparse(matching_api['url'])
         path = dbs_url.path
         api_pattern = re.compile(r'^/dbs/\S+/\S+/DBSReader/(?P<api>\S+)/.*')
         match_obj = api_pattern.match(path)
@@ -164,8 +164,8 @@
             match_obj = request_pattern.match(request)
             request = match_obj.groupdict()
             uri = request['uri']
-            parsed_uri = urlparse.urlparse(uri)
-            parsed_query_string = urlparse.parse_qs(parsed_uri.query)
+            parsed_uri = urllib.parse.urlparse(uri)
+            parsed_query_string = urllib.parse.parse_qs(parsed_uri.query)
             lookup, das_params = extract_das_parameters(parsed_query_string['input'][0])RefactoringTool: Refactored ./Server/Python/utils/das_logfile_parser.py
RefactoringTool: Refactored ./Server/Python/utils/dbs2_logfile_analyser.py
RefactoringTool: Refactored ./Server/Python/utils/dbs2_logfile_parser.py
RefactoringTool: Refactored ./Server/Python/utils/dbs3_bulk_block_input_validation.py
RefactoringTool: Refactored ./Server/Python/utils/dbs3_logfile_analyser.py
RefactoringTool: Refactored ./Server/Python/utils/dbs3_logfile_parser.py
RefactoringTool: Refactored ./Server/Python/utils/dbs3_logfile_parser_for_failed_blocks.py
RefactoringTool: Refactored ./SystemTests/bin/LifeCyclePlots.py
RefactoringTool: Refactored ./SystemTests/bin/StatsServer.py
RefactoringTool: Refactored ./SystemTests/bin/das_logfile_analyser.py
RefactoringTool: Refactored ./SystemTests/bin/das_logfile_parser.py
RefactoringTool: Refactored ./SystemTests/bin/dbs2_logfile_parser.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3BulkInsert.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3Crab3Workflow.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3CrabWorkflow.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3DASAccess.py

             dbs_api, dbs_params = das_mapping.create_dbs_query(lookup, das_params)
             if dbs_api:
--- ./Server/Python/utils/das_logfile_parser.py	(original)
+++ ./Server/Python/utils/das_logfile_parser.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from optparse import OptionParser
 import json, os, re, sys
 
--- ./Server/Python/utils/dbs2_logfile_analyser.py	(original)
+++ ./Server/Python/utils/dbs2_logfile_analyser.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from optparse import OptionParser
 import json, os, re, sys, time
 import numpy
--- ./Server/Python/utils/dbs2_logfile_parser.py	(original)
+++ ./Server/Python/utils/dbs2_logfile_parser.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from optparse import OptionParser
 import json, os, re, sys
 
--- ./Server/Python/utils/dbs3_bulk_block_input_validation.py	(original)
+++ ./Server/Python/utils/dbs3_bulk_block_input_validation.py	(refactored)
@@ -2,7 +2,7 @@
 """
 Script to validate the input of bulk block insert API using WMCore block dump
 """
-from __future__ import print_function
+
 from optparse import OptionParser
 from ast import literal_eval
 import glob
--- ./Server/Python/utils/dbs3_logfile_analyser.py	(original)
+++ ./Server/Python/utils/dbs3_logfile_analyser.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from optparse import OptionParser
 import json, os, re, sys, time
 import numpy
--- ./Server/Python/utils/dbs3_logfile_parser.py	(original)
+++ ./Server/Python/utils/dbs3_logfile_parser.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from optparse import OptionParser
 import json, os, re, sys
 
--- ./Server/Python/utils/dbs3_logfile_parser_for_failed_blocks.py	(original)
+++ ./Server/Python/utils/dbs3_logfile_parser_for_failed_blocks.py	(refactored)
@@ -6,7 +6,7 @@
 
 Not yet working since log files ar not consecutive at the moment
 """
-from __future__ import print_function
+
 from optparse import OptionParser
 
 import re
--- ./SystemTests/bin/LifeCyclePlots.py	(original)
+++ ./SystemTests/bin/LifeCyclePlots.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python2.6
-from __future__ import print_function
+
 from ROOT import gROOT, TFile
 from LifeCycleAnalysis.LifeCyclePlots.LifeCyclePlotManager import LifeCyclePlotManager
 from LifeCycleAnalysis.LifeCyclePlots.SqliteDAO import SqliteDAO
@@ -62,9 +62,8 @@
     list_of_errors = sqlite_dao.get_unique_column_list('Failures', 'Value')
 
     ### plot reader or/and writer tests
-    reader_tests = filter(lambda x: x.startswith('list') or x.startswith('status'), list_of_apis)
-    writer_tests = filter(lambda x: x.startswith('insert') or x.startswith('update') or x.startswith('submit'),
-                          list_of_apis)
+    reader_tests = [x for x in list_of_apis if x.startswith('list') or x.startswith('status')]
+    writer_tests = [x for x in list_of_apis if x.startswith('insert') or x.startswith('update') or x.startswith('submit')]
     migration_tests = sqlite_dao.table_exists(table='MigrationStatistics')
 
     statistic_categories = list()
--- ./SystemTests/bin/StatsServer.py	(original)
+++ ./SystemTests/bin/StatsServer.py	(refactored)
@@ -1,8 +1,8 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from optparse import OptionParser, OptionGroup
-from SimpleXMLRPCServer import SimpleXMLRPCServer
-import cPickle
+from xmlrpc.server import SimpleXMLRPCServer
+import pickle
 import sqlite3 as sqlite
 import os, signal, sys
 
@@ -131,7 +131,7 @@
     def handle_request(self):
         while True:
             try:
-                input_data = cPickle.load(self.f)
+                input_data = pickle.load(self.f)
             except EOFError: ### no new data available
                 return
             else:
--- ./SystemTests/bin/das_logfile_analyser.py	(original)
+++ ./SystemTests/bin/das_logfile_analyser.py	(refactored)
@@ -1,7 +1,7 @@
 #!/usr/bin/env python
 from optparse import OptionParser
-import urllib
-import urlparse
+import urllib.request, urllib.parse, urllib.error
+import urllib.parse
 import yaml
 import json, os, re, sys
 import pprint
@@ -69,7 +69,7 @@
             ###Needs translation using das3dbs_param_map
             das2dbs_param_map = api_call['das2dbs_param_map']
             das2dbs_key_changer = lambda key, map=das2dbs_param_map: map[key] if key in map else key
-            das_param_keys = set(map(das2dbs_key_changer, das_params.keys()))
+            das_param_keys = set(map(das2dbs_key_changer, list(das_params.keys())))
 
             api_params = set(api_call['params'].keys())
 
@@ -82,7 +82,7 @@
 
         dbs_params = {}
 
-        for param in matching_api['params'].keys():
+        for param in list(matching_api['params'].keys()):
             try:
                 dbs_params[param] = das_params[param]
             except KeyError as ke:
@@ -91,7 +91,7 @@
                 if matching_api['params'][param] != 'optional':
                     dbs_params[param] = matching_api['params'][param]
 
-        dbs_url = urlparse.urlparse(matching_api['url'])
+        dbs_url = urllib.parse.urlparse(matching_api['url'])
         path = dbs_url.path
         api_pattern = re.compile(r'^/dbs/\S+/\S+/DBSReader/(?P<api>\S+)/.*')
         match_obj = api_pattern.match(path)
@@ -164,8 +164,8 @@
             match_obj = request_pattern.match(request)
             request = match_obj.groupdict()
             uri = request['uri']
-            parsed_uri = urlparse.urlparse(uri)
-            parsed_query_string = urlparse.parse_qs(parsed_uri.query)
+            parsed_uri = urllib.parse.urlparse(uri)
+            parsed_query_string = urllib.parse.parse_qs(parsed_uri.query)
             lookup, das_params = extract_das_parameters(parsed_query_string['input'][0])
             dbs_api, dbs_params = das_mapping.create_dbs_query(lookup, das_params)
             if dbs_api:
--- ./SystemTests/bin/das_logfile_parser.py	(original)
+++ ./SystemTests/bin/das_logfile_parser.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from optparse import OptionParser
 import json, os, re, sys
 
--- ./SystemTests/bin/dbs2_logfile_parser.py	(original)
+++ ./SystemTests/bin/dbs2_logfile_parser.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from optparse import OptionParser
 import json, os, re, sys
 
--- ./SystemTests/bin/dbs3BulkInsert.py	(original)
+++ ./SystemTests/bin/dbs3BulkInsert.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from LifeCycleTests.LifeCycleTools.APIFactory import create_api
 from LifeCycleTests.LifeCycleTools.PayloadHandler import PayloadHandler, increase_interval
 from LifeCycleTests.LifeCycleTools.Timing import TimingStat
--- ./SystemTests/bin/dbs3Crab3Workflow.py	(original)
+++ ./SystemTests/bin/dbs3Crab3Workflow.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from LifeCycleTests.LifeCycleTools.APIFactory import create_api
 from LifeCycleTests.LifeCycleTools.PayloadHandler import PayloadHandler, increase_interval
 from LifeCycleTests.LifeCycleTools.Timing import TimingStat
--- ./SystemTests/bin/dbs3CrabWorkflow.py	(original)
+++ ./SystemTests/bin/dbs3CrabWorkflow.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from LifeCycleTests.LifeCycleTools.APIFactory import create_api
 from LifeCycleTests.LifeCycleTools.PayloadHandler import PayloadHandler, increase_interval
 from LifeCycleTests.LifeCycleTools.Timing import TimingStat
--- ./SystemTests/bin/dbs3DASAccess.py	(original)
+++ ./SystemTests/bin/dbs3DASAccess.py	(refactored)
@@ -8,7 +8,7 @@
 import os
 import sys
 import tempfile
-import urllib
+import urllib.request, urllib.parse, urllib.error
 
 options = get_command_line_options(__name__, sys.argv)
 
@@ -27,11 +27,11 @@
 das_queries = payload_handler.payload['workflow']['DASQueries']
 
 for das_query in das_queries:RefactoringTool: Refactored ./SystemTests/bin/dbs3DASGetQueries.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3GetBlocks.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3GetDatasets.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3GetFileLumis.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3GetFileParents.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3GetFiles.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3GetPrimaryDSType.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3IntroduceFailures.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3MigrationService.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3WriterStressTest.py
RefactoringTool: Refactored ./SystemTests/bin/dbs3_logfile_parser.py
RefactoringTool: No changes to ./SystemTests/bin/dbs3dasComparision.py
RefactoringTool: No changes to ./SystemTests/bin/getFakeData.py
RefactoringTool: No changes to ./SystemTests/src/python/LifeCycleAnalysis/__init__.py
RefactoringTool: No changes to ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/HistoManager.py
RefactoringTool: Refactored ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/Histogram.py
RefactoringTool: Refactored ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/LifeCyclePlotManager.py

-    api_call_name = das_query.keys()[0]
+    api_call_name = list(das_query.keys())[0]
     api_call = getattr(api, api_call_name)
     query = das_query[api_call_name]
 
-    encoded_query = urllib.urlencode(query, doseq=True)
+    encoded_query = urllib.parse.urlencode(query, doseq=True)
 
     timing = {'stats':{'query' : encoded_query, 'api' : api_call_name}}
 
--- ./SystemTests/bin/dbs3DASGetQueries.py	(original)
+++ ./SystemTests/bin/dbs3DASGetQueries.py	(refactored)
@@ -33,7 +33,7 @@
 with open(data_file, 'r') as f:
     queries = json.load(f)
 
-events = ['DASAccess' for _ in xrange(num_of_repetitions)]
+events = ['DASAccess' for _ in range(num_of_repetitions)]
 for chunk in split_list(queries, num_of_threads):
     p = payload_handler.clone_payload()
     p['workflow']['DASQueries'] = chunk
--- ./SystemTests/bin/dbs3GetBlocks.py	(original)
+++ ./SystemTests/bin/dbs3GetBlocks.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from LifeCycleTests.LifeCycleTools.APIFactory import create_api
 from LifeCycleTests.LifeCycleTools.PayloadHandler import PayloadHandler
 from LifeCycleTests.LifeCycleTools.Timing import TimingStat
--- ./SystemTests/bin/dbs3GetDatasets.py	(original)
+++ ./SystemTests/bin/dbs3GetDatasets.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from LifeCycleTests.LifeCycleTools.APIFactory import create_api
 from LifeCycleTests.LifeCycleTools.PayloadHandler import PayloadHandler, increase_interval
 from LifeCycleTests.LifeCycleTools.Timing import TimingStat
@@ -46,8 +46,8 @@
 timer.stat_to_server()
 
 #remove T0TEST datasets, since they are not analysed by users
-datasets = map(lambda x: x['dataset'], datasets)
-datasets = filter(lambda x: x.find('T0TEST')==-1, datasets)
+datasets = [x['dataset'] for x in datasets]
+datasets = [x for x in datasets if x.find('T0TEST')==-1]
 
 #re-arrange the order of datasets, to have a more realistic chaotic use
 shuffle(datasets)
--- ./SystemTests/bin/dbs3GetFileLumis.py	(original)
+++ ./SystemTests/bin/dbs3GetFileLumis.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from LifeCycleTests.LifeCycleTools.APIFactory import create_api
 from LifeCycleTests.LifeCycleTools.PayloadHandler import PayloadHandler
 from LifeCycleTests.LifeCycleTools.Timing import TimingStat
--- ./SystemTests/bin/dbs3GetFileParents.py	(original)
+++ ./SystemTests/bin/dbs3GetFileParents.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from LifeCycleTests.LifeCycleTools.APIFactory import create_api
 from LifeCycleTests.LifeCycleTools.PayloadHandler import PayloadHandler
 from LifeCycleTests.LifeCycleTools.Timing import TimingStat
--- ./SystemTests/bin/dbs3GetFiles.py	(original)
+++ ./SystemTests/bin/dbs3GetFiles.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from LifeCycleTests.LifeCycleTools.APIFactory import create_api
 from LifeCycleTests.LifeCycleTools.PayloadHandler import PayloadHandler, increase_interval
 from LifeCycleTests.LifeCycleTools.Timing import TimingStat
--- ./SystemTests/bin/dbs3GetPrimaryDSType.py	(original)
+++ ./SystemTests/bin/dbs3GetPrimaryDSType.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from LifeCycleTools.APIFactory import create_api
 from LifeCycleTools.PayloadHandler import PayloadHandler
 from LifeCycleTools.Timing import TimingStat
--- ./SystemTests/bin/dbs3IntroduceFailures.py	(original)
+++ ./SystemTests/bin/dbs3IntroduceFailures.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from LifeCycleTests.LifeCycleTools.PayloadHandler import PayloadHandler
 from LifeCycleTests.LifeCycleTools.OptParser import get_command_line_options
 
--- ./SystemTests/bin/dbs3MigrationService.py	(original)
+++ ./SystemTests/bin/dbs3MigrationService.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from LifeCycleTests.LifeCycleTools.APIFactory import create_api
 from LifeCycleTests.LifeCycleTools.PayloadHandler import PayloadHandler, increase_interval
 from LifeCycleTests.LifeCycleTools.Timing import TimingStat
--- ./SystemTests/bin/dbs3WriterStressTest.py	(original)
+++ ./SystemTests/bin/dbs3WriterStressTest.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from LifeCycleTests.LifeCycleTools.PayloadHandler import PayloadHandler, increase_interval
 from LifeCycleTests.LifeCycleTools.OptParser import get_command_line_options
 
@@ -20,10 +20,10 @@
 
 events = []
 
-for cycle in xrange(number_of_cycles):
+for cycle in range(number_of_cycles):
     events.extend(('payload_provider', 'dbs3BulkInsert'))
 
-for workflow in xrange(number_of_workflows):
+for workflow in range(number_of_workflows):
     p = payload_handler.clone_payload()
     p['workflow']['Events'] = events
     payload_handler.append_payload(p)
--- ./SystemTests/bin/dbs3_logfile_parser.py	(original)
+++ ./SystemTests/bin/dbs3_logfile_parser.py	(refactored)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-from __future__ import print_function
+
 from optparse import OptionParser
 import json, os, re, sys
 
--- ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/Histogram.py	(original)
+++ ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/Histogram.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 from ROOT import gStyle, TCanvas, TFile, TH1F, TH2F, THStack, TPaveStats
 
 class BasicHisto(object):
@@ -61,7 +61,7 @@
         self._canvas.SetLogy(self._log.get('y', False))
 
         # set additional options
-        for key, value in self._add_options.iteritems():
+        for key, value in self._add_options.items():
             # initialize callable with histogramm to start for-loop
             # to resolve nested calls like GetXaxis().SetLabelSize()
             callable_fkt = self._histogram
--- ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/LifeCyclePlotManager.py	(original)
+++ ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/LifeCyclePlotManager.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 from LifeCycleAnalysis.LifeCyclePlots.HistoManager import HistoManager
 from LifeCycleAnalysis.LifeCyclePlots.Histogram import Histo1D, Histo2D
 
@@ -23,7 +23,7 @@
         self._list_of_apis = list_of_apis
         # api as keys and numbers as value, to fill 0,1,2,3,4 bins in APIAccessCounter histogramm and
         # to set bin label later accordingly
-        self._enumerated_dict_of_apis = dict(list(zip(self._list_of_apis, xrange(len(self._list_of_apis)))))
+        self._enumerated_dict_of_apis = dict(list(zip(self._list_of_apis, range(len(self._list_of_apis)))))
         self._starttime = starttime
         self._endtime = endtime
         self._test_type = test_type
@@ -264,14 +264,13 @@
     _supported_categories = ['reader_stats', 'writer_stats', 'failures', 'migration_stats']
 
     def __init__(self, categories, list_of_apis, list_of_errors, starttime, endtime):
-        plot_creator = {'reader_stats': StatisticPlots(filter(lambda x: x.startswith('list') or x.startswith('status'),
-                                                              list_of_apis),
+        plot_creator = {'reader_stats': StatisticPlots([x for x in list_of_apis if x.startswith('list') or x.startswith('status')],
                                                         starttime=starttime,
                                                         endtime=endtime,
                                                         test_type='reader_stats'),
-                        'writer_stats': StatisticPlots(filter(lambda x: x.startswith('insert')
+                        'writer_stats': StatisticPlots([x for x in list_of_apis if x.startswith('insert')
                                                                         or x.startswith('update')
-                                                                        or x.startswith('submit'), list_of_apis),
+                                                                        or x.startswith('submit')],RefactoringTool: No changes to ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/SqliteDAO.py
RefactoringTool: No changes to ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/WebView.py
RefactoringTool: No changes to ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/__init__.py
RefactoringTool: No changes to ./SystemTests/src/python/LifeCycleTests/__init__.py
RefactoringTool: Refactored ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/APIFactory.py
RefactoringTool: Refactored ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/ConfigurationFactory.py
RefactoringTool: Refactored ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/OptParser.py
RefactoringTool: Refactored ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/PayloadHandler.py
RefactoringTool: Refactored ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/Stats2Sqlite.py
RefactoringTool: Refactored ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/StatsClient.py
RefactoringTool: Refactored ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/Timing.py
RefactoringTool: No changes to ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/__init__.py
RefactoringTool: Refactored ./doc/generate_modules.py
RefactoringTool: Refactored ./doc/dbs/conf.py
RefactoringTool: Files that were modified:
RefactoringTool: ./setup.py
RefactoringTool: ./Client/setup.py
RefactoringTool: ./Client/src/python/dbs/__init__.py
RefactoringTool: ./Client/src/python/dbs/apis/__init__.py
RefactoringTool: ./Client/src/python/dbs/apis/dbsClient.py
RefactoringTool: ./Client/src/python/dbs/exceptions/__init__.py
RefactoringTool: ./Client/src/python/dbs/exceptions/dbsClientException.py
RefactoringTool: ./Client/tests/setup_test.py
RefactoringTool: ./Client/tests/dbsclient_t/__init__.py
RefactoringTool: ./Client/tests/dbsclient_t/deployment/DBSDeployment_t.py
RefactoringTool: ./Client/tests/dbsclient_t/deployment/__init__.py
RefactoringTool: ./Client/tests/dbsclient_t/unittests/DBSClientBlockWriter_t.py
RefactoringTool: ./Client/tests/dbsclient_t/unittests/DBSClientReader_t.py
RefactoringTool: ./Client/tests/dbsclient_t/unittests/DBSClientWriter_t.py
RefactoringTool: ./Client/tests/dbsclient_t/unittests/__init__.py
RefactoringTool: ./Client/tests/dbsclient_t/utils/DBSDataProvider.py
RefactoringTool: ./Client/tests/dbsclient_t/utils/__init__.py
RefactoringTool: ./Client/tests/dbsclient_t/utils/timeout.py
RefactoringTool: ./Client/tests/dbsclient_t/validation/DBSValidation_t.py
RefactoringTool: ./Client/tests/dbsclient_t/validation/__init__.py
RefactoringTool: ./Client/tests/profiling_t/DBSProfiling_t.py
RefactoringTool: ./Client/utils/compareInstance.py
RefactoringTool: ./Client/utils/dbs2Todbs3DatasetMigrate.py
RefactoringTool: ./Client/utils/dbs3_block_dump_comparison.py
RefactoringTool: ./Client/utils/dbs3_bulk_block_insert.py
RefactoringTool: ./Client/utils/dbs3_bulk_block_insert2.py
RefactoringTool: ./Client/utils/getserverinfo.py
RefactoringTool: ./Client/utils/insertAcqEra.py
RefactoringTool: ./Client/utils/insertBlockBulk.py
RefactoringTool: ./Client/utils/insert_block_migration_requests-MINIAOD.py
RefactoringTool: ./Client/utils/insertblock.py
RefactoringTool: ./Client/utils/insertdataset.py
RefactoringTool: ./Client/utils/insertdatatier.py
RefactoringTool: ./Client/utils/insertfile.py
RefactoringTool: ./Client/utils/insertprimary.py
RefactoringTool: ./Client/utils/insertprocessingVersion.py
RefactoringTool: ./Client/utils/listBlockSummaries.py
RefactoringTool: ./Client/utils/listBlockTrio.py
RefactoringTool: ./Client/utils/listFileArray.py
RefactoringTool: ./Client/utils/listFileLumis.py
RefactoringTool: ./Client/utils/listFileParentsByLumi.py
RefactoringTool: ./Client/utils/listFileSummaries.py
RefactoringTool: ./Client/utils/listFileTests.py
RefactoringTool: ./Client/utils/listFileinBlock.py
RefactoringTool: ./Client/utils/listParentDSTrio.py
RefactoringTool: ./Client/utils/listblock.py
RefactoringTool: ./Client/utils/listblockchildren.py
RefactoringTool: ./Client/utils/listblockparents.py
RefactoringTool: ./Client/utils/listdataset.py
RefactoringTool: ./Client/utils/listdatasetchildren.py
RefactoringTool: ./Client/utils/listdatasetparents.py
RefactoringTool: ./Client/utils/listdatatier.py
RefactoringTool: ./Client/utils/listfile.py
RefactoringTool: ./Client/utils/listfilechildren.py
RefactoringTool: ./Client/utils/listfileparents.py
RefactoringTool: ./Client/utils/listfiles.py
RefactoringTool: ./Client/utils/listprimary.py
RefactoringTool: ./Client/utils/listsites.py
RefactoringTool: ./Client/utils/migrate.py
RefactoringTool: ./Client/utils/readTestLogs.py
RefactoringTool: ./Client/utils/run_test.py
RefactoringTool: ./Client/utils/updateDatasetType.py
RefactoringTool: ./Client/utils/updateFileStatus.py
RefactoringTool: ./Client/utils/updateblock.py
RefactoringTool: ./Client/utils/DataOpsScripts/DBS3SetDatasetStatus.py
RefactoringTool: ./Client/utils/DataOpsScripts/DBS3SetFileStatus.py
RefactoringTool: ./Client/utils/DataOpsScripts/EventsPerDay.py
RefactoringTool: ./Client/utils/UserScripts/DBS3ListRunLumiInfo.py
RefactoringTool: ./DBS2To3Migration/SQL/recreateSequence.py
RefactoringTool: ./DBS2To3Migration/test/DBSSqlQueries.py
RefactoringTool: ./DBS2To3Migration/test/ValidateDualDBSWriting.py
RefactoringTool: ./DBS2To3Migration/test/ValidateMigration_t.py
RefactoringTool: ./DBS2To3Migration/test/ValidateOriginSiteName_t.py
RefactoringTool: ./PycurlClient/setup.py
RefactoringTool: ./PycurlClient/src/python/RestClient/RestApi.py
RefactoringTool: ./PycurlClient/src/python/RestClient/__init__.py
RefactoringTool: ./PycurlClient/src/python/RestClient/AuthHandling/BasicAuth.py
RefactoringTool: ./PycurlClient/src/python/RestClient/AuthHandling/X509Auth.py
RefactoringTool: ./PycurlClient/src/python/RestClient/AuthHandling/__init__.py
RefactoringTool: ./PycurlClient/src/python/RestClient/ErrorHandling/RestClientExceptions.py
RefactoringTool: ./PycurlClient/src/python/RestClient/ErrorHandling/__init__.py
RefactoringTool: ./PycurlClient/src/python/RestClient/ProxyPlugins/Socks5Proxy.py
RefactoringTool: ./PycurlClient/src/python/RestClient/ProxyPlugins/__init__.py
RefactoringTool: ./PycurlClient/src/python/RestClient/RequestHandling/HTTPRequest.py
RefactoringTool: ./PycurlClient/src/python/RestClient/RequestHandling/HTTPResponse.py
RefactoringTool: ./PycurlClient/src/python/RestClient/RequestHandling/__init__.py
RefactoringTool: ./Schema/DDL/generate-schema-deployable.py
RefactoringTool: ./Schema/SQLTester/genSQLFromListDatasetContents.py
RefactoringTool: ./Schema/Scripts/generate_dao.py
RefactoringTool: ./Schema/Scripts/generate_dataobjs.py
RefactoringTool: ./Schema/Scripts/generate_trigs.py
RefactoringTool: ./Schema/Scripts/process_sql.py
RefactoringTool: ./Schema/Scripts/process_sql_4sqlexplorer.py
RefactoringTool: ./Server/Python/control/DBSConfig.py
RefactoringTool: ./Server/Python/control/default_config_multiple_service.py
RefactoringTool: ./Server/Python/control/default_config_single_server.py
RefactoringTool: ./Server/Python/control/default_config_single_server_reader_writer.py
RefactoringTool: ./Server/Python/src/dbs/__init__.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSAcquisitionEra.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSBlock.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSBlockInsert.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSDataTier.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSDataType.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSDataset.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSDatasetAccessType.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSDoNothing.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSFile.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSFileBuffer.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSMigrate.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSOutputConfig.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSPhysicsGroup.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSPrimaryDataset.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSProcessingEra.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSReleaseVersion.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSRun.py
RefactoringTool: ./Server/Python/src/dbs/business/DBSSite.py
RefactoringTool: ./Server/Python/src/dbs/business/__init__.py
RefactoringTool: ./Server/Python/src/dbs/components/__init__.py
RefactoringTool: ./Server/Python/src/dbs/components/migration/DBSMigrationServer.py
RefactoringTool: ./Server/Python/src/dbs/components/migration/DefaultConfig.py
RefactoringTool: ./Server/Python/src/dbs/components/migration/StartUp.py
RefactoringTool: ./Server/Python/src/dbs/components/migration/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/SequenceManager.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/dbsListPrimaryDatasetTypes.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/dbsListPrimaryDatasets.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/AcquisitionEra/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/AcquisitionEra/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/AcquisitionEra/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/AcquisitionEra/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ApplicationExecutable/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ApplicationExecutable/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ApplicationExecutable/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/AssociatedFile/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/AssociatedFile/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Block/BriefList.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Block/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Block/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Block/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Block/ListStats.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Block/UpdateStats.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Block/UpdateStatus.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Block/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/BlockParent/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/BlockParent/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/BlockParent/ListChild.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/BlockParent/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/BlockSite/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/BlockSite/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/BlockStorageElement/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/BlockStorageElement/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/BranchHashe/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/BranchHashe/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/BranchHashe/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ComponentStatus/DBSStatus.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ComponentStatus/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ComponentStatus/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ComponentStatus/Update.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ComponentStatus/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DataTier/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DataTier/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DataTier/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DataTier/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Dataset/BriefList.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Dataset/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Dataset/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Dataset/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Dataset/UpdateStatus.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Dataset/UpdateType.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Dataset/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetOutputMod_config/GetDSConfigs.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetOutputMod_config/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetOutputMod_config/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetParent/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetParent/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetParent/ListChild.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetParent/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetRun/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetRun/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetRun/ListBlockRuns.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetRun/ListDSRuns.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetRun/ListFileRuns.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetRun/UpdateStatus.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetRun/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetType/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetType/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DatasetType/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DbsVersion/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/DbsVersion/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/File/BriefList.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/File/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/File/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/File/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/File/MgrtList.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/File/UpdateStatus.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/File/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileBuffer/DeleteDuplicates.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileBuffer/DeleteFiles.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileBuffer/FindDuplicates.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileBuffer/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileBuffer/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileBuffer/ListBlocks.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileBuffer/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileLumi/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileLumi/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileLumi/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileOutputMod_config/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileOutputMod_config/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileParent/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileParent/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileParent/ListChild.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileParent/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileParentBlock/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileParentBlock/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileType/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileType/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/FileType/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/MigrationBlock/FindMigrateableBlocks.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/MigrationBlock/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/MigrationBlock/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/MigrationBlock/Update.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/MigrationBlock/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/FindPendingRequest.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/ListOldest.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/Update.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/UpdateRequestStatus.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/MigrationRequests/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/OutputModuleConfig/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/OutputModuleConfig/GetIDForBlockInsert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/OutputModuleConfig/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/OutputModuleConfig/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/OutputModuleConfig/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ParameterSetHashe/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ParameterSetHashe/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ParameterSetHashe/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/PhysicsGroup/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/PhysicsGroup/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/PhysicsGroup/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/PrimaryDSType/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/PrimaryDSType/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/PrimaryDSType/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/PrimaryDSType/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/PrimaryDataset/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/PrimaryDataset/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/PrimaryDataset/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/PrimaryDataset/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ProcessedDataset/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ProcessedDataset/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ProcessedDataset/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ProcessingEra/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ProcessingEra/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ProcessingEra/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ProcessingEra/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ReleaseVersion/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ReleaseVersion/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/ReleaseVersion/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Service/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Service/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Service/Update.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Service/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Site/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Site/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Site/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Site/ListBlockSite.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/Site/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/StorageElement/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/StorageElement/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/StorageElement/ListBlockSE.py
RefactoringTool: ./Server/Python/src/dbs/dao/MySQL/StorageElement/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/SequenceManager.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/AcquisitionEra/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/AcquisitionEra/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/AcquisitionEra/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/AcquisitionEra/List_CI.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/AcquisitionEra/UpdateEndDate.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/AcquisitionEra/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ApplicationExecutable/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ApplicationExecutable/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ApplicationExecutable/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/AssociatedFile/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/AssociatedFile/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Block/BriefList.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Block/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Block/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Block/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Block/ListBlockOrigin.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Block/ListStats.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Block/SummaryList.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Block/UpdateSiteName.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Block/UpdateStats.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Block/UpdateStatus.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Block/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/BlockParent/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/BlockParent/Insert2.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/BlockParent/Insert3.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/BlockParent/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/BlockParent/ListChild.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/BlockParent/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/BlockSite/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/BlockSite/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/BranchHashe/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/BranchHashe/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/BranchHashe/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DataTier/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DataTier/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DataTier/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DataTier/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Dataset/BriefList.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Dataset/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Dataset/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Dataset/Insert2.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Dataset/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Dataset/UpdateStatus.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Dataset/UpdateType.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Dataset/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetOutputMod_config/GetDSConfigs.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetOutputMod_config/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetOutputMod_config/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetParent/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetParent/Insert2.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetParent/Insert3.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetParent/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetParent/ListChild.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetParent/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetRun/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetRun/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetRun/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetType/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetType/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetType/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DatasetType/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DbsVersion/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DbsVersion/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DoNothing/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/DoNothing/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/File/BriefList.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/File/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/File/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/File/Insert2.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/File/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/File/MgrtList.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/File/SummaryList.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/File/UpdateStatus.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/File/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileBuffer/DeleteDuplicates.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileBuffer/DeleteFiles.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileBuffer/FindDuplicates.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileBuffer/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileBuffer/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileBuffer/ListBlocks.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileBuffer/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileLumi/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileLumi/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileLumi/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileOutputMod_config/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileOutputMod_config/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileParent/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileParent/Insert2.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileParent/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileParent/ListBlockFileLumiIds.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileParent/ListChild.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileParent/ListFileParentageByLumi.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileParent/ListParentDatasetFileLumiIds.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileParent/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileParentBlock/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileParentBlock/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileType/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileType/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/FileType/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/InsertTable/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/InsertTable/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationBlock/FindMigrateableBlocks.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationBlock/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationBlock/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationBlock/Update.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationBlock/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/FindPendingRequest.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/ListOldest.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/Remove.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/Update.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/UpdateRequestStatus.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/MigrationRequests/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/OutputModuleConfig/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/OutputModuleConfig/GetIDForBlockInsert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/OutputModuleConfig/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/OutputModuleConfig/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/OutputModuleConfig/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ParameterSetHashe/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ParameterSetHashe/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ParameterSetHashe/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PhysicsGroup/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PhysicsGroup/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PhysicsGroup/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PhysicsGroup/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PrimaryDSType/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PrimaryDSType/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PrimaryDSType/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PrimaryDSType/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PrimaryDataset/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PrimaryDataset/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PrimaryDataset/Insert2.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PrimaryDataset/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/PrimaryDataset/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ProcessedDataset/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ProcessedDataset/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ProcessedDataset/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ProcessingEra/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ProcessingEra/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ProcessingEra/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ProcessingEra/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ReleaseVersion/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ReleaseVersion/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ReleaseVersion/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/ReleaseVersion/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Run/SummaryList.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Run/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Service/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Service/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Service/Update.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Service/__init__.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Site/GetID.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Site/Insert.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Site/List.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Site/ListBlockSite.py
RefactoringTool: ./Server/Python/src/dbs/dao/Oracle/Site/__init__.py
RefactoringTool: ./Server/Python/src/dbs/utils/DBSDaoTools.py
RefactoringTool: ./Server/Python/src/dbs/utils/DBSInputValidation.py
RefactoringTool: ./Server/Python/src/dbs/utils/DBSTransformInputType.py
RefactoringTool: ./Server/Python/src/dbs/utils/RestClientPool.py
RefactoringTool: ./Server/Python/src/dbs/utils/__init__.py
RefactoringTool: ./Server/Python/src/dbs/utils/dbsException.py
RefactoringTool: ./Server/Python/src/dbs/utils/dbsExceptionDef.py
RefactoringTool: ./Server/Python/src/dbs/utils/dbsExceptionHandler.py
RefactoringTool: ./Server/Python/src/dbs/utils/dbsHTTPSAuthHandler.py
RefactoringTool: ./Server/Python/src/dbs/utils/dbsUtils.py
RefactoringTool: ./Server/Python/src/dbs/utils/docstring_parser.py
RefactoringTool: ./Server/Python/src/dbs/web/DBSMigrateModel.py
RefactoringTool: ./Server/Python/src/dbs/web/DBSReaderModel.py
RefactoringTool: ./Server/Python/src/dbs/web/DBSServicesRegistry.py
RefactoringTool: ./Server/Python/src/dbs/web/DBSWriterModel.py
RefactoringTool: ./Server/Python/src/dbs/web/__init__.py
RefactoringTool: ./Server/Python/tests/DBS3SimpleClient.py
RefactoringTool: ./Server/Python/tests/SimpleBizlogicTest.py
RefactoringTool: ./Server/Python/tests/SimpleDAOTest.py
RefactoringTool: ./Server/Python/tests/TestBusiness.py
RefactoringTool: ./Server/Python/tests/TestDAO.py
RefactoringTool: ./Server/Python/tests/TestInsertTime.py
RefactoringTool: ./Server/Python/tests/TestListTime.py
RefactoringTool: ./Server/Python/tests/TestServer.py
RefactoringTool: ./Server/Python/tests/TestStress.py
RefactoringTool: ./Server/Python/tests/setup_test.py
RefactoringTool: ./Server/Python/tests/dao/Dataset/testInsert.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/performance/BusinessList.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/performance/CXOracleList.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/performance/DBSRestProfile.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/performance/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSBlock_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSDatasetParent_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSDataset_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSFileLumi_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSFileParent_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSFile_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSOutputConfig_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/business_t/DBSPrimaryDataset_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/business_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/components_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/components_t/migration_t/DBSMigrationServer_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/components_t/migration_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/AcquisitionEra_t/Insert_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/AcquisitionEra_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/AcquisitionEra_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/BlockParent_t/Insert_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/BlockParent_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/BlockParent_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/Block_t/Insert_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/Block_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/Block_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/DataTier_t/Insert_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/DataTier_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/DataTier_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/DatasetParent_t/Insert_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/DatasetParent_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/DatasetParent_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/Dataset_t/Insert_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/Dataset_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/Dataset_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/FileLumi_t/Insert_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/FileLumi_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/FileLumi_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/FileParent_t/Insert_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/FileParent_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/FileParent_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/File_t/Insert_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/File_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/File_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/OutputModuleConfig_t/Insert_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/OutputModuleConfig_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/OutputModuleConfig_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PhysicsGroup_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PhysicsGroup_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PrimaryDSType_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PrimaryDSType_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PrimaryDataset_t/Insert_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PrimaryDataset_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/PrimaryDataset_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/ProcessingEra_t/Insert_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/ProcessingEra_t/List_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/dao_t/Oracle_t/ProcessingEra_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/web_t/DBSMigrateModel_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/web_t/DBSReaderModel_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/web_t/DBSWriterModel_t.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/web_t/__init__.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/unittests/web_t/listprofiling.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/utils/CXOracleSQL.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/utils/CreateTestData.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/utils/DBSDataProvider.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/utils/DBSRestApi.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/utils/DaoConfig.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/utils/ParseConfig.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/utils/TestTools.py
RefactoringTool: ./Server/Python/tests/dbsserver_t/utils/__init__.py
RefactoringTool: ./Server/Python/utils/das_logfile_analyser.py
RefactoringTool: ./Server/Python/utils/das_logfile_parser.py
RefactoringTool: ./Server/Python/utils/dbs2_logfile_analyser.py
RefactoringTool: ./Server/Python/utils/dbs2_logfile_parser.py
RefactoringTool: ./Server/Python/utils/dbs3_bulk_block_input_validation.py
RefactoringTool: ./Server/Python/utils/dbs3_logfile_analyser.py
RefactoringTool: ./Server/Python/utils/dbs3_logfile_parser.py
RefactoringTool: ./Server/Python/utils/dbs3_logfile_parser_for_failed_blocks.py
RefactoringTool: ./SystemTests/bin/LifeCyclePlots.py
RefactoringTool: ./SystemTests/bin/StatsServer.py
RefactoringTool: ./SystemTests/bin/das_logfile_analyser.py
RefactoringTool: ./SystemTests/bin/das_logfile_parser.py
RefactoringTool: ./SystemTests/bin/dbs2_logfile_parser.py
RefactoringTool: ./SystemTests/bin/dbs3BulkInsert.py
RefactoringTool: ./SystemTests/bin/dbs3Crab3Workflow.py
RefactoringTool: ./SystemTests/bin/dbs3CrabWorkflow.py
RefactoringTool: ./SystemTests/bin/dbs3DASAccess.py
RefactoringTool: ./SystemTests/bin/dbs3DASGetQueries.py
RefactoringTool: ./SystemTests/bin/dbs3GetBlocks.py
RefactoringTool: ./SystemTests/bin/dbs3GetDatasets.py
RefactoringTool: ./SystemTests/bin/dbs3GetFileLumis.py
RefactoringTool: ./SystemTests/bin/dbs3GetFileParents.py
RefactoringTool: ./SystemTests/bin/dbs3GetFiles.py
RefactoringTool: ./SystemTests/bin/dbs3GetPrimaryDSType.py
RefactoringTool: ./SystemTests/bin/dbs3IntroduceFailures.py
RefactoringTool: ./SystemTests/bin/dbs3MigrationService.py
RefactoringTool: ./SystemTests/bin/dbs3WriterStressTest.py
RefactoringTool: ./SystemTests/bin/dbs3_logfile_parser.py
RefactoringTool: ./SystemTests/bin/dbs3dasComparision.py
RefactoringTool: ./SystemTests/bin/getFakeData.py
RefactoringTool: ./SystemTests/src/python/LifeCycleAnalysis/__init__.py
RefactoringTool: ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/HistoManager.py
RefactoringTool: ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/Histogram.py
RefactoringTool: ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/LifeCyclePlotManager.py
RefactoringTool: ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/SqliteDAO.py
RefactoringTool: ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/WebView.py
RefactoringTool: ./SystemTests/src/python/LifeCycleAnalysis/LifeCyclePlots/__init__.py
RefactoringTool: ./SystemTests/src/python/LifeCycleTests/__init__.py
RefactoringTool: ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/APIFactory.py
RefactoringTool: ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/ConfigurationFactory.py
RefactoringTool: ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/OptParser.py
RefactoringTool: ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/PayloadHandler.py
RefactoringTool: ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/Stats2Sqlite.py
RefactoringTool: ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/StatsClient.py
RefactoringTool: ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/Timing.py
RefactoringTool: ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/__init__.py
RefactoringTool: ./doc/generate_modules.py
RefactoringTool: ./doc/dbs/conf.py

                                                         starttime=starttime,
                                                         endtime=endtime,
                                                         test_type='writer_stats'),
@@ -290,7 +289,7 @@
     @property
     def histo_names(self):
         histo_names = list()
-        for histo_manager in self._histo_managers.itervalues():
+        for histo_manager in self._histo_managers.values():
             histo_names.extend(histo_manager.histo_names)
         return histo_names
 
@@ -316,11 +315,11 @@
         self._histo_managers[category].update_histos(data)
 
     def draw_histos(self):
-        for histo_manager in self._histo_managers.itervalues():
+        for histo_manager in self._histo_managers.values():
             histo_manager.draw_histos()
 
     def save_histos_as(self, output_directory, format="png"):
-        for histo_manager in self._histo_managers.itervalues():
+        for histo_manager in self._histo_managers.values():
             histo_manager.save_histos_as(output_directory, format)
 
     def add_stacked_histos(self, categories=['reader_stats', 'writer_stats']):
@@ -330,7 +329,7 @@
             if category not in self._supported_categories:
                 raise NameError('Category %s is not supported by %s' % (category, self.__class__.__name__))
 
-            stripped_histo_names = map(lambda x: x.replace(category, ''), self._histo_managers[category].histo_names)
+            stripped_histo_names = [x.replace(category, '') for x in self._histo_managers[category].histo_names]
 
             if duplicated_histo_names:
                 duplicated_histo_names.intersection_update(stripped_histo_names)
--- ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/APIFactory.py	(original)
+++ ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/APIFactory.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 from dbs.apis.dbsClient import DbsApi
 from DataProvider.core.dbs_provider import DBSDataProvider
 from DataProvider.core.phedex_provider import PhedexDataProvider
--- ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/ConfigurationFactory.py	(original)
+++ ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/ConfigurationFactory.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 from dbs.apis.dbsClient import DbsApi
 
 class DBS3ApiFactory(object):
--- ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/OptParser.py	(original)
+++ ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/OptParser.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 from optparse import OptionParser
 
 def get_command_line_options(executable_name, arguments):
--- ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/PayloadHandler.py	(original)
+++ ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/PayloadHandler.py	(refactored)
@@ -8,7 +8,7 @@
         value += step
 
 def split_list(this_list, split_size):
-    for element in xrange(0, len(this_list), split_size):
+    for element in range(0, len(this_list), split_size):
         yield this_list[element:element+split_size]
         
 class PayloadHandler(object):
--- ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/Stats2Sqlite.py	(original)
+++ ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/Stats2Sqlite.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 from optparse import OptionParser
 import sqlite3 as sqlite
 import glob, json, os, sys
--- ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/StatsClient.py	(original)
+++ ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/StatsClient.py	(refactored)
@@ -1,10 +1,10 @@
 #!/usr/bin/env python
-import xmlrpclib
-import cPickle
+import xmlrpc.client
+import pickle
 
 class StatsXMLRPCClient(object):
     def __init__(self, host="localhost", port=9876):
-        self.stats_server = xmlrpclib.ServerProxy('http://%s:%s' % (host, port))
+        self.stats_server = xmlrpc.client.ServerProxy('http://%s:%s' % (host, port))
 
     def send(self, stats):
         self.stats_server.add_stats(stats)
@@ -19,7 +19,7 @@
     def __send(self, stats):
         try:
             self.f = open(self.named_pipe, 'wb')
-            cPickle.dump(stats, self.f, cPickle.HIGHEST_PROTOCOL)
+            pickle.dump(stats, self.f, pickle.HIGHEST_PROTOCOL)
             self.f.close()
         except IOError as xxx_todo_changeme:
             self._ex = xxx_todo_changeme
@@ -34,7 +34,7 @@
 
     def send(self, stats):
         #if there is a broken pipe, try to send data again (Try five times)
-        for _ in xrange(5):
+        for _ in range(5):
             if self.__send(stats):
                 return
         raise self._ex
--- ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/Timing.py	(original)
+++ ./SystemTests/src/python/LifeCycleTests/LifeCycleTools/Timing.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import print_function
+
 import time
 import json
 
--- ./doc/generate_modules.py	(original)
+++ ./doc/generate_modules.py	(refactored)
@@ -7,7 +7,7 @@
 creates ReST files appropriately to create code documentation with Sphinx.
 It also creates a modules index (named modules.<suffix>).
 """
-from __future__ import print_function
+
 
 # Copyright 2008 Socit des arts technologiques (SAT), http://www.sat.qc.ca/
 # Copyright 2010 Thomas Waldmann <tw AT waldmann-edv DOT de>
--- ./doc/dbs/conf.py	(original)
+++ ./doc/dbs/conf.py	(refactored)
@@ -40,8 +40,8 @@
 master_doc = 'index'
 
 # General information about the project.
-project = u'CMS Data Bookkeeping Service'
-copyright = u'2011, Yuyi Guo, Manuel Giffels'
+project = 'CMS Data Bookkeeping Service'
+copyright = '2011, Yuyi Guo, Manuel Giffels'
 
 # The version info for the project you're documenting, acts as replacement for
 # |version| and |release|, also used in various other places throughout the
@@ -178,8 +178,8 @@
 # Grouping the document tree into LaTeX files. List of tuples
 # (source start file, target name, title, author, documentclass [howto/manual]).
 latex_documents = [
-  ('index', 'DBS.tex', u'DBS Documentation',
-   u'Yuyi Guo, Manuel Giffels', 'manual'),
+  ('index', 'DBS.tex', 'DBS Documentation',
+   'Yuyi Guo, Manuel Giffels', 'manual'),
 ]
 
 # The name of an image file (relative to this directory) to place at the top of
